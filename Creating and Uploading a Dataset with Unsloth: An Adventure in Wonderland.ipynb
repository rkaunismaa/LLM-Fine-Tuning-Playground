{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Friday, August 9, 2024\n",
    "\n",
    "Still messing with this thing ... \n",
    "\n",
    "#### Thursday, August 8, 2024\n",
    "\n",
    "Continuing with trying to step through all of this code. \n",
    "\n",
    "Dammit. I can't get this thing to run the training ... why is it failing??\n",
    "\n",
    "#### Wednesday, August 7, 2024\n",
    "\n",
    "[Creating and Uploading a Dataset with Unsloth: An Adventure in Wonderland](https://huggingface.co/blog/dimentox/unsloth-mistral-training)\n",
    "\n",
    "Right now I have no idea if I can even load this model locally, but gonna try just cuz it looks interesting, and I have considered doing something like this anyways, cuz Github, right?! ... \n",
    "\n",
    "mamba activate unsloth_env2 (unsloth 2024.8)\n",
    "\n",
    "Original code was from the blog article, summarized into a single cell of python code. I am breaking that code down into cells inside this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need these next two statements, otherwise we get ...\n",
    "# NotImplementedError: Using RTX 4000 series doesn't support faster communication broadband via P2P or IB. Please set `NCCL_P2P_DISABLE=\\\"1\\\"` and `NCCL_IB_DISABLE=\\\"1\\\" or use `accelerate launch` which will do this automatically.\"\n",
    "# ... when we try to initialize SentenceTransformerTrainingArguments further on down ... \n",
    "os.environ[\"NCCL_P2P_DISABLE\"]=\"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is in the huggingface cache??\n",
    "!ls /home/rob/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Setting Up the Environment\n",
    "# !pip install beautifulsoup4 gitpython huggingface_hub datasets requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Cloning and Pulling the Repository\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from git import Repo\n",
    "from huggingface_hub import HfApi\n",
    "from datasets import Dataset, DatasetDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbose_print(message):\n",
    "    print(f\"[INFO] {message}\")\n",
    "\n",
    "def clone_or_pull_repo(repo_url, repo_name):\n",
    "    if os.path.exists(repo_name):\n",
    "        verbose_print(f\"Repository {repo_name} already exists. Pulling latest changes.\")\n",
    "        repo = Repo(repo_name)\n",
    "        repo.remotes.origin.pull()\n",
    "    else:\n",
    "        verbose_print(f\"Cloning repository from {repo_url}\")\n",
    "        Repo.clone_from(repo_url, repo_name)\n",
    "\n",
    "def extract_markdown_files(repo_path):\n",
    "    verbose_print(f\"Extracting Markdown files from {repo_path}\")\n",
    "    markdown_files = []\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".md\"):\n",
    "                markdown_files.append(os.path.join(root, file))\n",
    "    return markdown_files\n",
    "\n",
    "# Step 3: Parsing and Scraping Content\n",
    "def parse_markdown(file_path):\n",
    "    verbose_print(f\"Parsing Markdown file {file_path}\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    sections = content.split('\\n## ')\n",
    "    parsed_sections = [section.replace('\\n', ' ') for section in sections]\n",
    "    return parsed_sections\n",
    "\n",
    "def get_page_links(base_url, link_selector):\n",
    "    verbose_print(f\"Getting page links from {base_url}\")\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    page_links = []\n",
    "    for link in soup.select(link_selector):\n",
    "        href = link['href']\n",
    "        if not href.startswith('http') and href != '#':\n",
    "            href = base_url.rstrip('/') + '/' + href.lstrip('/')\n",
    "            page_links.append(href)\n",
    "    return page_links\n",
    "\n",
    "def scrape_page(url, content_selector):\n",
    "    verbose_print(f\"Scraping content from {url}\")\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    page_content = []\n",
    "    main_content = soup.select_one(content_selector)\n",
    "    if main_content:\n",
    "        sections = main_content.find_all(['h1', 'h2', 'h3', 'p', 'pre'])\n",
    "        for section in sections:\n",
    "            page_content.append(section.text)\n",
    "    return page_content\n",
    "\n",
    "# Step 4: Creating and Saving the Dataset\n",
    "def create_dataset(repo_url, doc_urls):\n",
    "    dataset = []\n",
    "\n",
    "    # Scrape GitHub repository\n",
    "    repo_name = repo_url.split('/')[-1].replace('.git', '')\n",
    "    clone_or_pull_repo(repo_url, repo_name)\n",
    "    markdown_files = extract_markdown_files(repo_name)\n",
    "    for md_file in markdown_files:\n",
    "        sections = parse_markdown(md_file)\n",
    "        for section in sections:\n",
    "            dataset.append({\n",
    "                'source': 'GitHub',\n",
    "                'repository': repo_name,\n",
    "                'file': md_file,\n",
    "                'label': 'autogen',\n",
    "                'content': section\n",
    "            })\n",
    "\n",
    "    # Scrape documentation site\n",
    "    for doc_url, link_selector, content_selector in doc_urls:\n",
    "        page_links = get_page_links(doc_url, link_selector)\n",
    "        for page_url in page_links:\n",
    "            page_content = scrape_page(page_url, content_selector)\n",
    "            for section in page_content:\n",
    "                dataset.append({\n",
    "                    'source': 'Documentation',\n",
    "                    'url': page_url,\n",
    "                    'label': 'autogen',\n",
    "                    'content': section\n",
    "                })\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def load_dataset_locally(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        verbose_print(f\"Loading existing dataset from {file_path}\")\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    verbose_print(f\"No existing dataset found at {file_path}\")\n",
    "    return []\n",
    "\n",
    "def save_dataset_locally(dataset, output_file):\n",
    "    verbose_print(f\"Saving dataset to {output_file}\")\n",
    "    with open(output_file, 'w') as file:\n",
    "        json.dump(dataset, file, indent=4)\n",
    "    verbose_print(\"Dataset saved successfully\")\n",
    "\n",
    "# Step 5: Uploading to Hugging Face\n",
    "def upload_to_huggingface(dataset, repo_id):\n",
    "    token = os.getenv(\"HF_TOKEN\")\n",
    "    verbose_print(f\"Uploading dataset to Hugging Face with repository ID {repo_id}\")\n",
    "    hf_api = HfApi()\n",
    "    hf_api.create_repo(repo_id, token=token, repo_type=\"dataset\", private=False)\n",
    "\n",
    "    # Create a DatasetDict and push to hub\n",
    "    dataset_dict = DatasetDict({\"train\": Dataset.from_list(dataset)})\n",
    "    dataset_dict.push_to_hub(repo_id, token=token)\n",
    "    verbose_print(f\"Dataset uploaded to Hugging Face with repository ID {repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_file_if_exists(file_path):\n",
    "    \"\"\"\n",
    "    Check if a file exists, and if it does, delete it.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the file to be checked and deleted.\n",
    "\n",
    "    Returns:\n",
    "    str: A message indicating the result of the operation.\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        return f\"File '{file_path}' has been deleted.\"\n",
    "    else:\n",
    "        return f\"File '{file_path}' does not exist.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "repo_url = 'https://github.com/microsoft/autogen.git'\n",
    "doc_urls = [\n",
    "    ('https://microsoft.github.io/autogen/docs/', 'a[href]', 'div.md-content'),\n",
    "    ('https://microsoft.github.io/autogen/docs/Examples', 'a[href]', 'div.md-content'),\n",
    "    ('https://microsoft.github.io/autogen/docs/notebooks', 'a[href]', 'div.md-content'),\n",
    "    ('https://microsoft.github.io/autogen/blog', 'a[href]', 'div.blog-content')\n",
    "]\n",
    "output_file = 'autogen_python_dataset.json'\n",
    "# repo_id = 'dimentox/autogen-python'\n",
    "repo_id = 'robkayinto/autogen-python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CREATE_DATASET = not os.path.exists(output_file)\n",
    "CREATE_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want to create this dataset once, so ... \n",
    "if CREATE_DATASET:\n",
    "    result = delete_file_if_exists(output_file)\n",
    "    print(result)\n",
    "    # running this will always append to any existing autogen_python_dataset.json ... \n",
    "    verbose_print(\"Starting dataset creation process\")\n",
    "    existing_dataset = load_dataset_locally(output_file)\n",
    "    new_dataset = create_dataset(repo_url, doc_urls)\n",
    "    combined_dataset = existing_dataset + new_dataset\n",
    "    save_dataset_locally(combined_dataset, output_file)\n",
    "    # upload_to_huggingface(combined_dataset, repo_id) # Nope for now!\n",
    "    verbose_print(\"Dataset creation and upload process completed\")\n",
    "\n",
    "# 36.2s\n",
    "\n",
    "# 48.4s\n",
    "\n",
    "# 48.4s ... re-run ... \n",
    "# 27m 56.7s ... run the first time ... cuz cloning autogen?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Fine-Tuning the Model\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveTrainer(SFTTrainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.prev_eval_loss = float('inf')\n",
    "\n",
    "    def evaluation_step(self, *args, **kwargs):\n",
    "        output = super().evaluation_step(*args, **kwargs)\n",
    "        current_eval_loss = output['eval_loss']\n",
    "\n",
    "        # Adaptive Learning Rate Adjustment\n",
    "        if current_eval_loss > self.prev_eval_loss:\n",
    "            self.args.learning_rate *= 0.9  # Reduce learning rate if loss increased\n",
    "            print(f\"Decreased learning rate to: {self.args.learning_rate}\")\n",
    "        else:\n",
    "            self.args.learning_rate *= 1.05  # Slightly increase if loss decreased\n",
    "            print(f\"Increased learning rate to: {self.args.learning_rate}\")\n",
    "\n",
    "        self.prev_eval_loss = current_eval_loss\n",
    "        return output\n",
    "\n",
    "    def training_step(self, *args, **kwargs):\n",
    "        # Adjust gradient clipping based on gradient norms\n",
    "        if self.state.global_step > 0 and self.state.global_step % self.args.eval_steps == 0:\n",
    "            current_grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.max_grad_norm)\n",
    "            print(f\"Adjusted gradient clipping to: {current_grad_norm}\")\n",
    "\n",
    "        return super().training_step(*args, **kwargs)\n",
    "\n",
    "def print_memory_stats(stage):\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    used_memory = round(torch.cuda.memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"[{stage}] GPU: {gpu_stats.name}, Memory Reserved: {used_memory} GB / {max_memory} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "==((====))==  Unsloth 2024.8: Fast Mistral patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.65 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925f1beab9cd43e48257bc5d591bedb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/165k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a491d9f5a7184cc983166b64f3aa6b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984aa3084b684d4abbbfbf2d868121e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9fe1473c9440419a054bdb2e215a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aea7d2b731f4ed2a36385105b1ea200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d957c1ecc14cbe82aa8317fcf26b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57c7b39e74c4113b41929fa2d9f94dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/181k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212f8b7b92644c44b1a7a2e1c31d17aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f08b9d664ff42d48b5a6ebf2c59ead3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "print(\"Loading model\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    token=\"token\"\n",
    ")\n",
    "\n",
    "# 7.9s \n",
    "# 14.1s ... re-load to the 4090 ... \n",
    "# 119m 51.7s ... using the compton connection ... \n",
    "#   3m 50.2s ... using the BELL connection ... Damn! What a difference!\n",
    "#  12m 11.3s ... downloaded again just to see if any difference ... \n",
    "#  13m 12.3s ... and again, just cuz I can ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug  9 15:10:35 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2070 ...    Off | 00000000:01:00.0  On |                  N/A |\n",
      "| 25%   35C    P8               8W / 215W |     73MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off | 00000000:02:00.0 Off |                  Off |\n",
      "|  0%   36C    P8              11W / 450W |   8980MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1716      G   /usr/lib/xorg/Xorg                           61MiB |\n",
      "|    0   N/A  N/A      1799      G   /usr/bin/gnome-shell                          8MiB |\n",
      "|    1   N/A  N/A      1716      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A      4783      C   ...forge3/envs/unsloth_env2/bin/python     8960MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Laura\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Laura\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb96a33f88464f6b9527eb10fb599091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Loading dataset\")\n",
    "# dataset_path = \"autogen_python_dataset.json\"\n",
    "# dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "dataset = load_dataset(\"json\", data_files=output_file, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd9d425b4cc4030ae2c0d9f5fb76cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/387 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "custom_prompt = \"\"\"Source: {}\n",
    "Repository: {}\n",
    "File: {}\n",
    "Label: {}\n",
    "Content: {}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    sources = examples[\"source\"]\n",
    "    repositories = examples[\"repository\"]\n",
    "    files = examples[\"file\"]\n",
    "    labels = examples[\"label\"]\n",
    "    contents = examples[\"content\"]\n",
    "    texts = []\n",
    "    for source, repository, file, label, content in zip(sources, repositories, files, labels, contents):\n",
    "        text = custom_prompt.format(source, repository, file, label, content) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048edc346aec4a3facfc2a436dcf3fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/387 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = AdaptiveTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        # output_dir=\"outputs\",\n",
    "        output_dir=\"aiw_outputs\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50,\n",
    "        eval_steps=1,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Before Training] GPU: NVIDIA GeForce RTX 4090, Memory Reserved: 8.588 GB / 23.65 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 387 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 48\n",
      " \"-____-\"     Number of trainable parameters = 57,016,320\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrobkayinto\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rob/Data/Documents/Github/rkaunismaa/LLM-Fine-Tuning-Playground/wandb/run-20240809_151058-w13pwzcd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/robkayinto/huggingface/runs/w13pwzcd' target=\"_blank\">aiw_outputs</a></strong> to <a href='https://wandb.ai/robkayinto/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/robkayinto/huggingface' target=\"_blank\">https://wandb.ai/robkayinto/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/robkayinto/huggingface/runs/w13pwzcd' target=\"_blank\">https://wandb.ai/robkayinto/huggingface/runs/w13pwzcd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m print_memory_stats(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBefore Training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# trainer_stats = trainer.train(resume_from_checkpoint=True)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# trainer_stats = trainer.train(resume_from_checkpoint=False)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m print_memory_stats(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter Training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<string>:126\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<string>:363\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 27\u001b[0m, in \u001b[0;36mAdaptiveTrainer.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m     current_grad_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdjusted gradient clipping to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_grad_norm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/unsloth_env2/lib/python3.10/site-packages/transformers/trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3318\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3323\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3324\u001b[0m ):\n",
      "File \u001b[0;32m~/miniforge3/envs/unsloth_env2/lib/python3.10/site-packages/transformers/trainer.py:3381\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[0;32m-> 3381\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3382\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3383\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3384\u001b[0m         )\n\u001b[1;32m   3385\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   3386\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "print_memory_stats(\"Before Training\")\n",
    "trainer_stats = trainer.train()\n",
    "# trainer_stats = trainer.train(resume_from_checkpoint=True)\n",
    "# trainer_stats = trainer.train(resume_from_checkpoint=False)\n",
    "print_memory_stats(\"After Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Using the Fine-Tuned Model\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "print(\"Loading fine-tuned model\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"lora_model\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    token=\"TOKEN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        \"\"\"\n",
    "        <s>\n",
    "        Q: What is the capital of France?\n",
    "        A:\n",
    "        \"\"\"\n",
    "    ],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "outputs = model.generate(**inputs, streamer=text_streamer, max_new_tokens=64)\n",
    "print(tokenizer.batch_decode(outputs))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
