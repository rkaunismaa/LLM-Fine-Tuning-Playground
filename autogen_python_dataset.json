[
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/README.md",
        "label": "autogen",
        "content": "<a name=\"readme-top\"></a>  [![PyPI version](https://badge.fury.io/py/pyautogen.svg)](https://badge.fury.io/py/pyautogen) [![Build](https://github.com/microsoft/autogen/actions/workflows/python-package.yml/badge.svg)](https://github.com/microsoft/autogen/actions/workflows/python-package.yml) ![Python Version](https://img.shields.io/badge/3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue) [![Downloads](https://static.pepy.tech/badge/pyautogen/week)](https://pepy.tech/project/pyautogen) [![Discord](https://img.shields.io/discord/1153072414184452236?logo=discord&style=flat)](https://aka.ms/autogen-dc) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)  [![NuGet version](https://badge.fury.io/nu/AutoGen.Core.svg)](https://badge.fury.io/nu/AutoGen.Core)  # AutoGen [\ud83d\udcda Cite paper](#related-papers). <!-- <p align=\"center\">     <img src=\"https://github.com/microsoft/autogen/blob/main/website/static/img/flaml.svg\"  width=200>     <br> </p> --> :fire: May 29, 2024: DeepLearning.ai launched a new short course [AI Agentic Design Patterns with AutoGen](https://www.deeplearning.ai/short-courses/ai-agentic-design-patterns-with-autogen), made in collaboration with Microsoft and Penn State University, and taught by AutoGen creators [Chi Wang](https://github.com/sonichi) and [Qingyun Wu](https://github.com/qingyun-wu).  :fire: May 24, 2024: Foundation Capital published an article on [Forbes: The Promise of Multi-Agent AI](https://www.forbes.com/sites/joannechen/2024/05/24/the-promise-of-multi-agent-ai/?sh=2c1e4f454d97) and a video [AI in the Real World Episode 2: Exploring Multi-Agent AI and AutoGen with Chi Wang](https://www.youtube.com/watch?v=RLwyXRVvlNk).  :fire: May 13, 2024: [The Economist](https://www.economist.com/science-and-technology/2024/05/13/todays-ai-models-are-impressive-teams-of-them-will-be-formidable) published an article about multi-agent systems (MAS) following a January 2024 interview with [Chi Wang](https://github.com/sonichi).  :fire: May 11, 2024: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://openreview.net/pdf?id=uAjxFFing2) received the best paper award at the [ICLR 2024 LLM Agents Workshop](https://llmagents.github.io/).  :fire: Apr 26, 2024: [AutoGen.NET](https://microsoft.github.io/autogen-for-net/) is available for .NET developers!  :fire: Apr 17, 2024: Andrew Ng cited AutoGen in [The Batch newsletter](https://www.deeplearning.ai/the-batch/issue-245/) and [What's next for AI agentic workflows](https://youtu.be/sal78ACtGTc?si=JduUzN_1kDnMq0vF) at Sequoia Capital's AI Ascent (Mar 26).  :fire: Mar 3, 2024: What's new in AutoGen? \ud83d\udcf0[Blog](https://microsoft.github.io/autogen/blog/2024/03/03/AutoGen-Update); \ud83d\udcfa[Youtube](https://www.youtube.com/watch?v=j_mtwQiaLGU).  :fire: Mar 1, 2024: the first AutoGen multi-agent experiment on the challenging [GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard) benchmark achieved the No. 1 accuracy in all the three levels.  <!-- :tada: Jan 30, 2024: AutoGen is highlighted by Peter Lee in Microsoft Research Forum [Keynote](https://t.co/nUBSjPDjqD). -->  :tada: Dec 31, 2023: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155) is selected by [TheSequence: My Five Favorite AI Papers of 2023](https://thesequence.substack.com/p/my-five-favorite-ai-papers-of-2023).  <!-- :fire: Nov 24: pyautogen [v0.2](https://github.com/microsoft/autogen/releases/tag/v0.2.0) is released with many updates and new features compared to v0.1.1. It switches to using openai-python v1. Please read the [migration guide](https://microsoft.github.io/autogen/docs/Installation#python). -->  <!-- :fire: Nov 11: OpenAI's Assistants are available in AutoGen and interoperatable with other AutoGen agents! Checkout our [blogpost](https://microsoft.github.io/autogen/blog/2023/11/13/OAI-assistants) for details and examples. -->  :tada: Nov 8, 2023: AutoGen is selected into [Open100: Top 100 Open Source achievements](https://www.benchcouncil.org/evaluation/opencs/annual.html) 35 days after spinoff from [FLAML](https://github.com/microsoft/FLAML).  <!-- :tada: Nov 6, 2023: AutoGen is mentioned by Satya Nadella in a [fireside chat](https://youtu.be/0pLBvgYtv6U). -->  <!-- :tada: Nov 1, 2023: AutoGen is the top trending repo on GitHub in October 2023. -->  <!-- :tada: Oct 03, 2023: AutoGen spins off from [FLAML](https://github.com/microsoft/FLAML) on GitHub. -->  <!-- :tada: Aug 16: Paper about AutoGen on [arxiv](https://arxiv.org/abs/2308.08155). -->  :tada: Mar 29, 2023: AutoGen is first created in [FLAML](https://github.com/microsoft/FLAML).  <!-- :fire: FLAML is highlighted in OpenAI's [cookbook](https://github.com/openai/openai-cookbook#related-resources-from-around-the-web).  :fire: [autogen](https://microsoft.github.io/autogen/) is released with support for ChatGPT and GPT-4, based on [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673).  :fire: FLAML supports Code-First AutoML & Tuning \u2013 Private Preview in [Microsoft Fabric Data Science](https://learn.microsoft.com/en-us/fabric/data-science/). -->  <p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">   <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">     \u2191 Back to Top \u2191   </a> </p> "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/README.md",
        "label": "autogen",
        "content": "What is AutoGen  AutoGen is an open-source programming framework for building AI agents and facilitating cooperation among multiple agents to solve tasks. AutoGen aims to streamline the development and research of agentic AI, much like PyTorch does for Deep Learning. It offers features such as agents capable of interacting with each other, facilitates the use of various large language models (LLMs) and tool use support, autonomous and human-in-the-loop workflows, and multi-agent conversation patterns.  **Open Source Statement**: The project welcomes contributions from developers and organizations worldwide. Our goal is to foster a collaborative and inclusive community where diverse perspectives and expertise can drive innovation and enhance the project's capabilities. Whether you are an individual contributor or represent an organization, we invite you to join us in shaping the future of this project. Together, we can build something truly remarkable.  The project is currently maintained by a [dynamic group of volunteers](https://butternut-swordtail-8a5.notion.site/410675be605442d3ada9a42eb4dfef30?v=fa5d0a79fd3d4c0f9c112951b2831cbb&pvs=4) from several different organizations. Contact project administrators Chi Wang and Qingyun Wu via auto-gen@outlook.com if you are interested in becoming a maintainer.   ![AutoGen Overview](https://github.com/microsoft/autogen/blob/main/website/static/img/autogen_agentchat.png)  - AutoGen enables building next-gen LLM applications based on [multi-agent conversations](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses. - It supports [diverse conversation patterns](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns) for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,   the number of agents, and agent conversation topology. - It provides a collection of working systems with different complexities. These systems span a [wide range of applications](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen) from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns. - AutoGen provides [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification). It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.  AutoGen is created out of collaborative [research](https://microsoft.github.io/autogen/docs/Research) from Microsoft, Penn State University, and the University of Washington.  <p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">   <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">     \u2191 Back to Top \u2191   </a> </p> "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/README.md",
        "label": "autogen",
        "content": "Roadmaps  To see what we are working on and what we plan to work on, please check our [Roadmap Issues](https://aka.ms/autogen-roadmap).  <p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">   <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">     \u2191 Back to Top \u2191   </a> </p> "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/README.md",
        "label": "autogen",
        "content": "Quickstart The easiest way to start playing is 1. Click below to use the GitHub Codespace      [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/autogen?quickstart=1)   2. Copy OAI_CONFIG_LIST_sample to ./notebook folder, name to OAI_CONFIG_LIST, and set the correct configuration.  3. Start playing with the notebooks!  *NOTE*: OAI_CONFIG_LIST_sample lists GPT-4 as the default model, as this represents our current recommendation, and is known to work well with AutoGen. If you use a model other than GPT-4, you may need to revise various system prompts (especially if using weaker models like GPT-3.5-turbo). Moreover, if you use models other than those hosted by OpenAI or Azure, you may incur additional risks related to alignment and safety. Proceed with caution if updating this default.  <p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">   <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">     \u2191 Back to Top \u2191   </a> </p> "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/README.md",
        "label": "autogen",
        "content": "[Installation](https://microsoft.github.io/autogen/docs/Installation) ### Option 1. Install and Run AutoGen in Docker  Find detailed instructions for users [here](https://microsoft.github.io/autogen/docs/installation/Docker#step-1-install-docker), and for developers [here](https://microsoft.github.io/autogen/docs/Contribute#docker-for-development).  ### Option 2. Install AutoGen Locally  AutoGen requires **Python version >= 3.8, < 3.13**. It can be installed from pip:  ```bash pip install pyautogen ```  Minimal dependencies are installed without extra options. You can install extra options based on the feature you need.  <!-- For example, use the following to install the dependencies needed by the [`blendsearch`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#blendsearch-economical-hyperparameter-optimization-with-blended-search-strategy) option. ```bash pip install \"pyautogen[blendsearch]\" ``` -->  Find more options in [Installation](https://microsoft.github.io/autogen/docs/Installation#option-2-install-autogen-locally-using-virtual-environment).  <!-- Each of the [`notebook examples`](https://github.com/microsoft/autogen/tree/main/notebook) may require a specific option to be installed. -->  Even if you are installing and running AutoGen locally outside of docker, the recommendation and default behavior of agents is to perform [code execution](https://microsoft.github.io/autogen/docs/FAQ/#code-execution) in docker. Find more instructions and how to change the default behaviour [here](https://microsoft.github.io/autogen/docs/Installation#code-execution-with-docker-(default)).  For LLM inference configurations, check the [FAQs](https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints).  <p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">   <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">     \u2191 Back to Top \u2191   </a> </p> "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/README.md",
        "label": "autogen",
        "content": "Multi-Agent Conversation Framework  Autogen enables the next-gen LLM applications with a generic [multi-agent conversation](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) framework. It offers customizable and conversable agents that integrate LLMs, tools, and humans. By automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code.  Features of this use case include:  - **Multi-agent conversations**: AutoGen agents can communicate with each other to solve tasks. This allows for more complex and sophisticated applications than would be possible with a single LLM. - **Customization**: AutoGen agents can be customized to meet the specific needs of an application. This includes the ability to choose the LLMs to use, the types of human input to allow, and the tools to employ. - **Human participation**: AutoGen seamlessly allows human participation. This means that humans can provide input and feedback to the agents as needed.  For [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),  ```python from autogen import AssistantAgent, UserProxyAgent, config_list_from_json # Load LLM inference endpoints from an env variable or a file # See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints # and OAI_CONFIG_LIST_sample config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\") # You can also set config_list directly as a list, for example, config_list = [{'model': 'gpt-4', 'api_key': '<your OpenAI API key here>'},] assistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list}) user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}) # IMPORTANT: set to True to run code in docker, recommended user_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\") # This initiates an automated chat between the two agents to solve the task ```  This example can be run with  ```python python test/twoagent.py ```  After the repo is cloned. The figure below shows an example conversation flow with AutoGen. ![Agent Chat Example](https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png)  Alternatively, the [sample code](https://github.com/microsoft/autogen/blob/main/samples/simple_chat.py) here allows a user to chat with an AutoGen agent in ChatGPT style. Please find more [code examples](https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat) for this feature.  <p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">   <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">     \u2191 Back to Top \u2191   </a> </p> "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/README.md",
        "label": "autogen",
        "content": "Enhanced LLM Inferences  Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification) with powerful functionalities like caching, error handling, multi-config inference and templating.  <!-- For example, you can optimize generations by LLM with your own tuning data, success metrics, and budgets.  ```python # perform tuning for openai<1 config, analysis = autogen.Completion.tune(     data=tune_data,     metric=\"success\",     mode=\"max\",     eval_func=eval_func,     inference_budget=0.05,     optimization_budget=3,     num_samples=-1, ) # perform inference for a test instance response = autogen.Completion.create(context=test_instance, **config) ```  Please find more [code examples](https://microsoft.github.io/autogen/docs/Examples#tune-gpt-models) for this feature. -->  <p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">   <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">     \u2191 Back to Top \u2191   </a> </p> "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/README.md",
        "label": "autogen",
        "content": "Documentation  You can find detailed documentation about AutoGen [here](https://microsoft.github.io/autogen/).  In addition, you can find:  - [Research](https://microsoft.github.io/autogen/docs/Research), [blogposts](https://microsoft.github.io/autogen/blog) around AutoGen, and [Transparency FAQs](https://github.com/microsoft/autogen/blob/main/TRANSPARENCY_FAQS.md)  - [Discord](https://aka.ms/autogen-dc)  - [Contributing guide](https://microsoft.github.io/autogen/docs/Contribute)  - [Roadmap](https://github.com/orgs/microsoft/projects/989/views/3)  <p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">   <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">     \u2191 Back to Top \u2191   </a> </p> "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/README.md",
        "label": "autogen",
        "content": "Related Papers  [AutoGen](https://arxiv.org/abs/2308.08155)  ``` @inproceedings{wu2023autogen,       title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},       author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Hassan Awadallah and Ryen W White and Doug Burger and Chi Wang},       year={2023},       eprint={2308.08155},       archivePrefix={arXiv},       primaryClass={cs.AI} } ```  [EcoOptiGen](https://arxiv.org/abs/2303.04673)  ``` @inproceedings{wang2023EcoOptiGen,     title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},     author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},     year={2023},     booktitle={AutoML'23}, } ```  [MathChat](https://arxiv.org/abs/2306.01337)  ``` @inproceedings{wu2023empirical,     title={An Empirical Study on Challenging Math Problem Solving with GPT-4},     author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},     year={2023},     booktitle={ArXiv preprint arXiv:2306.01337}, } ```  [AgentOptimizer](https://arxiv.org/pdf/2402.11359)  ``` @article{zhang2024training,   title={Training Language Model Agents without Modifying Language Models},   author={Zhang, Shaokun and Zhang, Jieyu and Liu, Jiale and Song, Linxin and Wang, Chi and Krishna, Ranjay and Wu, Qingyun},   journal={ICML'24},   year={2024} } ```  [StateFlow](https://arxiv.org/abs/2403.11322) ``` @article{wu2024stateflow,   title={StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows},   author={Wu, Yiran and Yue, Tianwei and Zhang, Shaokun and Wang, Chi and Wu, Qingyun},   journal={arXiv preprint arXiv:2403.11322},   year={2024} } ```  <p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">   <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">     \u2191 Back to Top \u2191   </a> </p> "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/README.md",
        "label": "autogen",
        "content": "Contributing  This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.  If you are new to GitHub, [here](https://opensource.guide/how-to-contribute/#how-to-submit-a-contribution) is a detailed help source on getting involved with development on GitHub.  When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.  This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information, see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.  <p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">   <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">     \u2191 Back to Top \u2191   </a> </p> "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/README.md",
        "label": "autogen",
        "content": "Contributors Wall <a href=\"https://github.com/microsoft/autogen/graphs/contributors\">   <img src=\"https://contrib.rocks/image?repo=microsoft/autogen&max=204\" /> </a>  <p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">   <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">     \u2191 Back to Top \u2191   </a> </p>  # Legal Notices  Microsoft and any contributors grant you a license to the Microsoft documentation and other content in this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode), see the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the [LICENSE-CODE](LICENSE-CODE) file.  Microsoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries. The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks. Microsoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.  Privacy information can be found at https://privacy.microsoft.com/en-us/  Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents, or trademarks, whether by implication, estoppel, or otherwise.  <p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">   <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">     \u2191 Back to Top \u2191   </a> </p> "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/SECURITY.md",
        "label": "autogen",
        "content": "<!-- BEGIN MICROSOFT SECURITY.MD V0.0.8 BLOCK --> "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/SECURITY.md",
        "label": "autogen",
        "content": "Security  Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet), [Xamarin](https://github.com/xamarin), and [our GitHub organizations](https://opensource.microsoft.com/).  If you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://aka.ms/opensource/security/definition), please report it to us as described below. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/SECURITY.md",
        "label": "autogen",
        "content": "Reporting Security Issues  **Please do not report security vulnerabilities through public GitHub issues.**  Instead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://aka.ms/opensource/security/create-report).  If you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://aka.ms/opensource/security/pgpkey).  You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://aka.ms/opensource/security/msrc).  Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:    * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)   * Full paths of source file(s) related to the manifestation of the issue   * The location of the affected source code (tag/branch/commit or direct URL)   * Any special configuration required to reproduce the issue   * Step-by-step instructions to reproduce the issue   * Proof-of-concept or exploit code (if possible)   * Impact of the issue, including how an attacker might exploit the issue  This information will help us triage your report more quickly.  If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://aka.ms/opensource/security/bounty) page for more details about our active programs. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/SECURITY.md",
        "label": "autogen",
        "content": "Preferred Languages  We prefer all communications to be in English. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/SECURITY.md",
        "label": "autogen",
        "content": "Policy  Microsoft follows the principle of [Coordinated Vulnerability Disclosure](https://aka.ms/opensource/security/cvd).  <!-- END MICROSOFT SECURITY.MD BLOCK --> "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/TRANSPARENCY_FAQS.md",
        "label": "autogen",
        "content": "# AutoGen: Responsible AI FAQs "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/TRANSPARENCY_FAQS.md",
        "label": "autogen",
        "content": "What is AutoGen? AutoGen is a framework for simplifying the orchestration, optimization, and automation of LLM workflows. It offers customizable and conversable agents that leverage the strongest capabilities of the most advanced LLMs, like GPT-4, while addressing their limitations by integrating with humans and tools and having conversations between multiple agents via automated chat. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/TRANSPARENCY_FAQS.md",
        "label": "autogen",
        "content": "What can AutoGen do? AutoGen is an experimentational framework for building a complex multi-agent conversation system by: - Defining a set of agents with specialized capabilities and roles. -\tDefining the interaction behavior between agents, i.e., what to reply when an agent receives messages from another agent.  The agent conversation-centric design has numerous benefits, including that it: -\tNaturally handles ambiguity, feedback, progress, and collaboration. -\tEnables effective coding-related tasks, like tool use with back-and-forth troubleshooting. -\tAllows users to seamlessly opt in or opt out via an agent in the chat. -\tAchieves a collective goal with the cooperation of multiple specialists. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/TRANSPARENCY_FAQS.md",
        "label": "autogen",
        "content": "\tWhat is/are AutoGen\u2019s intended use(s)? Please note that AutoGen is an open-source library under active development and intended for  use for research purposes. It should not be used in any downstream applications without additional detailed evaluation of robustness, safety issues and assessment of any potential harm or bias in the proposed application.  AutoGen is a generic infrastructure that can be used in multiple scenarios. The system\u2019s intended uses include:  -\tBuilding LLM workflows that solve more complex tasks: Users can create agents that interleave reasoning and tool use capabilities of the latest LLMs such as GPT-4. To solve complex tasks, multiple agents can converse to work together (e.g., by partitioning a complex problem into simpler steps or by providing different viewpoints or perspectives). -\tApplication-specific agent topologies: Users can create application specific agent topologies and patterns for agents to interact. The exact topology may depend on the domain\u2019s complexity and semantic capabilities of the LLM available. -\tCode generation and execution: Users can implement agents that can assume the roles of writing code and other agents that can execute code. Agents can do this with varying levels of human involvement. Users can add more agents and program the conversations to enforce constraints on code and output. -\tQuestion answering: Users can create agents that can help answer questions using retrieval augmented generation. -\tEnd user and multi-agent chat and debate: Users can build chat applications where they converse with multiple agents at the same time.  While AutoGen automates LLM workflows, decisions about how to use specific LLM outputs should always have a human in the loop. For example, you should not use AutoGen to automatically post LLM generated content to social media. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/TRANSPARENCY_FAQS.md",
        "label": "autogen",
        "content": "How was AutoGen evaluated? What metrics are used to measure performance? -\tCurrent version of AutoGen was evaluated on six applications to illustrate its potential in simplifying the development of high-performance multi-agent applications. These applications are selected based on their real-world relevance,  problem difficulty and problem solving capabilities enabled by AutoGen, and innovative potential. -\tThese applications involve using AutoGen to solve math problems, question answering, decision making in text world environments, supply chain optimization, etc. For each of these domains AutoGen was evaluated on various success based metrics (i.e., how often the AutoGen based implementation solved the task). And, in some cases, AutoGen based approach was also evaluated on implementation efficiency (e.g., to track reductions in developer effort to build). More details can be found at: https://aka.ms/AutoGen/TechReport - The team has conducted tests where a \u201cred\u201d agent attempts to get the default AutoGen assistant to break from its alignment and guardrails. The team has observed that out of 70 attempts to break guardrails, only 1 was successful in producing text that would have been flagged as problematic by Azure OpenAI filters. The team has not observed any evidence that AutoGen (or GPT models as hosted by OpenAI or Azure) can produce novel code exploits or jailbreak prompts, since direct prompts to \u201cbe a hacker\u201d, \u201cwrite exploits\u201d, or \u201cproduce a phishing email\u201d are refused by existing filters. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/TRANSPARENCY_FAQS.md",
        "label": "autogen",
        "content": "What are the limitations of AutoGen? How can users minimize the impact of AutoGen\u2019s limitations when using the system? AutoGen relies on existing LLMs. Experimenting with AutoGen would retain common limitations of large language models; including:  - Data Biases: Large language models, trained on extensive data, can inadvertently carry biases present in the source data. Consequently, the models may generate outputs that could be potentially biased or unfair. -\tLack of Contextual Understanding: Despite their impressive capabilities in language understanding and generation, these models exhibit limited real-world understanding, resulting in potential inaccuracies or nonsensical responses. -\tLack of Transparency: Due to the complexity and size, large language models can act as `black boxes,' making it difficult to comprehend the rationale behind specific outputs or decisions. -\tContent Harms: There are various types of content harms that large language models can cause. It is important to be aware of them when using these models, and to take actions to prevent them. It is recommended to leverage various content moderation services provided by different companies and institutions. -\tInaccurate or ungrounded content: It is important to be aware and cautious not to entirely rely on a given language model for critical decisions or information that might have deep impact as it is not obvious how to prevent these models to fabricate content without high authority input sources. -\tPotential for Misuse: Without suitable safeguards, there is a risk that these models could be maliciously used for generating disinformation or harmful content.   Additionally, AutoGen\u2019s multi-agent framework may amplify or introduce additional risks, such as: -\tPrivacy and Data Protection: The framework allows for human participation in conversations between agents. It is important to ensure that user data and conversations are protected and that developers use appropriate measures to safeguard privacy. -\tAccountability and Transparency: The framework involves multiple agents conversing and collaborating, it is important to establish clear accountability and transparency mechanisms. Users should be able to understand and trace the decision-making process of the agents involved in order to ensure accountability and address any potential issues or biases. -\tTrust and reliance: The framework leverages human understanding and intelligence while providing automation through conversations between agents. It is important to consider the impact of this interaction on user experience, trust, and reliance on AI systems. Clear communication and user education about the capabilities and limitations of the system will be essential. -\tSecurity & unintended consequences: The use of multi-agent conversations and automation in complex tasks may have unintended consequences. Especially, allowing LLM agents to make changes in external environments through code execution or function calls, such as install packages, could pose significant risks. Developers should carefully consider the potential risks and ensure that appropriate safeguards are in place to prevent harm or negative outcomes, including keeping a human in the loop for decision making. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/TRANSPARENCY_FAQS.md",
        "label": "autogen",
        "content": "What operational factors and settings allow for effective and responsible use of AutoGen? -\tCode execution: AutoGen recommends using docker containers so that code execution can happen in a safer manner. Users can use function call instead of free-form code to execute pre-defined functions only. That helps increase the reliability and safety. Users can customize the code execution environment to tailor to their requirements. -\tHuman involvement: AutoGen prioritizes human involvement in multi agent conversation. The overseers can step in to give feedback to agents and steer them in the correct direction. By default, users get chance to confirm before code is executed. -\tAgent modularity: Modularity allows agents to have different levels of information access. Additional agents can assume roles that help keep other agents in check. For example, one can easily add a dedicated agent to play the role of safeguard. -\tLLMs: Users can choose the LLM that is optimized for responsible use. The default LLM is GPT-4 which inherits the existing RAI mechanisms and filters from the LLM provider. Caching is enabled by default to increase reliability and control cost. We encourage developers to review [OpenAI\u2019s Usage policies](https://openai.com/policies/usage-policies) and [Azure OpenAI\u2019s Code of Conduct](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/code-of-conduct) when using GPT-4. -\tMulti-agent setup: When using auto replies, the users can limit the number of auto replies, termination conditions etc. in the settings to increase reliability. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/CODE_OF_CONDUCT.md",
        "label": "autogen",
        "content": "# Microsoft Open Source Code of Conduct  This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).  Resources:  - [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/) - [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) - Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/auto-anny/README.md",
        "label": "autogen",
        "content": "<div align=\"center\">   <img src=\"images/icon.png\" alt=\"Repo Icon\" width=\"100\" height=\"100\"> </div>  # AutoAnny  AutoAnny is a Discord bot built using AutoGen to help with AutoGen's Discord server. Actually Anny can help with any OSS GitHub project (set `ANNY_GH_REPO` below). "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/auto-anny/README.md",
        "label": "autogen",
        "content": "Features  - **`/heyanny help`**: Lists commands. - **`/heyanny ghstatus`**: Summarizes GitHub activity. - **`/heyanny ghgrowth`**: Shows GitHub repo growth indicators. - **`/heyanny ghunattended`**: Lists unattended issues and PRs. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/auto-anny/README.md",
        "label": "autogen",
        "content": "Installation  1. Clone the AutoGen repository and `cd samples/apps/auto-anny` 2. Install dependencies: `pip install -r requirements.txt` 3. Export Discord token and GitHub API token,     ```     export OAI_CONFIG_LIST=your-autogen-config-list     export DISCORD_TOKEN=your-bot-token     export GH_TOKEN=your-gh-token     export ANNY_GH_REPO=microsoft/autogen  # you may choose a different repo name     ```     To get a Discord token, you will need to set up your Discord bot using these [instructions](https://discordpy.readthedocs.io/en/stable/discord.html). 4. Start the bot: `python bot.py`  Note: By default Anny will log data to `autoanny.log`.  "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/auto-anny/README.md",
        "label": "autogen",
        "content": "Roadmap  - Enable access control - Enable a richer set of commands - Enrich agents with tool use  "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/auto-anny/README.md",
        "label": "autogen",
        "content": "Contributing  Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/cap/TODO.md",
        "label": "autogen",
        "content": "- ~~Pretty print debug_logs~~   - ~~colors~~   - ~~messages to oai should be condensed~~ - ~~remove orchestrator in scenario 4 and have the two actors talk to each other~~ - ~~pass a complex multi-part message~~ - ~~protobuf for messages~~ - ~~make changes to autogen to enable scenario 3 to work with CAN~~ - ~~make groupchat work~~ - ~~actors instead of agents~~ - clean up for PR into autogen   - ~~Create folder structure under Autogen examples~~   - ~~CAN -> CAP (Composable Actor Protocol)~~ - CAP actor lookup should use zmq - Add min C# actors & reorganize - Hybrid GroupChat with C# ProductManager - C++ Msg Layer - Rust Msg Layer - Node Msg Layer - Java Msg Layer - Investigate a standard logging framework that supports color in windows   - structlog? "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/cap/README.md",
        "label": "autogen",
        "content": "# Composable Actor Platform (CAP) for AutoGen "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/cap/README.md",
        "label": "autogen",
        "content": "I just want to run the remote AutoGen agents! *Python Instructions (Windows, Linux, MacOS):*  0) cd py 1) pip install -r autogencap/requirements.txt 2) python ./demo/App.py 3) Choose (5) and follow instructions to run standalone Agents 4) Choose other options for other demos  *Demo Notes:* 1) Options involving AutoGen require OAI_CONFIG_LIST.    AutoGen python requirements: 3.8 <= python <= 3.11 2) For option 2, type something in and see who receives the message.  Quit to quit. 3) To view any option that displays a chart (such as option 4), you will need to disable Docker code execution. You can do this by setting the environment variable `AUTOGEN_USE_DOCKER` to `False`.  *Demo Reference:* ``` Select the Composable Actor Platform (CAP) demo app to run: (enter anything else to quit) 1. Hello World 2. Complex Agent (e.g. Name or Quit) 3. AutoGen Pair 4. AutoGen GroupChat 5. AutoGen Agents in different processes 6. List Actors in CAP (Registry) Enter your choice (1-6): ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/cap/README.md",
        "label": "autogen",
        "content": "What is Composable Actor Platform (CAP)? AutoGen is about Agents and Agent Orchestration.  CAP extends AutoGen to allows Agents to communicate via a message bus.  CAP, therefore, deals with the space between these components.  CAP is a message based, actor platform that allows actors to be composed into arbitrary graphs.  Actors can register themselves with CAP, find other agents, construct arbitrary graphs, send and receive messages independently and many, many, many other things. ```python     # CAP Platform     network = LocalActorNetwork()     # Register an agent     network.register(GreeterAgent())     # Tell agents to connect to other agents     network.connect()     # Get a channel to the agent     greeter_link = network.lookup_agent(\"Greeter\")     # Send a message to the agent     greeter_link.send_txt_msg(\"Hello World!\")     # Cleanup     greeter_link.close()     network.disconnect() ``` ### Check out other demos in the `py/demo` directory.  We show the following: ### 1) Hello World shown above 2) Many CAP Actors interacting with each other 3) A pair of interacting AutoGen Agents wrapped in CAP Actors 4) CAP wrapped AutoGen Agents in a group chat 5) Two AutoGen Agents running in different processes and communicating through CAP 6) List all registered agents in CAP ### Coming soon. Stay tuned! ### 1) AutoGen integration to list all registered agents "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/cap/py/README.md",
        "label": "autogen",
        "content": "# Composable Actor Platform (CAP) for AutoGen "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/cap/py/README.md",
        "label": "autogen",
        "content": "I just want to run the remote AutoGen agents! *Python Instructions (Windows, Linux, MacOS):*  pip install autogencap  1) AutoGen require OAI_CONFIG_LIST.    AutoGen python requirements: 3.8 <= python <= 3.11  ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/cap/py/README.md",
        "label": "autogen",
        "content": "What is Composable Actor Platform (CAP)? AutoGen is about Agents and Agent Orchestration.  CAP extends AutoGen to allows Agents to communicate via a message bus.  CAP, therefore, deals with the space between these components.  CAP is a message based, actor platform that allows actors to be composed into arbitrary graphs.  Actors can register themselves with CAP, find other agents, construct arbitrary graphs, send and receive messages independently and many, many, many other things.  ```python # CAP Library from autogencap.ComponentEnsemble import ComponentEnsemble from autogencap.Actor import Actor  # A simple Agent class GreeterAgent(Actor):     def __init__(self):         super().__init__(             agent_name=\"Greeter\",             description=\"This is the greeter agent, who knows how to greet people.\")      # Prints out the message it receives     def on_txt_msg(self, msg):         print(f\"Greeter received: {msg}\")         return True  ensemble = ComponentEnsemble() # Create an agent agent = GreeterAgent() # Register an agent ensemble.register(agent) # start message processing # call on_connect() on all Agents ensemble.connect() # Get a channel to the agent greeter_link = ensemble.find_by_name(\"Greeter\") #Send a message to the agent greeter_link.send_txt_msg(\"Hello World!\") # Cleanup greeter_link.close() ensemble.disconnect() ```  ### Check out other demos in the `py/demo` directory.  We show the following: ### 1) Hello World shown above 2) Many CAP Actors interacting with each other 3) A pair of interacting AutoGen Agents wrapped in CAP Actors 4) CAP wrapped AutoGen Agents in a group chat 5) Two AutoGen Agents running in different processes and communicating through CAP 6) List all registered agents in CAP 7) Run Agent in user supplied message loop "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/cap/c++/Readme.md",
        "label": "autogen",
        "content": "Coming soon... "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/cap/node/Readme.md",
        "label": "autogen",
        "content": "Coming soon... "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/cap/c#/Readme.md",
        "label": "autogen",
        "content": "Coming soon... "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/autogen-studio/README.md",
        "label": "autogen",
        "content": "# AutoGen Studio  [![PyPI version](https://badge.fury.io/py/autogenstudio.svg)](https://badge.fury.io/py/autogenstudio) [![Downloads](https://static.pepy.tech/badge/autogenstudio/week)](https://pepy.tech/project/autogenstudio)  ![ARA](./docs/ara_stockprices.png)  AutoGen Studio is an AutoGen-powered AI app (user interface) to help you rapidly prototype AI agents, enhance them with skills, compose them into workflows and interact with them to accomplish tasks. It is built on top of the [AutoGen](https://microsoft.github.io/autogen) framework, which is a toolkit for building AI agents.  Code for AutoGen Studio is on GitHub at [microsoft/autogen](https://github.com/microsoft/autogen/tree/main/samples/apps/autogen-studio)  > **Note**: AutoGen Studio is meant to help you rapidly prototype multi-agent workflows and demonstrate an example of end user interfaces built with AutoGen. It is not meant to be a production-ready app.  > [!WARNING] > AutoGen Studio is currently under active development and we are iterating quickly. Kindly consider that we may introduce breaking changes in the releases during the upcoming weeks, and also the `README` might be outdated. Please see the AutoGen Studio [docs](https://microsoft.github.io/autogen/docs/autogen-studio/getting-started) page for the most up-to-date information.  **Updates**  > April 17: AutoGen Studio database layer is now rewritten to use [SQLModel](https://sqlmodel.tiangolo.com/) (Pydantic + SQLAlchemy). This provides entity linking (skills, models, agents and workflows are linked via association tables) and supports multiple [database backend dialects](https://docs.sqlalchemy.org/en/20/dialects/) supported in SQLAlchemy (SQLite, PostgreSQL, MySQL, Oracle, Microsoft SQL Server). The backend database can be specified a `--database-uri` argument when running the application. For example, `autogenstudio ui --database-uri sqlite:///database.sqlite` for SQLite and `autogenstudio ui --database-uri postgresql+psycopg://user:password@localhost/dbname` for PostgreSQL.  > March 12: Default directory for AutoGen Studio is now /home/<user>/.autogenstudio. You can also specify this directory using the `--appdir` argument when running the application. For example, `autogenstudio ui --appdir /path/to/folder`. This will store the database and other files in the specified directory e.g. `/path/to/folder/database.sqlite`. `.env` files in that directory will be used to set environment variables for the app.  Project Structure:  - _autogenstudio/_ code for the backend classes and web api (FastAPI) - _frontend/_ code for the webui, built with Gatsby and TailwindCSS  ### Installation  There are two ways to install AutoGen Studio - from PyPi or from source. We **recommend installing from PyPi** unless you plan to modify the source code.  1.  **Install from PyPi**      We recommend using a virtual environment (e.g., conda) to avoid conflicts with existing Python packages. With Python 3.10 or newer active in your virtual environment, use pip to install AutoGen Studio:      ```bash     pip install autogenstudio     ```  2.  **Install from Source**      > Note: This approach requires some familiarity with building interfaces in React.      If you prefer to install from source, ensure you have Python 3.10+ and Node.js (version above 14.15.0) installed. Here's how you get started:      - Clone the AutoGen Studio repository and install its Python dependencies:        ```bash       pip install -e .       ```      - Navigate to the `samples/apps/autogen-studio/frontend` directory, install dependencies, and build the UI:        ```bash       npm install -g gatsby-cli       npm install --global yarn       cd frontend       yarn install       yarn build       ```  For Windows users, to build the frontend, you may need alternative commands to build the frontend.  ```bash    gatsby clean && rmdir /s /q ..\\\\autogenstudio\\\\web\\\\ui 2>nul & (set \\\"PREFIX_PATH_VALUE=\\\" || ver>nul) && gatsby build --prefix-paths && xcopy /E /I /Y public ..\\\\autogenstudio\\\\web\\\\ui  ```  ### Running the Application  Once installed, run the web UI by entering the following in your terminal:  ```bash autogenstudio ui --port 8081 ```  This will start the application on the specified port. Open your web browser and go to `http://localhost:8081/` to begin using AutoGen Studio.  AutoGen Studio also takes several parameters to customize the application:  - `--host <host>` argument to specify the host address. By default, it is set to `localhost`. Y - `--appdir <appdir>` argument to specify the directory where the app files (e.g., database and generated user files) are stored. By default, it is set to the a `.autogenstudio` directory in the user's home directory. - `--port <port>` argument to specify the port number. By default, it is set to `8080`. - `--reload` argument to enable auto-reloading of the server when changes are made to the code. By default, it is set to `False`. - `--database-uri` argument to specify the database URI. Example values include `sqlite:///database.sqlite` for SQLite and `postgresql+psycopg://user:password@localhost/dbname` for PostgreSQL. If this is not specified, the database URIL defaults to a `database.sqlite` file in the `--appdir` directory.  Now that you have AutoGen Studio installed and running, you are ready to explore its capabilities, including defining and modifying agent workflows, interacting with agents and sessions, and expanding agent skills. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/autogen-studio/README.md",
        "label": "autogen",
        "content": "Contribution Guide  We welcome contributions to AutoGen Studio. We recommend the following general steps to contribute to the project:  - Review the overall AutoGen project [contribution guide](https://github.com/microsoft/autogen?tab=readme-ov-file#contributing) - Please review the AutoGen Studio [roadmap](https://github.com/microsoft/autogen/issues/737) to get a sense of the current priorities for the project. Help is appreciated especially with Studio issues tagged with `help-wanted` - Please initiate a discussion on the roadmap issue or a new issue to discuss your proposed contribution. - Please review the autogenstudio dev branch here [dev branch](https://github.com/microsoft/autogen/tree/autogenstudio) and use as a base for your contribution. This way, your contribution will be aligned with the latest changes in the AutoGen Studio project. - Submit a pull request with your contribution! - If you are modifying AutoGen Studio, it has its own devcontainer. See instructions in `.devcontainer/README.md` to use it - Please use the tag `studio` for any issues, questions, and PRs related to Studio "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/autogen-studio/README.md",
        "label": "autogen",
        "content": "FAQ  Please refer to the AutoGen Studio [FAQs](https://microsoft.github.io/autogen/docs/autogen-studio/faqs) page for more information. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/autogen-studio/README.md",
        "label": "autogen",
        "content": "Acknowledgements  AutoGen Studio is Based on the [AutoGen](https://microsoft.github.io/autogen) project. It was adapted from a research prototype built in October 2023 (original credits: Gagan Bansal, Adam Fourney, Victor Dibia, Piali Choudhury, Saleema Amershi, Ahmed Awadallah, Chi Wang). "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/autogen-studio/frontend/README.md",
        "label": "autogen",
        "content": "## \ud83d\ude80 Running UI in Dev Mode  Run the UI in dev mode (make changes and see them reflected in the browser with hotreloading):  - npm install - npm run start  This should start the server on port 8000. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/autogen-studio/frontend/README.md",
        "label": "autogen",
        "content": "Design Elements  - **Gatsby**: The app is created in Gatsby. A guide on bootstrapping a Gatsby app can be found here - https://www.gatsbyjs.com/docs/quick-start/.   This provides an overview of the project file structure include functionality of files like `gatsby-config.js`, `gatsby-node.js`, `gatsby-browser.js` and `gatsby-ssr.js`. - **TailwindCSS**: The app uses TailwindCSS for styling. A guide on using TailwindCSS with Gatsby can be found here - https://tailwindcss.com/docs/guides/gatsby.https://tailwindcss.com/docs/guides/gatsby . This will explain the functionality in tailwind.config.js and postcss.config.js. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/autogen-studio/frontend/README.md",
        "label": "autogen",
        "content": "Modifying the UI, Adding Pages  The core of the app can be found in the `src` folder. To add pages, add a new folder in `src/pages` and add a `index.js` file. This will be the entry point for the page. For example to add a route in the app like `/about`, add a folder `about` in `src/pages` and add a `index.tsx` file. You can follow the content style in `src/pages/index.tsx` to add content to the page.  Core logic for each component should be written in the `src/components` folder and then imported in pages as needed. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/autogen-studio/frontend/README.md",
        "label": "autogen",
        "content": "connecting to front end  the front end makes request to the backend api and expects it at /api on localhost port 8081 "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/autogen-studio/frontend/README.md",
        "label": "autogen",
        "content": "setting env variables for the UI  - please look at `.env.default` - make a copy of this file and name it `.env.development` - set the values for the variables in this file   - The main variable here is `GATSBY_API_URL` which should be set to `http://localhost:8081/api` for local development. This tells the UI where to make requests to the backend. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/promptflow-autogen/README.md",
        "label": "autogen",
        "content": "# What is Promptflow  Promptflow is a comprehensive suite of tools that simplifies the development, testing, evaluation, and deployment of LLM based AI applications. It also supports integration with Azure AI for cloud-based operations and is designed to streamline end-to-end development.  Refer to [Promptflow docs](https://microsoft.github.io/promptflow/) for more information.  Quick links:  - Why use Promptflow - [Link](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/overview-what-is-prompt-flow) - Quick start guide - [Link](https://microsoft.github.io/promptflow/how-to-guides/quick-start.html) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/promptflow-autogen/README.md",
        "label": "autogen",
        "content": "Getting Started  - Install required python packages    ```bash   cd samples/apps/promptflow-autogen   pip install -r requirements.txt   ```  - This example assumes a working Redis cache service to be available. You can get started locally using this [guide](https://redis.io/docs/latest/operate/oss_and_stack/install/install-redis/) or use your favorite managed service "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/promptflow-autogen/README.md",
        "label": "autogen",
        "content": "Chat flow  Chat flow is designed for conversational application development, building upon the capabilities of standard flow and providing enhanced support for chat inputs/outputs and chat history management. With chat flow, you can easily create a chatbot that handles chat input and output. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/promptflow-autogen/README.md",
        "label": "autogen",
        "content": "Create connection for LLM tool to use  You can follow these steps to create a connection required by a LLM tool.  Currently, there are two connection types supported by LLM tool: \"AzureOpenAI\" and \"OpenAI\". If you want to use \"AzureOpenAI\" connection type, you need to create an Azure OpenAI service first. Please refer to [Azure OpenAI Service](https://azure.microsoft.com/en-us/products/cognitive-services/openai-service/) for more details. If you want to use \"OpenAI\" connection type, you need to create an OpenAI account first. Please refer to [OpenAI](https://platform.openai.com/) for more details.  ```bash # Override keys with --set to avoid yaml file changes  # Create Azure open ai connection pf connection create --file azure_openai.yaml --set api_key=<your_api_key> api_base=<your_api_base> --name open_ai_connection  # Create the custom connection for Redis Cache pf connection create -f custom_conn.yaml --set secrets.redis_url=<your-redis-connection-url> --name redis_connection_url # Sample redis connection string rediss://:PASSWORD@redis_host_name.redis.cache.windows.net:6380/0 ```  Note in [flow.dag.yaml](flow.dag.yaml) we are using connection named `aoai_connection` for Azure Open AI and `redis_connection_url` for redis.  ```bash # show registered connection pf connection show --name open_ai_connection ```  Please refer to connections [document](https://promptflow.azurewebsites.net/community/local/manage-connections.html) and [example](https://github.com/microsoft/promptflow/tree/main/examples/connections) for more details. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/promptflow-autogen/README.md",
        "label": "autogen",
        "content": "Develop a chat flow  The most important elements that differentiate a chat flow from a standard flow are **Chat Input**, **Chat History**, and **Chat Output**.  - **Chat Input**: Chat input refers to the messages or queries submitted by users to the chatbot. Effectively handling chat input is crucial for a successful conversation, as it involves understanding user intentions, extracting relevant information, and triggering appropriate responses.  - **Chat History**: Chat history is the record of all interactions between the user and the chatbot, including both user inputs and AI-generated outputs. Maintaining chat history is essential for keeping track of the conversation context and ensuring the AI can generate contextually relevant responses. Chat History is a special type of chat flow input, that stores chat messages in a structured format.    - NOTE: Currently the sample flows do not send chat history messages to agent workflow.  - **Chat Output**: Chat output refers to the AI-generated messages that are sent to the user in response to their inputs. Generating contextually appropriate and engaging chat outputs is vital for a positive user experience.  A chat flow can have multiple inputs, but Chat History and Chat Input are required inputs in chat flow. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/promptflow-autogen/README.md",
        "label": "autogen",
        "content": "Interact with chat flow  Promptflow supports interacting via vscode or via Promptflow CLI provides a way to start an interactive chat session for chat flow. Customer can use below command to start an interactive chat session:  ```bash pf flow test --flow <flow_folder> --interactive ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/promptflow-autogen/README.md",
        "label": "autogen",
        "content": "Autogen State Flow  [Autogen State Flow](./autogen_stateflow.py) contains stateflow example shared at [StateFlow](https://microsoft.github.io/autogen/blog/2024/02/29/StateFlow/) with Promptflow. All the interim messages are sent to Redis channel. You can use these to stream to frontend or take further actions. Output of Prompflow is `summary` message from group chat. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/promptflow-autogen/README.md",
        "label": "autogen",
        "content": "Agent Nested Chat  [Autogen Nested Chat](./agentchat_nestedchat.py) contains Scenario 1 of nested chat example shared at [Nested Chats](https://microsoft.github.io/autogen/docs/notebooks/agentchat_nestedchat) with Promptflow. All the interim messages are sent to Redis channel. You can use these to stream to frontend or take further actions. Output of Prompflow is `summary` message from group chat. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/promptflow-autogen/README.md",
        "label": "autogen",
        "content": "Redis for Data cache and Interim Messages  Autogen supports Redis for [data caching](https://microsoft.github.io/autogen/docs/reference/cache/redis_cache/) and since redis supports a pub-subs model as well, this Promptflow example is configured for all agent callbacks to send messages to a Redis channel. This is optional feature but is essential for long running workflows and provides access to interim messages for your frontend. NOTE: Currently Promtpflow only support [SSE](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events) for streaming data and does not support websockets. NOTE: In multi user chat bot environment please make necessary changes to send messages to corresponding channel. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/websockets/README.md",
        "label": "autogen",
        "content": "# Using websockets with FastAPI and AutoGen "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/apps/websockets/README.md",
        "label": "autogen",
        "content": "Running the example  1. Navigate to the directory containing the example:     ```     cd samples/apps/websockets     ```  2. Install the necessary dependencies:     ```     ./setup.py     ```  3. Run the application:     ```     uvicorn application:app --reload     ```  You should now be able to access the application in your web browser at `http://localhost:8000`. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/finetuning/README.md",
        "label": "autogen",
        "content": "# Tools for fine-tuning the local models that power agents  This directory aims to contain tools for fine-tuning the local models that power agents. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/finetuning/README.md",
        "label": "autogen",
        "content": "Fine tune a custom model client  AutoGen supports the use of custom models to power agents [see blog post here](https://microsoft.github.io/autogen/blog/2024/01/26/Custom-Models). This directory contains a tool to provide feedback to that model, that can be used to fine-tune the model.  The creator of the Custom Model Client will have to decide what kind of data is going to be fed back and how it will be used to fine-tune the model. This tool is designed to be flexible and allow for a wide variety of feedback mechanisms.  Custom Model Client will have follow the protocol client defined in `update_model.py` `UpdateableModelClient` which is a subclass of `ModelClient` and adds the following method:  ```python def update_model(     self, preference_data: List[Dict[str, Any]], inference_messages: List[Dict[str, Any]], **kwargs: Any ) -> Dict[str, Any]:     \"\"\"Optional method to learn from the preference data, if the model supports learning. Can be omitted.      Learn from the preference data.      Args:         preference_data: The preference data.         inference_messages: The messages that were used during inference between the agent that is being updated and another agent.         **kwargs: other arguments.      Returns:         Dict of learning stats.     \"\"\" ```  The function provided in the file `update_model.py` is called by passing these arguments:  - the agent whose model is to be updated - the preference data - the agent whose conversation is being used to provide the inference messages  The function will find the conversation thread that occurred between the \"update agent\" and the \"other agent\", and call the `update_model` method of the model client. It will return a dictionary containing the update stats, inference messages, and preference data:  ```python {     \"update_stats\": <the dictionary returned by the custom model client implementation>,     \"inference_messages\": <message used for inference>,     \"preference_data\": <the preference data passed in when update_model was called> } ```  **NOTES**:  `inference_messages` will contain messages that were passed into the custom model client when `create` was called and a response was needed from the model. It is up to the author of the custom model client to decide which parts of the conversation are needed and how to use this data to fine-tune the model.  If a conversation has been long-running before `update_model` is called, then the `inference_messages` will contain a conversation thread that was used for multiple inference steps. It is again up to the author of the custom model client to decide which parts of the conversation correspond to the preference data and how to use this data to fine-tune the model.  An example of how to use this tool is shown below:  ```python from finetuning.update_model import update_model  assistant = AssistantAgent(     \"assistant\",     system_message=\"You are a helpful assistant.\",     human_input_mode=\"NEVER\",     llm_config={         \"config_list\": [<the config list containing the custom model>],     }, )  assistant.register_model_client(model_client_cls=<TheCustomModelClientClass>)  user_proxy = UserProxyAgent(     \"user_proxy\",     human_input_mode=\"NEVER\",     max_consecutive_auto_reply=1,     code_execution_config=False,     llm_config=False, )  res = user_proxy.initiate_chat(assistant, message=\"the message\") response_content = res.summary  # Evaluate the summary here and provide feedback. Pretending I am going to perform DPO on the response.  # preference_data will be passed on as-is to the custom model client's update_model implementation # so it should be in the format that the custom model client expects and is completely up to the author of the custom model client preference_data = [(\"this is what the response should have been like\", response_content)]  update_model_stats = update_model(assistant, preference_data, user_proxy) ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/webarena/README.md",
        "label": "autogen",
        "content": "# WebArena Benchmark  This directory helps run AutoGen agents on the [WebArena](https://arxiv.org/pdf/2307.13854.pdf) benchmark. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/webarena/README.md",
        "label": "autogen",
        "content": "Installing WebArena  WebArena can be installed by following the instructions from [WebArena's GitHub repository](git@github.com:web-arena-x/webarena.git)  If using WebArena with AutoGen there is a clash on the versions of OpenAI and some code changes are needed in WebArena to be compatible with AutoGen's OpenAI version:  - webarena's openai version is `openai==0.27.0` - autogen's openai version is: `openai>=1.3`  Prior to installation, in the WebArena codebase, any file containing `openai.error` needs to be replaced with `openai`. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/webarena/README.md",
        "label": "autogen",
        "content": "Running with AutoGen agents  You can use the `run.py` file in the `webarena` directory to run WebArena with AutoGen. The OpenAI (or AzureOpenAI or other model) configuration can be setup via `OAI_CONFIG_LIST`. The config list will be filtered by whatever model is passed in the `--model` argument.  e.g. of running `run.py`:  ``` mkdir myresultdir python run.py --instruction_path agent/prompts/jsons/p_cot_id_actree_2s.json --test_start_idx 27 --test_end_idx 28 --model gpt-4 --result_dir myresultdir ```  The original `run.py` file has been modified to use AutoGen agents which are defined in the `webarena_agents.py` file. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/webarena/README.md",
        "label": "autogen",
        "content": "References **WebArena: A Realistic Web Environment for Building Autonomous Agents**<br/> Zhou, Shuyan and Xu, Frank F and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Bisk, Yonatan and Fried, Daniel and Alon, Uri and others<br/> [https://arxiv.org/pdf/2307.13854.pdf](https://arxiv.org/pdf/2307.13854.pdf) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/CONTRIBUTING.md",
        "label": "autogen",
        "content": "# Contributing to AutoGenBench  As part of the broader AutoGen project, AutoGenBench welcomes community contributions. Contributions are subject to AutoGen's [contribution guidelines](https://microsoft.github.io/autogen/docs/Contribute), as well as a few additional AutoGenBench-specific requirements outlined here. You may also wish to develop your own private benchmark scenarios and the guidance in this document will help with such efforts as well. Below you will find the general requirements, followed by a detailed technical description. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/CONTRIBUTING.md",
        "label": "autogen",
        "content": "General Contribution Requirements We ask that all contributions to AutoGenBench adhere to the following:  - Follow AutoGen's broader [contribution guidelines](https://microsoft.github.io/autogen/docs/Contribute) - All AutoGenBench benchmarks should live in a subfolder of `/samples/tools/autogenbench/scenarios` alongside `HumanEval`, `GAIA`, etc. - Benchmark scenarios should include a detailed README.md, in the root of their folder, describing the benchmark and providing citations where warranted. - Benchmark data (tasks, ground truth, etc.) should be downloaded from their original sources rather than hosted in the AutoGen repository (unless the benchmark is original, and the repository *is* the original source)     - You can use the `Scripts/init_tasks.py` file to automate this download. - Basic scoring should be compatible with the `autogenbench tabulate` command (e.g., by outputting logs compatible with the default tabulation mechanism, or by providing a `Scripts/custom_tabulate.py` file) - If you wish your benchmark to be compatible with the `autogenbench clone` command, include a `MANIFEST.json` file in the root of your folder.  These requirements are further detailed below, but if you simply copy the `HumanEval` folder, you will already be off to a great start. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/CONTRIBUTING.md",
        "label": "autogen",
        "content": "Implementing and Running Benchmark Tasks At the core of any benchmark is a set of tasks. To implement tasks that are runnable by AutoGenBench, you must adhere to AutoGenBench's templating and scenario expansion algorithms, as outlined below.  ### Task Definitions  All tasks are stored in JSONL files (in subdirectories under `./Tasks`). Each line of a tasks file is a JSON object with the following schema:  ``` {    \"id\": string,    \"template\": dirname,    \"substitutions\" {        \"filename1\": {        \t   \"find_string1_1\": replace_string1_1,            \"find_string1_2\": replace_string1_2,            ...            \"find_string1_M\": replace_string1_N        }        \"filename2\": {        \t   \"find_string2_1\": replace_string2_1,            \"find_string2_2\": replace_string2_2,            ...            \"find_string2_N\": replace_string2_N        }    } } ```  For example:  ``` {     \"id\": \"two_agent_stocks_gpt4\",     \"template\": \"default_two_agents\",     \"substitutions\": { \t\"scenario.py\": {             \"__MODEL__\": \"gpt-4\", \t}, \t\"prompt.txt\": {             \"__PROMPT__\": \"Plot and save to disk a chart of NVDA and TESLA stock price YTD.\"         }     } } ```  In this example, the string `__MODEL__` will be replaced in the file `scenarios.py`, while the string `__PROMPT__` will be replaced in the `prompt.txt` file.  The `template` field can also take on a list value, but this usage is considered advanced and is not described here. See the `autogenbench/run_cmd.py` code, or the `GAIA` benchmark tasks files for additional information about this option.  "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/CONTRIBUTING.md",
        "label": "autogen",
        "content": "Task Instance Expansion Algorithm  Once the tasks have been defined, as per above, they must be \"instantiated\" before they can be run. This instantiation happens automatically when the user issues the `autogenbench run` command and involves creating a local folder to share with Docker. Each instance and repetition gets its own folder along the path: `./results/[scenario]/[task_id]/[instance_id]`. For the sake of brevity we will refer to this folder as the `DEST_FOLDER`.  The algorithm for populating the `DEST_FOLDER` is as follows:  1. Pre-populate DEST_FOLDER with all the basic starter files for running a scenario (found in `autogenbench/template`). 2. Recursively copy the template folder specified in the JSONL line to DEST_FOLDER (if the JSON `template` attribute points to a folder) If the JSONs `template` attribute instead points to a file, copy the file, but rename it to `scenario.py` 3. Apply any string replacements, as outlined in the prior section. 4. Write a run.sh file to DEST_FOLDER that will be executed by Docker when it is loaded. The `run.sh` is described below. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/CONTRIBUTING.md",
        "label": "autogen",
        "content": "Scenario Execution Algorithm  Once the task has been instantiated it is run (via run.sh). This script will execute the following steps:  1. If a file named `global_init.sh` is present, run it. 2. If a file named `scenario_init.sh` is present, run it. 3. Install the requirements.txt file (if running in Docker) 4. Run the task via `python scenario.py` 5. If the scenario.py exited cleanly (exit code 0), then print \"SCENARIO.PY COMPLETE !#!#\" 6. Clean up (delete cache, etc.) 7. If a file named `scenario_finalize.sh` is present, run it. 8. If a file named `global_finalize.sh` is present, run it. 9. echo \"RUN COMPLETE !#!#\", signaling that all steps completed.  Notably, this means that scenarios can add custom init and teardown logic by including `scenario_init.sh` and `scenario_finalize.sh` files.  At the time of this writing, the run.sh file is as follows:  ```sh export AUTOGEN_TESTBED_SETTING=\"Docker\" umask 000  # Run the global init script if it exists if [ -f global_init.sh ] ; then     . ./global_init.sh fi  # Run the scenario init script if it exists if [ -f scenario_init.sh ] ; then     . ./scenario_init.sh fi  # Run the scenario pip install -r requirements.txt python scenario.py EXIT_CODE=$? if [ $EXIT_CODE -ne 0 ]; then     echo SCENARIO.PY EXITED WITH CODE: $EXIT_CODE !#!# else     echo SCENARIO.PY COMPLETE !#!# fi  # Clean up if [ -d .cache ] ; then     rm -Rf .cache fi  # Run the scenario finalize script if it exists if [ -f scenario_finalize.sh ] ; then     . ./scenario_finalize.sh fi  # Run the global finalize script if it exists if [ -f global_finalize.sh ] ; then     . ./global_finalize.sh fi  echo RUN.SH COMPLETE !#!# ```  Be warned that this listing is provided here for illustration purposes, and may vary over time. The source of truth are the `run.sh` files found in the ``./results/[taskset]/[task_id]/[instance_id]`` folders.  "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/CONTRIBUTING.md",
        "label": "autogen",
        "content": "Integrating with the `tabulate` and `clone` commands.  The above details are sufficient for defining and running tasks, but if you wish to support the `autogenbench tabulate` and `autogenbench clone` commands, a few additional steps are required.  ### Tabulations  If you wish to leverage the default tabulation logic, it is as simple as arranging your `scenario.py` file to output the string \"ALL TESTS PASSED !#!#\" to the console in the event that a task was solved correctly.  If you wish to implement your own tabulation logic, simply create the file `Scripts/custom_tabulate.py` and include a `main(args)` method. Here, the `args` parameter will be provided by AutoGenBench, and is a drop-in replacement for `sys.argv`. In particular, `args[0]` will be the invocation command (similar to the executable or script name in `sys.argv`), and the remaining values (`args[1:]`) are the command line parameters.  Should you provide a custom tabulation script, please implement `--help` and `-h` options for documenting your interface.  The `scenarios/GAIA/Scripts/custom_tabulate.py` is a great example of custom tabulation. It also shows how you can reuse some components of the default tabulator to speed up development.   ### Cloning  If you wish your benchmark to be available via the `autogenbench clone` command, you will need to take three additional steps:  #### Manifest First, provide a `MANIFEST.json` file in the root of your benchmark. An example is provided below, from which you can see the schema:  ```json {     \"files\": {         \"Templates/TwoAgents/prompt.txt\": \"Templates/TwoAgents/prompt.txt\",         \"Templates/TwoAgents/coding/my_tests.py\": \"Templates/TwoAgents/coding/my_tests.py\",         \"Templates/TwoAgents/scenario.py\": \"Templates/TwoAgents/scenario.py\",         \"README.md\": \"README.md\", \t\"Scripts/init_tasks.py\": \"Scripts/init_tasks.py\", \t\"Scripts/custom_tabulate.py\": \"Scripts/custom_tabulate.py\"     } } ```  The keys of the `files` dictionary are local paths, relative to your benchmark's root directory. The values are relative paths in the AutoGen GitHub repository (relative to the folder where the MANIFEST.json file is located). In most cases, the keys and values will be identical.  #### SCENARIOS dictionary Second, you must add an entry to the `scenarios` dictionary in `autogen/samples/tools/autogenbench/scenarios/MANIFEST.json`.  #### Scripts/init_tasks.py Finally, you should provide an `Scripts/init_tasks.py` file, in your benchmark folder, and include a `main()` method therein. This method will be loaded and called automatically by `autogenbench clone` after all manifest files have been downloaded.  This `init_tasks.py` script is a great place to download benchmarks from their original sources and convert them to the JSONL format required by AutoGenBench: - See `HumanEval/Scripts/init_tasks.py` for an example of how to expand a benchmark from an original GitHub repository. - See `GAIA/Scripts/init_tasks.py` for an example of how to expand a benchmark from `Hugging Face Hub`. - See `MATH/SCripts/init_tasks.py` for an example of how to expand a benchmark from an author-hosted website. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/README.md",
        "label": "autogen",
        "content": "# AutoGenBench  AutoGenBench is a tool for repeatedly running a set of pre-defined AutoGen tasks in a setting with tightly-controlled initial conditions. With each run, AutoGenBench will start from a blank slate. The agents being evaluated will need to work out what code needs to be written, and what libraries or dependencies to install, to solve tasks. The results of each run are logged, and can be ingested by analysis or metrics scripts (such as `autogenbench tabulate`). By default, all runs are conducted in freshly-initialized docker containers, providing the recommended level of consistency and safety.  AutoGenBench works with all AutoGen 0.1.*, and 0.2.* versions. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/README.md",
        "label": "autogen",
        "content": "Technical Specifications  If you are already an AutoGenBench pro, and want the full technical specifications, please review the [contributor's guide](CONTRIBUTING.md).  "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/README.md",
        "label": "autogen",
        "content": "Docker Requirement AutoGenBench also requires Docker (Desktop or Engine). **It will not run in GitHub codespaces**, unless you opt for native execution (with is strongly discouraged). To install Docker Desktop see [https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/). "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/README.md",
        "label": "autogen",
        "content": "Installation and Setup  **To get the most out of AutoGenBench, the `autogenbench` package should be installed**. At present, the easiest way to do this is to install it via `pip`:  ``` pip install autogenbench ```  If you would prefer working from source code (e.g., for development, or to utilize an alternate branch), simply clone the [AutoGen](https://github.com/microsoft/autogen) repository, then install `autogenbench` via:  ``` pip install -e autogen/samples/tools/autogenbench ```  After installation, you must configure your API keys. As with other AutoGen applications, AutoGenBench will look for the OpenAI keys in the OAI_CONFIG_LIST file in the current working directory, or the OAI_CONFIG_LIST environment variable. This behavior can be overridden using a command-line parameter described later.  If you will be running multiple benchmarks, it is often most convenient to leverage the environment variable option. You can load your keys into the environment variable by executing:  ``` export OAI_CONFIG_LIST=$(cat ./OAI_CONFIG_LIST) ```  If an OAI_CONFIG_LIST is *not* provided (by means of file or environment variable), AutoGenBench will use the OPENAI_API_KEY environment variable instead.   For some benchmark scenarios, additional keys may be required (e.g., keys for the Bing Search API). These can be added to an `ENV.json` file in the current working folder. An example `ENV.json` file is provided below:  ``` {     \"BING_API_KEY\": \"xxxyyyzzz\" } ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/README.md",
        "label": "autogen",
        "content": "A Typical Session Once AutoGenBench and necessary keys are installed, a typical session will look as follows:  ``` autogenbench clone HumanEval cd HumanEval autogenbench run Tasks/r_human_eval_two_agents.jsonl autogenbench tabulate results/r_human_eval_two_agents ```  Where: - `autogenbench clone HumanEval` downloads and expands the HumanEval benchmark scenario. - `autogenbench run Tasks/r_human_eval_two_agents.jsonl` runs the tasks defined in `Tasks/r_human_eval_two_agents.jsonl` - `autogenbench tablue results/r_human_eval_two_agents` tabulates the results of the run  Each of these commands has extensive in-line help via:  - `autogenbench --help` - `autogenbench clone --help` - `autogenbench run --help` - `autogenbench tabulate --help`  **NOTE:** If you are running `autogenbench` from within the repository, you don\u2019t need to run `autogenbench clone`. Instead, navigate to the appropriate scenario folder (e.g., `scenarios/HumanEval`) and run the `Scripts/init_tasks.py` file.  More details of each command are provided in the sections that follow. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/README.md",
        "label": "autogen",
        "content": "Cloning Benchmarks To clone an existing benchmark, simply run: ``` autogenbench clone [BENCHMARK] ```  For example,  ``` autogenbench clone HumanEval ```  To see which existing benchmarks are available to clone, run:  ``` autogenbench clone --list ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/README.md",
        "label": "autogen",
        "content": "Running AutoGenBench  To run a benchmark (which executes the tasks, but does not compute metrics), simply execute: ``` cd [BENCHMARK] autogenbench run Tasks ```  For example, ``` cd HumanEval autogenbench run Tasks ```  The default is to run each task once. To run each scenario 10 times, use:  ``` autogenbench run --repeat 10 Tasks ```  The `autogenbench` command-line tool allows a number of command-line arguments to control various parameters of execution. Type ``autogenbench -h`` to explore these options:  ``` 'autogenbench run' will run the specified autogen scenarios for a given number of repetitions and record all logs and trace information. When running in a Docker environment (default), each run will begin from a common, tightly controlled, environment. The resultant logs can then be further processed by other scripts to produce metrics.  positional arguments:   scenario      The JSONL scenario file to run. If a directory is specified,                 then all JSONL scenarios in the directory are run. (default:                 ./scenarios)  options:   -h, --help            show this help message and exit   -c CONFIG, --config CONFIG                         The environment variable name or path to the OAI_CONFIG_LIST (default: OAI_CONFIG_LIST).   -r REPEAT, --repeat REPEAT                         The number of repetitions to run for each scenario (default: 1).   -s SUBSAMPLE, --subsample SUBSAMPLE                         Run on a subsample of the tasks in the JSONL file(s). If a decimal value is specified, then run on                         the given proportion of tasks in each file. For example \"0.7\" would run on 70% of tasks, and \"1.0\"                         would run on 100% of tasks. If an integer value is specified, then randomly select *that* number of                         tasks from each specified JSONL file. For example \"7\" would run tasks, while \"1\" would run only 1                         task from each specified JSONL file. (default: 1.0; which is 100%)   -m MODEL, --model MODEL                         Filters the config_list to include only models matching the provided model name (default: None, which                         is all models).   --requirements REQUIREMENTS                         The requirements file to pip install before running the scenario.   -d DOCKER_IMAGE, --docker-image DOCKER_IMAGE                         The Docker image to use when running scenarios. Can not be used together with --native. (default:                         'autogenbench:default', which will be created if not present)   --native              Run the scenarios natively rather than in docker. NOTE: This is not advisable, and should be done                         with great caution. ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/README.md",
        "label": "autogen",
        "content": "Results  By default, the AutoGenBench stores results in a folder hierarchy with the following template:  ``./results/[scenario]/[task_id]/[instance_id]``  For example, consider the following folders:  ``./results/default_two_agents/two_agent_stocks/0`` ``./results/default_two_agents/two_agent_stocks/1``  ...  ``./results/default_two_agents/two_agent_stocks/9``  This folder holds the results for the ``two_agent_stocks`` task of the ``default_two_agents`` tasks file. The ``0`` folder contains the results of the first instance / run. The ``1`` folder contains the results of the second run, and so on. You can think of the _task_id_ as mapping to a prompt, or a unique set of parameters, while the _instance_id_ defines a specific attempt or run.  Within each folder, you will find the following files:  - *timestamp.txt*: records the date and time of the run, along with the version of the pyautogen library installed - *console_log.txt*: all console output produced by Docker when running AutoGen. Read this like you would a regular console. - *[agent]_messages.json*: for each Agent, a log of their messages dictionaries - *./coding*: A directory containing all code written by AutoGen, and all artifacts produced by that code. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/README.md",
        "label": "autogen",
        "content": "Contributing or Defining New Tasks or Benchmarks  If you would like to develop -- or even contribute -- your own tasks or benchmarks, please review the [contributor's guide](CONTRIBUTING.md) for complete technical details. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/scenarios/MATH/README.md",
        "label": "autogen",
        "content": "# MATH Benchmark  This scenario implements the [MATH](https://arxiv.org/abs/2103.03874) benchmark. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/scenarios/MATH/README.md",
        "label": "autogen",
        "content": "Running the tasks  ``` autogenbench run Tasks/math_two_agents.jsonl autogenbench tabulate Results/math_two_agents ```  By default, only a small subset (17 of 5000) MATH problems are exposed. Edit `Scripts/init_tasks.py` to expose more tasks. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/scenarios/MATH/README.md",
        "label": "autogen",
        "content": "Note on automated evaluation In this scenario, we adopted an automated evaluation pipeline (from [AutoGen](https://arxiv.org/abs/2308.08155) evaluation) that uses LLM to compare the results. Thus, the metric above is only an estimation of the agent's performance on math problems. We also find a similar practice of using LLM as judger for MATH dataset from the [Cumulative Reasoning](https://arxiv.org/abs/2308.04371) paper ([code](https://github.com/iiis-ai/cumulative-reasoning/blob/main/MATH/math-cr-4shot.py)).  The static checking from MATH dataset requires an exact match ('comparing 2.0 and 2 results in False'). We haven't found an established way that accurately compares the answer, so human involvement is still needed to confirm the result. In AutoGen, the conversation will end at \u201cTERMINATE\u201d by default. To enable an automated way of answer extraction and evaluation, we prompt an LLM with 1. the given problem 2. the ground truth answer 3. the last response from the solver, to extract the answer and compare it with the ground truth answer.  We evaluate the 17 problems for 3 times and go through these problems manually to check the answers. Compared with the automated result evaluation (the model is gpt-4-0613), we find that in 2/3 trials, the automated evaluation determined 1 correct answer as wrong (False Negative). This means 49/51 problems are evaluated correctly. We also went through 200 random sampled problems from whole dataset to check the results. There are 1 False Negative and 2 False Positives.  We note that False Positive is also possible due to the hallucination of LLMs, and the variety of problems. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/scenarios/MATH/README.md",
        "label": "autogen",
        "content": "References **Measuring Mathematical Problem Solving With the MATH Dataset**<br/> Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt<br/> [https://arxiv.org/abs/2103.03874](https://arxiv.org/abs/2103.03874)  **AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation**<br/> Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang and Chi Wang<br/> [https://arxiv.org/abs/2308.08155](https://arxiv.org/abs/2308.08155)  **Cumulative Reasoning with Large Language Models**<br/> Yifan Zhang, Jingqin Yang, Yang Yuan, Andrew Chi-Chih Yao<br/> [https://arxiv.org/abs/2308.04371](https://arxiv.org/abs/2308.04371) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/scenarios/Examples/README.md",
        "label": "autogen",
        "content": "# Example Tasks  Various AutoGen example tasks. Unlike other benchmark tasks, these tasks have no automated evaluation. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/scenarios/Examples/README.md",
        "label": "autogen",
        "content": "Running the tasks  ``` autogenbench run Tasks/default_two_agents ```  Some tasks require a Bing API key. Edit the ENV.json file to provide a valid BING_API_KEY, or simply allow that task to fail (it is only required by one task). "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/scenarios/AutoGPT/README.md",
        "label": "autogen",
        "content": "# AutoGPT Benchmark  This scenario implements an older subset of the [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT-Benchmarks/tree/master/agbenchmark#readme) benchmark.  Tasks were selected in November 2023, and may have since been deprecated. They are nonetheless useful for comparison and development. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/scenarios/AutoGPT/README.md",
        "label": "autogen",
        "content": "Running the tasks  ``` autogenbench run Tasks/autogpt__two_agents.jsonl autogenbench tabulate Results/autogpt__two_agents ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/scenarios/GAIA/README.md",
        "label": "autogen",
        "content": "# GAIA Benchmark  This scenario implements the [GAIA](https://arxiv.org/abs/2311.12983) agent benchmark. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/scenarios/GAIA/README.md",
        "label": "autogen",
        "content": "Running the TwoAgents tasks  Level 1 tasks: ```sh autogenbench run Tasks/gaia_test_level_1__two_agents.jsonl autogenbench tabulate Results/gaia_test_level_1__two_agents ```  Level 2 and 3 tasks are executed similarly. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/scenarios/GAIA/README.md",
        "label": "autogen",
        "content": "Running the SocietyOfMind tasks  Running the SocietyOfMind tasks is similar to the TwoAgentTasks, but requires an `ENV.json` file with a working BING API key. This file should be located in the root current working directory from where you are running autogenbench, and should have at least the following contents:  ```json {     \"BING_API_KEY\": \"Your_API_key\" } ```  Once created, simply run:  ```sh autogenbench run Tasks/gaia_test_level_1__soc.jsonl autogenbench tabulate Results/gaia_test_level_1__soc ```  And similarly for level 2 and 3. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/scenarios/GAIA/README.md",
        "label": "autogen",
        "content": "References **GAIA: a benchmark for General AI Assistants**<br/> Gr\u00e9goire Mialon, Cl\u00e9mentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, Thomas Scialom<br/> [https://arxiv.org/abs/2311.12983](https://arxiv.org/abs/2311.12983) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/scenarios/HumanEval/README.md",
        "label": "autogen",
        "content": "# HumanEval Benchmark  This scenario implements a modified version of the [HumanEval](https://arxiv.org/abs/2107.03374) benchmark. Compared to the original benchmark, there are **two key differences** here:  - A chat model rather than a completion model is used. - The agents get pass/fail feedback about their implementations, and can keep trying until they succeed or run out of tokens or turns. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/scenarios/HumanEval/README.md",
        "label": "autogen",
        "content": "Running the tasks  ``` autogenbench run Tasks/human_eval_two_agents.jsonl autogenbench tabulate Results/human_eval_two_agents ```  For faster development and iteration, a reduced HumanEval set is available via `Tasks/r_human_eval_two_agents.jsonl`, and contains only 26 problems of varying difficulty. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/samples/tools/autogenbench/scenarios/HumanEval/README.md",
        "label": "autogen",
        "content": "References **Evaluating Large Language Models Trained on Code**<br/> Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba<br/> [https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/autogen/agentchat/contrib/agent_eval/README.md",
        "label": "autogen",
        "content": "Agents for running the [AgentEval](https://microsoft.github.io/autogen/blog/2023/11/20/AgentEval/) pipeline.  AgentEval is a process for evaluating a LLM-based system's performance on a given task.  When given a task to evaluate and a few example runs, the critic and subcritic agents create evaluation criteria for evaluating a system's solution. Once the criteria has been created, the quantifier agent can evaluate subsequent task solutions based on the generated criteria.  For more information see: [AgentEval Integration Roadmap](https://github.com/microsoft/autogen/issues/2162)  See our [blog post](https://microsoft.github.io/autogen/blog/2024/06/21/AgentEval) for usage examples and general explanations. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/.github/PULL_REQUEST_TEMPLATE.md",
        "label": "autogen",
        "content": "<!-- Thank you for your contribution! Please review https://microsoft.github.io/autogen/docs/Contribute before opening a pull request. -->  <!-- Please add a reviewer to the assignee section when you create a PR. If you don't have the access to it, we will shortly find a reviewer and assign them to your PR. --> "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/.github/PULL_REQUEST_TEMPLATE.md",
        "label": "autogen",
        "content": "Why are these changes needed?  <!-- Please give a short summary of the change and the problem this solves. --> "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/.github/PULL_REQUEST_TEMPLATE.md",
        "label": "autogen",
        "content": "Related issue number  <!-- For example: \"Closes #1234\" --> "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/.github/PULL_REQUEST_TEMPLATE.md",
        "label": "autogen",
        "content": "Checks  - [ ] I've included any doc changes needed for https://microsoft.github.io/autogen/. See https://microsoft.github.io/autogen/docs/Contribute#documentation to build and test documentation locally. - [ ] I've added tests (if relevant) corresponding to the changes introduced in this PR. - [ ] I've made sure all auto checks have passed. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/.github/ISSUE_TEMPLATE.md",
        "label": "autogen",
        "content": "### Description <!-- A clear and concise description of the issue or feature request. -->  ### Environment - AutoGen version: <!-- Specify the AutoGen version (e.g., v0.2.0) --> - Python version: <!-- Specify the Python version (e.g., 3.8) --> - Operating System: <!-- Specify the OS (e.g., Windows 10, Ubuntu 20.04) -->  ### Steps to Reproduce (for bugs) <!-- Provide detailed steps to reproduce the issue. Include code snippets, configuration files, or any other relevant information. -->  1. Step 1 2. Step 2 3. ...  ### Expected Behavior <!-- Describe what you expected to happen. -->  ### Actual Behavior <!-- Describe what actually happened. Include any error messages, stack traces, or unexpected behavior. -->  ### Screenshots / Logs (if applicable) <!-- If relevant, include screenshots or logs that help illustrate the issue. -->  ### Additional Information <!-- Include any additional information that might be helpful, such as specific configurations, data samples, or context about the environment. -->  ### Possible Solution (if you have one) <!-- If you have suggestions on how to address the issue, provide them here. -->  ### Is this a Bug or Feature Request? <!-- Choose one: Bug | Feature Request -->  ### Priority <!-- Choose one: High | Medium | Low -->  ### Difficulty <!-- Choose one: Easy | Moderate | Hard -->  ### Any related issues? <!-- If this is related to another issue, reference it here. -->  ### Any relevant discussions? <!-- If there are any discussions or forum threads related to this issue, provide links. -->  ### Checklist <!-- Please check the items that you have completed --> - [ ] I have searched for similar issues and didn't find any duplicates. - [ ] I have provided a clear and concise description of the issue. - [ ] I have included the necessary environment details. - [ ] I have outlined the steps to reproduce the issue. - [ ] I have included any relevant logs or screenshots. - [ ] I have indicated whether this is a bug or a feature request. - [ ] I have set the priority and difficulty levels.  ### Additional Comments <!-- Any additional comments or context that you think would be helpful. --> "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/README.md",
        "label": "autogen",
        "content": "# Website  This website is built using [Docusaurus 3](https://docusaurus.io/), a modern static website generator. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/README.md",
        "label": "autogen",
        "content": "Prerequisites  To build and test documentation locally, begin by downloading and installing [Node.js](https://nodejs.org/en/download/), and then installing [Yarn](https://classic.yarnpkg.com/en/). On Windows, you can install via the npm package manager (npm) which comes bundled with Node.js:  ```console npm install --global yarn ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/README.md",
        "label": "autogen",
        "content": "Installation  ```console pip install pydoc-markdown pyyaml colored cd website yarn install ```  ### Install Quarto  `quarto` is used to render notebooks.  Install it [here](https://github.com/quarto-dev/quarto-cli/releases).  > Note: Ensure that your `quarto` version is `1.5.23` or higher. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/README.md",
        "label": "autogen",
        "content": "Local Development  Navigate to the `website` folder and run:  ```console pydoc-markdown python ./process_notebooks.py render yarn start ```  This command starts a local development server and opens up a browser window. Most changes are reflected live without having to restart the server. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/Migration-Guide.md",
        "label": "autogen",
        "content": "# Migration Guide "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/Migration-Guide.md",
        "label": "autogen",
        "content": "Migrating to 0.2  openai v1 is a total rewrite of the library with many breaking changes. For example, the inference requires instantiating a client, instead of using a global class method. Therefore, some changes are required for users of `pyautogen<0.2`.  - `api_base` -> `base_url`, `request_timeout` -> `timeout` in `llm_config` and `config_list`. `max_retry_period` and `retry_wait_time` are deprecated. `max_retries` can be set for each client. - MathChat is unsupported until it is tested in future release. - `autogen.Completion` and `autogen.ChatCompletion` are deprecated. The essential functionalities are moved to `autogen.OpenAIWrapper`:  ```python from autogen import OpenAIWrapper client = OpenAIWrapper(config_list=config_list) response = client.create(messages=[{\"role\": \"user\", \"content\": \"2+2=\"}]) print(client.extract_text_or_completion_object(response)) ```  - Inference parameter tuning and inference logging features are updated: ```python import autogen.runtime_logging  # Start logging autogen.runtime_logging.start()  # Stop logging autogen.runtime_logging.stop() ``` Checkout [Logging documentation](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#logging) and [Logging example notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_logging.ipynb) to learn more.  Inference parameter tuning can be done via [`flaml.tune`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function). - `seed` in autogen is renamed into `cache_seed` to accommodate the newly added `seed` param in openai chat completion api. `use_cache` is removed as a kwarg in `OpenAIWrapper.create()` for being automatically decided by `cache_seed`: int | None. The difference between autogen's `cache_seed` and openai's `seed` is that:   - autogen uses local disk cache to guarantee the exactly same output is produced for the same input and when cache is hit, no openai api call will be made.   - openai's `seed` is a best-effort deterministic sampling with no guarantee of determinism. When using openai's `seed` with `cache_seed` set to None, even for the same input, an openai api call will be made and there is no guarantee for getting exactly the same output. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/Examples.md",
        "label": "autogen",
        "content": "# Examples "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/Examples.md",
        "label": "autogen",
        "content": "Automated Multi Agent Chat  AutoGen offers conversable agents powered by LLM, tool or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation via multi-agent conversation. Please find documentation about this feature [here](/docs/Use-Cases/agent_chat).  Links to notebook examples:  ### Code Generation, Execution, and Debugging  - Automated Task Solving with Code Generation, Execution & Debugging - [View Notebook](/docs/notebooks/agentchat_auto_feedback_from_code_execution) - Automated Code Generation and Question Answering with Retrieval Augmented Agents - [View Notebook](/docs/notebooks/agentchat_RetrieveChat) - Automated Code Generation and Question Answering with [Qdrant](https://qdrant.tech/) based Retrieval Augmented Agents - [View Notebook](/docs/notebooks/agentchat_RetrieveChat_qdrant)  ### Multi-Agent Collaboration (>3 Agents)  - Automated Task Solving by Group Chat (with 3 group member agents and 1 manager agent) - [View Notebook](/docs/notebooks/agentchat_groupchat) - Automated Data Visualization by Group Chat (with 3 group member agents and 1 manager agent) - [View Notebook](/docs/notebooks/agentchat_groupchat_vis) - Automated Complex Task Solving by Group Chat (with 6 group member agents and 1 manager agent) - [View Notebook](/docs/notebooks/agentchat_groupchat_research) - Automated Task Solving with Coding & Planning Agents - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_planning.ipynb) - Automated Task Solving with transition paths specified in a graph - [View Notebook](https://microsoft.github.io/autogen/docs/notebooks/agentchat_groupchat_finite_state_machine) - Running a group chat as an inner-monolgue via the SocietyOfMindAgent - [View Notebook](/docs/notebooks/agentchat_society_of_mind) - Running a group chat with custom speaker selection function - [View Notebook](/docs/notebooks/agentchat_groupchat_customized)  ### Sequential Multi-Agent Chats  - Solving Multiple Tasks in a Sequence of Chats Initiated by a Single Agent - [View Notebook](/docs/notebooks/agentchat_multi_task_chats) - Async-solving Multiple Tasks in a Sequence of Chats Initiated by a Single Agent - [View Notebook](/docs/notebooks/agentchat_multi_task_async_chats) - Solving Multiple Tasks in a Sequence of Chats Initiated by Different Agents - [View Notebook](/docs/notebooks/agentchats_sequential_chats)  ### Nested Chats  - Solving Complex Tasks with Nested Chats - [View Notebook](/docs/notebooks/agentchat_nestedchat) - Solving Complex Tasks with A Sequence of Nested Chats - [View Notebook](/docs/notebooks/agentchat_nested_sequential_chats) - OptiGuide for Solving a Supply Chain Optimization Problem with Nested Chats with a Coding Agent and a Safeguard Agent - [View Notebook](/docs/notebooks/agentchat_nestedchat_optiguide) - Conversational Chess with Nested Chats and Tool Use - [View Notebook](/docs/notebooks/agentchat_nested_chats_chess)  ### Applications  - Automated Continual Learning from New Data - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_stream.ipynb) - [OptiGuide](https://github.com/microsoft/optiguide) - Coding, Tool Using, Safeguarding & Question Answering for Supply Chain Optimization - [AutoAnny](https://github.com/microsoft/autogen/tree/main/samples/apps/auto-anny) - A Discord bot built using AutoGen  ### Tool Use  - **Web Search**: Solve Tasks Requiring Web Info - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_web_info.ipynb) - Use Provided Tools as Functions - [View Notebook](/docs/notebooks/agentchat_function_call_currency_calculator) - Use Tools via Sync and Async Function Calling - [View Notebook](/docs/notebooks/agentchat_function_call_async) - Task Solving with Langchain Provided Tools as Functions - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_langchain.ipynb) - **RAG**: Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent) - [View Notebook](/docs/notebooks/agentchat_groupchat_RAG) - Function Inception: Enable AutoGen agents to update/remove functions during conversations. - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_inception_function.ipynb) - Agent Chat with Whisper - [View Notebook](/docs/notebooks/agentchat_video_transcript_translate_with_whisper) - Constrained Responses via Guidance - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_guidance.ipynb) - Browse the Web with Agents - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_surfer.ipynb) - **SQL**: Natural Language Text to SQL Query using the [Spider](https://yale-lily.github.io/spider) Text-to-SQL Benchmark - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_sql_spider.ipynb) - **Web Scraping**: Web Scraping with Apify - [View Notebook](/docs/notebooks/agentchat_webscraping_with_apify) - **Write a software app, task by task, with specially designed functions.** - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_function_call_code_writing.ipynb).  ### Human Involvement  - Simple example in ChatGPT style [View example](https://github.com/microsoft/autogen/blob/main/samples/simple_chat.py) - Auto Code Generation, Execution, Debugging and **Human Feedback** - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_human_feedback.ipynb) - Automated Task Solving with GPT-4 + **Multiple Human Users** - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_two_users.ipynb) - Agent Chat with **Async Human Inputs** - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/Async_human_input.ipynb)  ### Agent Teaching and Learning  - Teach Agents New Skills & Reuse via Automated Chat - [View Notebook](/docs/notebooks/agentchat_teaching) - Teach Agents New Facts, User Preferences and Skills Beyond Coding - [View Notebook](/docs/notebooks/agentchat_teachability) - Teach OpenAI Assistants Through GPTAssistantAgent - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_teachable_oai_assistants.ipynb) - Agent Optimizer: Train Agents in an Agentic Way - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_agentoptimizer.ipynb)  ### Multi-Agent Chat with OpenAI Assistants in the loop  - Hello-World Chat with OpenAi Assistant in AutoGen - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_assistant_twoagents_basic.ipynb) - Chat with OpenAI Assistant using Function Call - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_assistant_function_call.ipynb) - Chat with OpenAI Assistant with Code Interpreter - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_code_interpreter.ipynb) - Chat with OpenAI Assistant with Retrieval Augmentation - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_assistant_retrieval.ipynb) - OpenAI Assistant in a Group Chat - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_assistant_groupchat.ipynb) - GPTAssistantAgent based Multi-Agent Tool Use - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/gpt_assistant_agent_function_call.ipynb)  ### Non-OpenAI Models - Conversational Chess using non-OpenAI Models - [View Notebook](/docs/notebooks/agentchat_nested_chats_chess_altmodels)  ### Multimodal Agent  - Multimodal Agent Chat with DALLE and GPT-4V   - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_dalle_and_gpt4v.ipynb) - Multimodal Agent Chat with Llava  - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb) - Multimodal Agent Chat with GPT-4V - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_gpt-4v.ipynb)  ### Long Context Handling  <!-- - Conversations with Chat History Compression Enabled - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_compression.ipynb) --> - Long Context Handling as A Capability - [View Notebook](/docs/notebooks/agentchat_transform_messages)  ### Evaluation and Assessment  - AgentEval: A Multi-Agent System for Assess Utility of LLM-powered Applications - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agenteval_cq_math.ipynb)  ### Automatic Agent Building  - Automatically Build Multi-agent System with AgentBuilder - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/autobuild_basic.ipynb) - Automatically Build Multi-agent System from Agent Library - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/autobuild_agent_library.ipynb)  ### Observability - Track LLM calls, tool usage, actions and errors using AgentOps - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_agentops.ipynb) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/Examples.md",
        "label": "autogen",
        "content": "Enhanced Inferences  ### Utilities  - API Unification  - [View Documentation with Code Example](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference/#api-unification) - Utility Functions to Help Managing API configurations effectively - [View Notebook](/docs/topics/llm_configuration) - Cost Calculation - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_cost_token_tracking.ipynb)  ### Inference Hyperparameters Tuning  AutoGen offers a cost-effective hyperparameter optimization technique [EcoOptiGen](https://arxiv.org/abs/2303.04673) for tuning Large Language Models. The research study finds that tuning hyperparameters can significantly improve the utility of them. Please find documentation about this feature [here](/docs/Use-Cases/enhanced_inference).  Links to notebook examples: * [Optimize for Code Generation](https://github.com/microsoft/autogen/blob/main/notebook/oai_completion.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/oai_completion.ipynb) * [Optimize for Math](https://github.com/microsoft/autogen/blob/main/notebook/oai_chatgpt_gpt4.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/oai_chatgpt_gpt4.ipynb) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/Research.md",
        "label": "autogen",
        "content": "# Research  For technical details, please check our technical report and research publications.  * [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155). Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang and Chi Wang. ArXiv 2023.  ```bibtex @inproceedings{wu2023autogen,       title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},       author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Shaokun Zhang and Erkang Zhu and Beibin Li and Li Jiang and Xiaoyun Zhang and Chi Wang},       year={2023},       eprint={2308.08155},       archivePrefix={arXiv},       primaryClass={cs.AI} } ```  * [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673). Chi Wang, Susan Xueqing Liu, Ahmed H. Awadallah. AutoML'23.  ```bibtex @inproceedings{wang2023EcoOptiGen,     title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},     author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},     year={2023},     booktitle={AutoML'23}, } ```  * [An Empirical Study on Challenging Math Problem Solving with GPT-4](https://arxiv.org/abs/2306.01337). Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2306.01337 (2023).  ```bibtex @inproceedings{wu2023empirical,     title={An Empirical Study on Challenging Math Problem Solving with GPT-4},     author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},     year={2023},     booktitle={ArXiv preprint arXiv:2306.01337}, } ```  * [EcoAssistant: Using LLM Assistant More Affordably and Accurately](https://arxiv.org/abs/2310.03046). Jieyu Zhang, Ranjay Krishna, Ahmed H. Awadallah, Chi Wang. ArXiv preprint arXiv:2310.03046 (2023).  ```bibtex @inproceedings{zhang2023ecoassistant,     title={EcoAssistant: Using LLM Assistant More Affordably and Accurately},     author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi},     year={2023},     booktitle={ArXiv preprint arXiv:2310.03046}, } ```  * [Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications](https://arxiv.org/abs/2402.09015). Negar Arabzadeh, Julia Kiseleva, Qingyun Wu, Chi Wang, Ahmed Awadallah, Victor Dibia, Adam Fourney, Charles Clarke. ArXiv preprint arXiv:2402.09015 (2024).  ```bibtex @misc{Kiseleva2024agenteval,       title={Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications},       author={Negar Arabzadeh and Julia Kiseleva and Qingyun Wu and Chi Wang and Ahmed Awadallah and Victor Dibia and Adam Fourney and Charles Clarke},       year={2024},       eprint={2402.09015},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```  * [Training Language Model Agents without Modifying Language Models](https://arxiv.org/abs/2402.11359). Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, Qingyun Wu. ICML'24.  ```bibtex @misc{zhang2024agentoptimizer,       title={Training Language Model Agents without Modifying Language Models},       author={Shaokun Zhang and Jieyu Zhang and Jiale Liu and Linxin Song and Chi Wang and Ranjay Krishna and Qingyun Wu},       year={2024},       booktitle={ICML'24}, } ```  * [AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks](https://arxiv.org/abs/2403.04783). Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, Qingyun Wu. ArXiv preprint arXiv:2403.04783 (2024).  ```bibtex @misc{zeng2024autodefense,       title={AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks},       author={Yifan Zeng and Yiran Wu and Xiao Zhang and Huazheng Wang and Qingyun Wu},       year={2024},       eprint={2403.04783},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  * [StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows](https://arxiv.org/abs/2403.11322). Yiran Wu, Tianwei Yue, Shaokun Zhang, Chi Wang, Qingyun Wu. ArXiv preprint arXiv:2403.11322 (2024).  ```bibtex @misc{wu2024stateflow,         title={StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows},         author={Yiran Wu and Tianwei Yue and Shaokun Zhang and Chi Wang and Qingyun Wu},         year={2024},         eprint={2403.11322},         archivePrefix={arXiv},         primaryClass={cs.CL} } ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/ecosystem/agentops.md",
        "label": "autogen",
        "content": "# Agent Monitoring and Debugging with AgentOps  <img src=\"https://github.com/AgentOps-AI/agentops/blob/main/docs/images/external/logo/banner-badge.png?raw=true\" style=\"width: 40%;\" alt=\"AgentOps logo\"/>  [AgentOps](https://agentops.ai/?=autogen) provides session replays, metrics, and monitoring for AI agents.  At a high level, AgentOps gives you the ability to monitor LLM calls, costs, latency, agent failures, multi-agent interactions, tool usage, session-wide statistics, and more. For more info, check out the [AgentOps Repo](https://github.com/AgentOps-AI/agentops).  |                                       |                                                               | | ------------------------------------- | ------------------------------------------------------------- | | \ud83d\udcca **Replay Analytics and Debugging** | Step-by-step agent execution graphs                           | | \ud83d\udcb8 **LLM Cost Management**            | Track spend with LLM foundation model providers               | | \ud83e\uddea **Agent Benchmarking**             | Test your agents against 1,000+ evals                         | | \ud83d\udd10 **Compliance and Security**        | Detect common prompt injection and data exfiltration exploits | | \ud83e\udd1d **Framework Integrations**         | Native Integrations with CrewAI, AutoGen, & LangChain         |  <details open>   <summary><b><u>Agent Dashboard</u></b></summary>   <a href=\"https://app.agentops.ai?ref=gh\">    <img src=\"https://github.com/AgentOps-AI/agentops/blob/main/docs/images/external/app_screenshots/overview.png?raw=true\" style=\"width: 70%;\" alt=\"Agent Dashboard\"/>   </a> </details>  <details>   <summary><b><u>Session Analytics</u></b></summary>   <a href=\"https://app.agentops.ai?ref=gh\">     <img src=\"https://github.com/AgentOps-AI/agentops/blob/main/docs/images/external/app_screenshots/session-overview.png?raw=true\" style=\"width: 70%;\" alt=\"Session Analytics\"/>   </a> </details>  <details>   <summary><b><u>Session Replays</u></b></summary>   <a href=\"https://app.agentops.ai?ref=gh\">     <img src=\"https://github.com/AgentOps-AI/agentops/blob/main/docs/images/external/app_screenshots/session-replay.png?raw=true\" style=\"width: 70%;\" alt=\"Session Replays\"/>   </a> </details>  "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/ecosystem/agentops.md",
        "label": "autogen",
        "content": "Installation  AgentOps works seamlessly with applications built using Autogen.  1. **Install AgentOps** ```bash pip install agentops ```  2. **Create an API Key:** Create a user API key here: [Create API Key](https://app.agentops.ai/settings/projects)  3. **Configure Your Environment:** Add your API key to your environment variables  ``` AGENTOPS_API_KEY=<YOUR_AGENTOPS_API_KEY> ```  4. **Initialize AgentOps**  To start tracking all available data on Autogen runs, simply add two lines of code before implementing Autogen.  ```python import agentops agentops.init() # Or: agentops.init(api_key=\"your-api-key-here\") ```  After initializing AgentOps, Autogen will now start automatically tracking your agent runs. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/ecosystem/agentops.md",
        "label": "autogen",
        "content": "Features  - **LLM Costs**: Track spend with foundation model providers - **Replay Analytics**: Watch step-by-step agent execution graphs - **Recursive Thought Detection**: Identify when agents fall into infinite loops - **Custom Reporting:** Create custom analytics on agent performance - **Analytics Dashboard:** Monitor high level statistics about agents in development and production - **Public Model Testing**: Test your agents against benchmarks and leaderboards - **Custom Tests:** Run your agents against domain specific tests - **Time Travel Debugging**:  Save snapshots of session states to rewind and replay agent runs from chosen checkpoints. - **Compliance and Security**: Create audit logs and detect potential threats such as profanity and PII leaks - **Prompt Injection Detection**: Identify potential code injection and secret leaks "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/ecosystem/agentops.md",
        "label": "autogen",
        "content": "Autogen + AgentOps examples * [AgentChat with AgentOps Notebook](/docs/notebooks/agentchat_agentops) * [More AgentOps Examples](https://docs.agentops.ai/v1/quickstart) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/ecosystem/agentops.md",
        "label": "autogen",
        "content": "Extra links  - [\ud83d\udc26 Twitter](https://twitter.com/agentopsai/) - [\ud83d\udce2 Discord](https://discord.gg/JHPt4C7r) - [\ud83d\udd87\ufe0f AgentOps Dashboard](https://app.agentops.ai/ref?=autogen) - [\ud83d\udcd9 Documentation](https://docs.agentops.ai/introduction) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/ecosystem/ollama.md",
        "label": "autogen",
        "content": "# Ollama  ![Ollama Example](img/ecosystem-ollama.png)  [Ollama](https://ollama.com/) allows the users to run open-source large language models, such as Llama 2, locally. Ollama bundles model weights, configuration, and data into a single package, defined by a Modelfile. It optimizes setup and configuration details, including GPU usage.  - [Ollama + AutoGen instruction](https://ollama.ai/blog/openai-compatibility) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/ecosystem/microsoft-fabric.md",
        "label": "autogen",
        "content": "# Microsoft Fabric  ![Fabric Example](img/ecosystem-fabric.png)  [Microsoft Fabric](https://learn.microsoft.com/en-us/fabric/get-started/microsoft-fabric-overview) is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place. In this notenook, we give a simple example for using AutoGen in Microsoft Fabric.  - [Microsoft Fabric + AutoGen Code Examples](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_microsoft_fabric.ipynb) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/ecosystem/pgvector.md",
        "label": "autogen",
        "content": "# PGVector  [PGVector](https://github.com/pgvector/pgvector) is an open-source vector similarity search for Postgres.  - [PGVector + AutoGen Code Examples](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat_pgvector.ipynb) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/ecosystem/promptflow.md",
        "label": "autogen",
        "content": "# Promptflow  Promptflow is a comprehensive suite of tools that simplifies the development, testing, evaluation, and deployment of LLM based AI applications. It also supports integration with Azure AI for cloud-based operations and is designed to streamline end-to-end development.  Refer to [Promptflow docs](https://microsoft.github.io/promptflow/) for more information.  Quick links:  - Why use Promptflow - [Link](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/overview-what-is-prompt-flow) - Quick start guide - [Link](https://microsoft.github.io/promptflow/how-to-guides/quick-start.html) - Sample application for Promptflow + AutoGen integration - [Link](https://github.com/microsoft/autogen/tree/main/samples/apps/promptflow-autogen) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/ecosystem/promptflow.md",
        "label": "autogen",
        "content": "Sample Flow  ![Sample Promptflow](./img/ecosystem-promptflow.png) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/ecosystem/composio.md",
        "label": "autogen",
        "content": "# Composio  ![Composio Example](img/ecosystem-composio.png)  Composio empowers AI agents to seamlessly connect with external tools, Apps, and APIs to perform actions and receive triggers. With built-in support for AutoGen, Composio enables the creation of highly capable and adaptable AI agents that can autonomously execute complex tasks and deliver personalized experiences.  - [Composio + AutoGen Documentation with Code Examples](https://docs.composio.dev/framework/autogen) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/ecosystem/databricks.md",
        "label": "autogen",
        "content": "# Databricks  ![Databricks Data Intelligence Platform](img/ecosystem-databricks.png)  The [Databricks Data Intelligence Platform ](https://www.databricks.com/product/data-intelligence-platform) allows your entire organization to use data and AI. It\u2019s built on a lakehouse to provide an open, unified foundation for all data and governance, and is powered by a Data Intelligence Engine that understands the uniqueness of your data.   This example demonstrates how to use AutoGen alongside Databricks Foundation Model APIs and open-source LLM DBRX.  - [Databricks + AutoGen Code Examples](/docs/notebooks/agentchat_databricks_dbrx) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/ecosystem/memgpt.md",
        "label": "autogen",
        "content": "# MemGPT  ![MemGPT Example](img/ecosystem-memgpt.png)  MemGPT enables LLMs to manage their own memory and overcome limited context windows. You can use MemGPT to create perpetual chatbots that learn about you and modify their own personalities over time. You can connect MemGPT to your own local filesystems and databases, as well as connect MemGPT to your own tools and APIs. The MemGPT + AutoGen integration allows you to equip any AutoGen agent with MemGPT capabilities.  - [MemGPT + AutoGen Documentation with Code Examples](https://memgpt.readme.io/docs/autogen) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/ecosystem/llamaindex.md",
        "label": "autogen",
        "content": "# Llamaindex  ![Llamaindex Example](img/ecosystem-llamaindex.png)  [Llamaindex](https://www.llamaindex.ai/) allows the users to create Llamaindex agents and integrate them in autogen conversation patterns.  - [Llamaindex + AutoGen Code Examples](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_group_chat_with_llamaindex_agents.ipynb) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/ecosystem/azure_cosmos_db.md",
        "label": "autogen",
        "content": "# Azure Cosmos DB  > \"OpenAI relies on Cosmos DB to dynamically scale their ChatGPT service \u2013 one of the fastest-growing consumer apps ever \u2013 enabling high reliability and low maintenance.\" > \u2013 Satya Nadella, Microsoft chairman and chief executive officer  Azure Cosmos DB is a fully managed [NoSQL](https://learn.microsoft.com/en-us/azure/cosmos-db/distributed-nosql), [relational](https://learn.microsoft.com/en-us/azure/cosmos-db/distributed-relational), and [vector database](https://learn.microsoft.com/azure/cosmos-db/vector-database). It offers single-digit millisecond response times, automatic and instant scalability, along with guaranteed speed at any scale. Your business continuity is assured with up to 99.999% availability backed by SLA.  Your can simplify your application development by using this single database service for all your AI agent memory system needs, from [geo-replicated distributed cache](https://medium.com/@marcodesanctis2/using-azure-cosmos-db-as-your-persistent-geo-replicated-distributed-cache-b381ad80f8a0) to tracing/logging to [vector database](https://learn.microsoft.com/en-us/azure/cosmos-db/vector-database).  Learn more about how Azure Cosmos DB enhances the performance of your [AI agent](https://learn.microsoft.com/en-us/azure/cosmos-db/ai-agents).  - [Try Azure Cosmos DB free](https://learn.microsoft.com/en-us/azure/cosmos-db/try-free) - [Use Azure Cosmos DB lifetime free tier](https://learn.microsoft.com/en-us/azure/cosmos-db/free-tier) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/llm-observability.md",
        "label": "autogen",
        "content": "# Agent Observability  AutoGen supports advanced LLM agent observability and monitoring through built-in logging and partner providers. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/llm-observability.md",
        "label": "autogen",
        "content": "AutoGen Observability Integrations  ### Built-In Logging AutoGen's SQLite and File Logger - [Tutorial Notebook](/docs/notebooks/agentchat_logging)  ### Full-Service Partner Integrations AutoGen partners with [AgentOps](https://agentops.ai) to provide multi-agent tracking, metrics, and monitoring - [Tutorial Notebook](/docs/notebooks/agentchat_agentops)  "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/llm-observability.md",
        "label": "autogen",
        "content": "What is Observability? Observability provides developers with the necessary insights to understand and improve the internal workings of their agents. Observability is necessary for maintaining reliability, tracking costs, and ensuring AI safety.  **Without observability tools, developers face significant hurdles:**  - Tracking agent activities across sessions becomes a complex, error-prone task. - Manually sifting through verbose terminal outputs to understand LLM interactions is inefficient. - Pinpointing the exact moments of tool invocations is often like finding a needle in a haystack.   **Key Features of Observability Dashboards:** - Human-readable overview analytics and replays of agent activities. - LLM cost, prompt, completion, timestamp, and metadata tracking for performance monitoring. - Tool invocation, events, and agent-to-agent interactions for workflow monitoring. - Error flagging and notifications for faster debugging. - Access to a wealth of data for developers using supported agent frameworks, such as environments, SDK versions, and more.  ### Compliance  Observability is not just a development convenience\u2014it's a compliance necessity, especially in regulated industries: - It offers insights into AI decision-making processes, fostering trust and transparency. - Anomalies and unintended behaviors are detected promptly, reducing various risks. - Ensuring adherence to data privacy regulations, thereby safeguarding sensitive information. - Compliance violations are quickly identified and addressed, enhancing incident management. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/retrieval_augmentation.md",
        "label": "autogen",
        "content": "# Retrieval Augmentation  Retrieval Augmented Generation (RAG) is a powerful technique that combines language models with external knowledge retrieval to improve the quality and relevance of generated responses.  One way to realize RAG in AutoGen is to construct agent chats with `RetrieveAssistantAgent` and `RetrieveUserProxyAgent` classes. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/retrieval_augmentation.md",
        "label": "autogen",
        "content": "Example Setup: RAG with Retrieval Augmented Agents The following is an example setup demonstrating how to create retrieval augmented agents in AutoGen:  ### Step 1. Create an instance of `RetrieveAssistantAgent` and `RetrieveUserProxyAgent`.  Here `RetrieveUserProxyAgent` instance acts as a proxy agent that retrieves relevant information based on the user's input. ```python assistant = RetrieveAssistantAgent(     name=\"assistant\",     system_message=\"You are a helpful assistant.\",     llm_config={         \"timeout\": 600,         \"cache_seed\": 42,         \"config_list\": config_list,     }, ) ragproxyagent = RetrieveUserProxyAgent(     name=\"ragproxyagent\",     human_input_mode=\"NEVER\",     max_consecutive_auto_reply=3,     retrieve_config={         \"task\": \"code\",         \"docs_path\": [             \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",             \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\",             os.path.join(os.path.abspath(\"\"), \"..\", \"website\", \"docs\"),         ],         \"custom_text_types\": [\"mdx\"],         \"chunk_token_size\": 2000,         \"model\": config_list[0][\"model\"],         \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),         \"embedding_model\": \"all-mpnet-base-v2\",         \"get_or_create\": True,  # set to False if you don't want to reuse an existing collection, but you'll need to remove the collection manually     },     code_execution_config=False,  # set to False if you don't want to execute the code ) ```  ### Step 2. Initiating Agent Chat with Retrieval Augmentation  Once the retrieval augmented agents are set up, you can initiate a chat with retrieval augmentation using the following code:  ```python code_problem = \"How can I use FLAML to perform a classification task and use spark to do parallel training. Train 30 seconds and force cancel jobs if time limit is reached.\" ragproxyagent.initiate_chat(     assistant, message=ragproxyagent.message_generator, problem=code_problem, search_string=\"spark\" )  # search_string is used as an extra filter for the embeddings search, in this case, we only want to search documents that contain \"spark\". ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/retrieval_augmentation.md",
        "label": "autogen",
        "content": "Example Setup: RAG with Retrieval Augmented Agents with PGVector The following is an example setup demonstrating how to create retrieval augmented agents in AutoGen:  ### Step 1. Create an instance of `RetrieveAssistantAgent` and `RetrieveUserProxyAgent`.  Here `RetrieveUserProxyAgent` instance acts as a proxy agent that retrieves relevant information based on the user's input.  Specify the connection_string, or the host, port, database, username, and password in the db_config.  ```python assistant = RetrieveAssistantAgent(     name=\"assistant\",     system_message=\"You are a helpful assistant.\",     llm_config={         \"timeout\": 600,         \"cache_seed\": 42,         \"config_list\": config_list,     }, ) ragproxyagent = RetrieveUserProxyAgent(     name=\"ragproxyagent\",     human_input_mode=\"NEVER\",     max_consecutive_auto_reply=3,     retrieve_config={         \"task\": \"code\",         \"docs_path\": [             \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",             \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\",             os.path.join(os.path.abspath(\"\"), \"..\", \"website\", \"docs\"),         ],         \"vector_db\": \"pgvector\",         \"collection_name\": \"autogen_docs\",         \"db_config\": {             \"connection_string\": \"postgresql://testuser:testpwd@localhost:5432/vectordb\", # Optional - connect to an external vector database             # \"host\": None, # Optional vector database host             # \"port\": None, # Optional vector database port             # \"database\": None, # Optional vector database name             # \"username\": None, # Optional vector database username             # \"password\": None, # Optional vector database password         },         \"custom_text_types\": [\"mdx\"],         \"chunk_token_size\": 2000,         \"model\": config_list[0][\"model\"],         \"get_or_create\": True,     },     code_execution_config=False, ) ```  ### Step 2. Initiating Agent Chat with Retrieval Augmentation  Once the retrieval augmented agents are set up, you can initiate a chat with retrieval augmentation using the following code:  ```python code_problem = \"How can I use FLAML to perform a classification task and use spark to do parallel training. Train 30 seconds and force cancel jobs if time limit is reached.\" ragproxyagent.initiate_chat(     assistant, message=ragproxyagent.message_generator, problem=code_problem, search_string=\"spark\" )  # search_string is used as an extra filter for the embeddings search, in this case, we only want to search documents that contain \"spark\". ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/retrieval_augmentation.md",
        "label": "autogen",
        "content": "Online Demo [Retrival-Augmented Chat Demo on Huggingface](https://huggingface.co/spaces/thinkall/autogen-demos) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/retrieval_augmentation.md",
        "label": "autogen",
        "content": "More Examples and Notebooks For more detailed examples and notebooks showcasing the usage of retrieval augmented agents in AutoGen, refer to the following: - Automated Code Generation and Question Answering with Retrieval Augmented Agents - [View Notebook](/docs/notebooks/agentchat_RetrieveChat) - Automated Code Generation and Question Answering with [PGVector](https://github.com/pgvector/pgvector) based Retrieval Augmented Agents - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat_pgvector.ipynb) - Automated Code Generation and Question Answering with [Qdrant](https://qdrant.tech/) based Retrieval Augmented Agents - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat_qdrant.ipynb) - Chat with OpenAI Assistant with Retrieval Augmentation - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_assistant_retrieval.ipynb) - **RAG**: Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent) - [View Notebook](/docs/notebooks/agentchat_groupchat_RAG) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/retrieval_augmentation.md",
        "label": "autogen",
        "content": "Roadmap  Explore our detailed roadmap [here](https://github.com/microsoft/autogen/issues/1657) for further advancements plan around RAG. Your contributions, feedback, and use cases are highly appreciated! We invite you to engage with us and play a pivotal role in the development of this impactful feature. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/llm-caching.md",
        "label": "autogen",
        "content": "# LLM Caching  AutoGen supports caching API requests so that they can be reused when the same request is issued. This is useful when repeating or continuing experiments for reproducibility and cost saving.  Since version [`0.2.8`](https://github.com/microsoft/autogen/releases/tag/v0.2.8), a configurable context manager allows you to easily configure LLM cache, using either [`DiskCache`](/docs/reference/cache/disk_cache#diskcache), [`RedisCache`](/docs/reference/cache/redis_cache#rediscache), or Cosmos DB Cache. All agents inside the context manager will use the same cache.  ```python from autogen import Cache  # Use Redis as cache with Cache.redis(redis_url=\"redis://localhost:6379/0\") as cache:     user.initiate_chat(assistant, message=coding_task, cache=cache)  # Use DiskCache as cache with Cache.disk() as cache:     user.initiate_chat(assistant, message=coding_task, cache=cache)  # Use Azure Cosmos DB as cache with Cache.cosmos_db(connection_string=\"your_connection_string\", database_id=\"your_database_id\", container_id=\"your_container_id\") as cache:     user.initiate_chat(assistant, message=coding_task, cache=cache)  ```  The cache can also be passed directly to the model client's create call.  ```python client = OpenAIWrapper(...) with Cache.disk() as cache:     client.create(..., cache=cache) ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/llm-caching.md",
        "label": "autogen",
        "content": "Controlling the seed  You can vary the `cache_seed` parameter to get different LLM output while still using cache.  ```python # Setting the cache_seed to 1 will use a different cache from the default one # and you will see different output. with Cache.disk(cache_seed=1) as cache:     user.initiate_chat(assistant, message=coding_task, cache=cache) ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/llm-caching.md",
        "label": "autogen",
        "content": "Cache path  By default [`DiskCache`](/docs/reference/cache/disk_cache#diskcache) uses `.cache` for storage. To change the cache directory, set `cache_path_root`:  ```python with Cache.disk(cache_path_root=\"/tmp/autogen_cache\") as cache:     user.initiate_chat(assistant, message=coding_task, cache=cache) ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/llm-caching.md",
        "label": "autogen",
        "content": "Disabling cache  For backward compatibility, [`DiskCache`](/docs/reference/cache/disk_cache#diskcache) is on by default with `cache_seed` set to 41. To disable caching completely, set `cache_seed` to `None` in the `llm_config` of the agent.  ```python assistant = AssistantAgent(     \"coding_agent\",     llm_config={         \"cache_seed\": None,         \"config_list\": OAI_CONFIG_LIST,         \"max_tokens\": 1024,     }, ) ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/llm-caching.md",
        "label": "autogen",
        "content": "Difference between `cache_seed` and OpenAI's `seed` parameter  OpenAI v1.1 introduced a new parameter `seed`. The difference between AutoGen's `cache_seed` and OpenAI's `seed` is AutoGen uses an explicit request cache to guarantee the exactly same output is produced for the same input and when cache is hit, no OpenAI API call will be made. OpenAI's `seed` is a best-effort deterministic sampling with no guarantee of determinism. When using OpenAI's `seed` with `cache_seed` set to `None`, even for the same input, an OpenAI API call will be made and there is no guarantee for getting exactly the same output. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/non-openai-models/about-using-nonopenai-models.md",
        "label": "autogen",
        "content": "# Non-OpenAI Models  AutoGen allows you to use non-OpenAI models through proxy servers that provide an OpenAI-compatible API or a [custom model client](https://microsoft.github.io/autogen/blog/2024/01/26/Custom-Models) class.  Benefits of this flexibility include access to hundreds of models, assigning specialized models to agents (e.g., fine-tuned coding models), the ability to run AutoGen entirely within your environment, utilising both OpenAI and non-OpenAI models in one system, and cost reductions in inference. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/non-openai-models/about-using-nonopenai-models.md",
        "label": "autogen",
        "content": "OpenAI-compatible API proxy server Any proxy server that provides an API that is compatible with [OpenAI's API](https://platform.openai.com/docs/api-reference) will work with AutoGen.  These proxy servers can be cloud-based or running locally within your environment.  ![Cloud or Local Proxy Servers](images/cloudlocalproxy.png)  ### Cloud-based proxy servers By using cloud-based proxy servers, you are able to use models without requiring the hardware and software to run them.  These providers can host open source/weight models, like [Hugging Face](https://huggingface.co/) and [Mistral AI](https://mistral.ai/), or their own closed models.  When cloud-based proxy servers provide an OpenAI-compatible API, using them in AutoGen is straightforward. With [LLM Configuration](/docs/topics/llm_configuration) done in the same way as when using OpenAI's models, the primary difference is typically the authentication which is usually handled through an API key.  Examples of using cloud-based proxy servers providers that have an OpenAI-compatible API are provided below:  - [Together AI example](/docs/topics/non-openai-models/cloud-togetherai) - [Mistral AI example](/docs/topics/non-openai-models/cloud-mistralai) - [Anthropic Claude example](/docs/topics/non-openai-models/cloud-anthropic)   ### Locally run proxy servers An increasing number of LLM proxy servers are available for use locally. These can be open-source (e.g., LiteLLM, Ollama, vLLM) or closed-source (e.g., LM Studio), and are typically used for running the full-stack within your environment.  Similar to cloud-based proxy servers, as long as these proxy servers provide an OpenAI-compatible API, running them in AutoGen is straightforward.  Examples of using locally run proxy servers that have an OpenAI-compatible API are provided below:  - [LiteLLM with Ollama example](/docs/topics/non-openai-models/local-litellm-ollama) - [LM Studio](/docs/topics/non-openai-models/local-lm-studio) - [vLLM example](/docs/topics/non-openai-models/local-vllm)  ````mdx-code-block :::tip If you are planning to use Function Calling, not all cloud-based and local proxy servers support Function Calling with their OpenAI-compatible API, so check their documentation. ::: ````  ### Configuration for Non-OpenAI models  Whether you choose a cloud-based or locally-run proxy server, the configuration is done in the same way as using OpenAI's models, see [LLM Configuration](/docs/topics/llm_configuration) for further information.  You can use [model configuration filtering](/docs/topics/llm_configuration#config-list-filtering) to assign specific models to agents.  "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/non-openai-models/about-using-nonopenai-models.md",
        "label": "autogen",
        "content": "Custom Model Client class For more advanced users, you can create your own custom model client class, enabling you to define and load your own models.  See the [AutoGen with Custom Models: Empowering Users to Use Their Own Inference Mechanism](/blog/2024/01/26/Custom-Models) blog post and [this notebook](/docs/notebooks/agentchat_custom_model/) for a guide to creating custom model client classes. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/non-openai-models/best-tips-for-nonopenai-models.md",
        "label": "autogen",
        "content": "# Tips for Non-OpenAI Models  Here are some tips for using non-OpenAI Models with AutoGen. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/non-openai-models/best-tips-for-nonopenai-models.md",
        "label": "autogen",
        "content": "Finding the right model Every model will perform differently across the operations within your AutoGen setup, such as speaker selection, coding, function calling, content creation, etc. On the whole, larger models (13B+) perform better with following directions and providing more cohesive responses.  Content creation can be performed by most models.  Fine-tuned models can be great for very specific tasks, such as function calling and coding.  Specific tasks, such as speaker selection in a Group Chat scenario, that require very accurate outputs can be a challenge with most open source/weight models. The use of chain-of-thought and/or few-shot prompting can help guide the LLM to provide the output in the format you want. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/non-openai-models/best-tips-for-nonopenai-models.md",
        "label": "autogen",
        "content": "Validating your program Testing your AutoGen setup against a very large LLM, such as OpenAI's ChatGPT or Anthropic's Claude 3, can help validate your agent setup and configuration.  Once a setup is performing as you want, you can replace the models for your agents with non-OpenAI models and iteratively tweak system messages, prompts, and model selection. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/non-openai-models/best-tips-for-nonopenai-models.md",
        "label": "autogen",
        "content": "Chat template AutoGen utilises a set of chat messages for the conversation between AutoGen/user and LLMs. Each chat message has a role attribute that is typically `user`, `assistant`, or `system`.  A chat template is applied during inference and some chat templates implement rules about what roles can be used in specific sequences of messages.  For example, when using Mistral AI's API the last chat message must have a role of `user`. In a Group Chat scenario the message used to select the next speaker will have a role of `system` by default and the API will throw an exception for this step. To overcome this the GroupChat's constructor has a parameter called `role_for_select_speaker_messages` that can be used to change the role name to `user`.  ```python groupchat = autogen.GroupChat(     agents=[user_proxy, coder, pm],     messages=[],     max_round=12,     # Role for select speaker message will be set to 'user' instead of 'system'     role_for_select_speaker_messages='user', ) ```  If the chat template associated with a model you want to use doesn't support the role sequence and names used in AutoGen you can modify the chat template. See an example of this on our [vLLM page](/docs/topics/non-openai-models/local-vllm#chat-template). "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/non-openai-models/best-tips-for-nonopenai-models.md",
        "label": "autogen",
        "content": "Discord Join AutoGen's [#alt-models](https://discord.com/channels/1153072414184452236/1201369716057440287) channel on their Discord and discuss non-OpenAI models and configurations. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/non-openai-models/local-vllm.md",
        "label": "autogen",
        "content": "# vLLM [vLLM](https://github.com/vllm-project/vllm) is a locally run proxy and inference server, providing an OpenAI-compatible API. As it performs both the proxy and the inferencing, you don't need to install an additional inference server.  Note: vLLM does not support OpenAI's [Function Calling](https://platform.openai.com/docs/guides/function-calling) (usable with AutoGen). However, it is in development and may be available by the time you read this.  Running this stack requires the installation of: 1. AutoGen ([installation instructions](/docs/installation)) 2. vLLM  Note: We recommend using a virtual environment for your stack, see [this article](https://microsoft.github.io/autogen/docs/installation/#create-a-virtual-environment-optional) for guidance. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/non-openai-models/local-vllm.md",
        "label": "autogen",
        "content": "Installing vLLM  In your terminal:  ```bash pip install vllm ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/non-openai-models/local-vllm.md",
        "label": "autogen",
        "content": "Choosing models  vLLM will download new models when you run the server.  The models are sourced from [Hugging Face](https://huggingface.co), a filtered list of Text Generation models is [here](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending) and vLLM has a list of [commonly used models](https://docs.vllm.ai/en/latest/models/supported_models.html). Use the full model name, e.g. `mistralai/Mistral-7B-Instruct-v0.2`. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/non-openai-models/local-vllm.md",
        "label": "autogen",
        "content": "Chat Template  vLLM uses a pre-defined chat template, unless the model has a chat template defined in its config file on Hugging Face. This can cause an issue if the chat template doesn't allow `'role' : 'system'` messages, as used in AutoGen.  Therefore, we will create a chat template for the Mistral.AI Mistral 7B model we are using that allows roles of 'user', 'assistant', and 'system'.  Create a file name `autogenmistraltemplate.jinja` with the following content: ```` text {{ bos_token }} {% for message in messages %}     {% if ((message['role'] == 'user' or message['role'] == 'system') != (loop.index0 % 2 == 0)) %}         {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}     {% endif %}      {% if (message['role'] == 'user' or message['role'] == 'system') %}         {{ '[INST] ' + message['content'] + ' [/INST]' }}     {% elif message['role'] == 'assistant' %}         {{ message['content'] + eos_token}}     {% else %}         {{ raise_exception('Only system, user and assistant roles are supported!') }}     {% endif %} {% endfor %} ````  ````mdx-code-block :::warning Chat Templates are specific to the model/model family. The example shown here is for Mistral-based models like Mistral 7B and Mixtral 8x7B.  vLLM has a number of [example templates](https://github.com/vllm-project/vllm/tree/main/examples) for models that can be a starting point for your chat template. Just remember, the template may need to be adjusted to support 'system' role messages. ::: ```` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/non-openai-models/local-vllm.md",
        "label": "autogen",
        "content": "Running vLLM proxy server  To run vLLM with the chosen model and our chat template, in your terminal:  ```bash python -m vllm.entrypoints.openai.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --chat-template autogenmistraltemplate.jinja ```  By default, vLLM will run on 'http://0.0.0.0:8000'. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/non-openai-models/local-vllm.md",
        "label": "autogen",
        "content": "Using vLLM with AutoGen  Now that we have the URL for the vLLM proxy server, you can use it within AutoGen in the same way as OpenAI or cloud-based proxy servers.  As you are running this proxy server locally, no API key is required. As ```api_key``` is a mandatory field for configurations within AutoGen we put a dummy value in it, as per the example below.  Although we are specifying the model when running the vLLM command, we must still put it into the ```model``` value for vLLM.   ```python from autogen import UserProxyAgent, ConversableAgent  local_llm_config={     \"config_list\": [         {             \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\", # Same as in vLLM command             \"api_key\": \"NotRequired\", # Not needed             \"base_url\": \"http://0.0.0.0:8000/v1\"  # Your vLLM URL, with '/v1' added         }     ],     \"cache_seed\": None # Turns off caching, useful for testing different models }  # Create the agent that uses the LLM. assistant = ConversableAgent(\"agent\", llm_config=local_llm_config,system_message=\"\")  # Create the agent that represents the user in the conversation. user_proxy = UserProxyAgent(\"user\", code_execution_config=False,system_message=\"\")  # Let the assistant start the conversation.  It will end when the user types exit. assistant.initiate_chat(user_proxy, message=\"How can I help you today?\") ```  Output:  ```` text agent (to user):  How can I help you today?  -------------------------------------------------------------------------------- Provide feedback to agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: Why is the sky blue? user (to agent):  Why is the sky blue?  --------------------------------------------------------------------------------  >>>>>>>> USING AUTO REPLY... agent (to user):   The sky appears blue due to a phenomenon called Rayleigh scattering. As sunlight reaches Earth's atmosphere, it interacts with molecules and particles in the air, causing the scattering of light. Blue light has a shorter wavelength and gets scattered more easily than other colors, which is why the sky appears blue during a clear day.  However, during sunrise and sunset, the sky can appear red, orange, or purple due to a different type of scattering called scattering by dust, pollutants, and water droplets, which scatter longer wavelengths of light more effectively.  -------------------------------------------------------------------------------- Provide feedback to agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: and why does it turn red? user (to agent):  and why does it turn red?  --------------------------------------------------------------------------------  >>>>>>>> USING AUTO REPLY... agent (to user):   During sunrise and sunset, the angle of the sun's rays in the sky is lower, and they have to pass through more of the Earth's atmosphere before reaching an observer. This additional distance results in more scattering of sunlight, which preferentially scatters the longer wavelengths (red, orange, and yellow) more than the shorter wavelengths (blue and green).  The scattering of sunlight by the Earth's atmosphere causes the red, orange, and yellow colors to be more prevalent in the sky during sunrise and sunset, resulting in the beautiful display of colors often referred to as a sunrise or sunset.  As the sun continues to set, the sky can transition to various shades of purple, pink, and eventually dark blue or black, as the available sunlight continues to decrease and the longer wavelengths are progressively scattered less effectively.  -------------------------------------------------------------------------------- Provide feedback to agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit ```` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/handling_long_contexts/intro_to_transform_messages.md",
        "label": "autogen",
        "content": "# Introduction to Transform Messages  Why do we need to handle long contexts? The problem arises from several constraints and requirements:  1. Token limits: LLMs have token limits that restrict the amount of textual data they can process. If we exceed these limits, we may encounter errors or incur additional costs. By preprocessing the chat history, we can ensure that we stay within the acceptable token range.  2. Context relevance: As conversations progress, retaining the entire chat history may become less relevant or even counterproductive. Keeping only the most recent and pertinent messages can help the LLMs focus on the most crucial context, leading to more accurate and relevant responses.  3. Efficiency: Processing long contexts can consume more computational resources, leading to slower response times. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/handling_long_contexts/intro_to_transform_messages.md",
        "label": "autogen",
        "content": "Transform Messages Capability  The `TransformMessages` capability is designed to modify incoming messages before they are processed by the LLM agent. This can include limiting the number of messages, truncating messages to meet token limits, and more.  :::info Requirements Install `pyautogen`:  ```bash pip install pyautogen ```  For more information, please refer to the [installation guide](/docs/installation/). :::  ### Exploring and Understanding Transformations  Let's start by exploring the available transformations and understanding how they work. We will start off by importing the required modules.  ```python import copy import pprint  from autogen.agentchat.contrib.capabilities import transforms ```  #### Example 1: Limiting the Total Number of Messages  Consider a scenario where you want to limit the context history to only the most recent messages to maintain efficiency and relevance. You can achieve this with the MessageHistoryLimiter transformation:  ```python # Limit the message history to the 3 most recent messages max_msg_transfrom = transforms.MessageHistoryLimiter(max_messages=3)  messages = [     {\"role\": \"user\", \"content\": \"hello\"},     {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"there\"}]},     {\"role\": \"user\", \"content\": \"how\"},     {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"are you doing?\"}]},     {\"role\": \"user\", \"content\": \"very very very very very very long string\"}, ]  processed_messages = max_msg_transfrom.apply_transform(copy.deepcopy(messages)) pprint.pprint(processed_messages) ```  ```console [{'content': 'how', 'role': 'user'}, {'content': [{'text': 'are you doing?', 'type': 'text'}], 'role': 'assistant'}, {'content': 'very very very very very very long string', 'role': 'user'}] ```  By applying the `MessageHistoryLimiter`, we can see that we were able to limit the context history to the 3 most recent messages. However, if the splitting point is between a \"tool_calls\" and \"tool\" pair, the complete pair will be included to obey the OpenAI API call constraints.  ```python max_msg_transfrom = transforms.MessageHistoryLimiter(max_messages=3)  messages = [     {\"role\": \"user\", \"content\": \"hello\"},     {\"role\": \"tool_calls\", \"content\": \"calling_tool\"},     {\"role\": \"tool\", \"content\": \"tool_response\"},     {\"role\": \"user\", \"content\": \"how are you\"},     {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"are you doing?\"}]}, ]  processed_messages = max_msg_transfrom.apply_transform(copy.deepcopy(messages)) pprint.pprint(processed_messages) ``` ```console [{'content': 'calling_tool', 'role': 'tool_calls'}, {'content': 'tool_response', 'role': 'tool'}, {'content': 'how are you', 'role': 'user'}, {'content': [{'text': 'are you doing?', 'type': 'text'}], 'role': 'assistant'}] ```  #### Example 2: Limiting the Number of Tokens  To adhere to token limitations, use the `MessageTokenLimiter` transformation. This limits tokens per message and the total token count across all messages. Additionally, a `min_tokens` threshold can be applied:  ```python # Limit the token limit per message to 3 tokens token_limit_transform = transforms.MessageTokenLimiter(max_tokens_per_message=3, min_tokens=10)  processed_messages = token_limit_transform.apply_transform(copy.deepcopy(messages))  pprint.pprint(processed_messages) ```  ```console [{'content': 'hello', 'role': 'user'}, {'content': [{'text': 'there', 'type': 'text'}], 'role': 'assistant'}, {'content': 'how', 'role': 'user'}, {'content': [{'text': 'are you doing', 'type': 'text'}], 'role': 'assistant'}, {'content': 'very very very', 'role': 'user'}] ```  We can see that we were able to limit the number of tokens to 3, which is equivalent to 3 words for this instance.  In the following example we will explore the effect of the `min_tokens` threshold.  ```python short_messages = [     {\"role\": \"user\", \"content\": \"hello there, how are you?\"},     {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"hello\"}]}, ]  processed_short_messages = token_limit_transform.apply_transform(copy.deepcopy(short_messages))  pprint.pprint(processed_short_messages) ```  ```console [{'content': 'hello there, how are you?', 'role': 'user'},  {'content': [{'text': 'hello', 'type': 'text'}], 'role': 'assistant'}] ```  We can see that no transformation was applied, because the threshold of 10 total tokens was not reached.  ### Apply Transformations Using Agents  So far, we have only tested the `MessageHistoryLimiter` and `MessageTokenLimiter` transformations individually, let's test these transformations with AutoGen's agents.  #### Setting Up the Stage  ```python import os import copy  import autogen from autogen.agentchat.contrib.capabilities import transform_messages, transforms from typing import Dict, List  config_list = [{\"model\": \"gpt-3.5-turbo\", \"api_key\": os.getenv(\"OPENAI_API_KEY\")}]  # Define your agent; the user proxy and an assistant assistant = autogen.AssistantAgent(     \"assistant\",     llm_config={\"config_list\": config_list}, ) user_proxy = autogen.UserProxyAgent(     \"user_proxy\",     human_input_mode=\"NEVER\",     is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\", \"\"),     max_consecutive_auto_reply=10, ) ```  :::tip Learn more about configuring LLMs for agents [here](/docs/topics/llm_configuration). :::  We first need to write the `test` function that creates a very long chat history by exchanging messages between an assistant and a user proxy agent, and then attempts to initiate a new chat without clearing the history, potentially triggering an error due to token limits.  ```python # Create a very long chat history that is bound to cause a crash for gpt 3.5 def test(assistant: autogen.ConversableAgent, user_proxy: autogen.UserProxyAgent):     for _ in range(1000):         # define a fake, very long messages         assitant_msg = {\"role\": \"assistant\", \"content\": \"test \" * 1000}         user_msg = {\"role\": \"user\", \"content\": \"\"}          assistant.send(assitant_msg, user_proxy, request_reply=False, silent=True)         user_proxy.send(user_msg, assistant, request_reply=False, silent=True)      try:         user_proxy.initiate_chat(assistant, message=\"plot and save a graph of x^2 from -10 to 10\", clear_history=False)     except Exception as e:         print(f\"Encountered an error with the base assistant: \\n{e}\") ```  The first run will be the default implementation, where the agent does not have the `TransformMessages` capability.  ```python test(assistant, user_proxy) ```  Running this test will result in an error due to the large number of tokens sent to OpenAI's gpt 3.5.  ```console user_proxy (to assistant):  plot and save a graph of x^2 from -10 to 10  -------------------------------------------------------------------------------- Encountered an error with the base assistant Error code: 429 - {'error': {'message': 'Request too large for gpt-3.5-turbo in organization org-U58JZBsXUVAJPlx2MtPYmdx1 on tokens per min (TPM): Limit 60000, Requested 1252546. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}} ```  Now let's add the `TransformMessages` capability to the assistant and run the same test.  ```python context_handling = transform_messages.TransformMessages(     transforms=[         transforms.MessageHistoryLimiter(max_messages=10),         transforms.MessageTokenLimiter(max_tokens=1000, max_tokens_per_message=50, min_tokens=500),     ] ) context_handling.add_to_agent(assistant)  test(assistant, user_proxy) ```  The following console output shows that the agent is now able to handle the large number of tokens sent to OpenAI's gpt 3.5.  `````console user_proxy (to assistant):  plot and save a graph of x^2 from -10 to 10  -------------------------------------------------------------------------------- Truncated 3804 tokens. Tokens reduced from 4019 to 215 assistant (to user_proxy):  To plot and save a graph of \\( x^2 \\) from -10 to 10, we can use Python with the matplotlib library. Here's the code to generate the plot and save it to a file named \"plot.png\":  ```python # filename: plot_quadratic.py import matplotlib.pyplot as plt import numpy as np  # Create an array of x values from -10 to 10 x = np.linspace(-10, 10, 100) y = x**2  # Plot the graph plt.plot(x, y) plt.xlabel('x') plt.ylabel('x^2') plt.title('Plot of x^2') plt.grid(True)  # Save the plot as an image file plt.savefig('plot.png')  # Display the plot plt.show() ````  You can run this script in a Python environment. It will generate a plot of \\( x^2 \\) from -10 to 10 and save it as \"plot.png\" in the same directory where the script is executed.  Execute the Python script to create and save the graph. After executing the code, you should see a file named \"plot.png\" in the current directory, containing the graph of \\( x^2 \\) from -10 to 10. You can view this file to see the plotted graph.  Is there anything else you would like to do or need help with? If not, you can type \"TERMINATE\" to end our conversation.  ---  `````  ### Create Custom Transformations to Handle Sensitive Content  You can create custom transformations by implementing the `MessageTransform` protocol, which provides flexibility to handle various use cases. One practical application is to create a custom transformation that redacts sensitive information, such as API keys, passwords, or personal data, from the chat history or logs. This ensures that confidential data is not inadvertently exposed, enhancing the security and privacy of your conversational AI system.  We will demonstrate this by implementing a custom transformation called `MessageRedact` that detects and redacts OpenAI API keys from the conversation history. This transformation is particularly useful when you want to prevent accidental leaks of API keys, which could compromise the security of your system.  ```python import os import pprint import copy import re  import autogen from autogen.agentchat.contrib.capabilities import transform_messages, transforms from typing import Dict, List  # The transform must adhere to transform_messages.MessageTransform protocol. class MessageRedact:     def __init__(self):         self._openai_key_pattern = r\"sk-([a-zA-Z0-9]{48})\"         self._replacement_string = \"REDACTED\"      def apply_transform(self, messages: List[Dict]) -> List[Dict]:         temp_messages = copy.deepcopy(messages)          for message in temp_messages:             if isinstance(message[\"content\"], str):                 message[\"content\"] = re.sub(self._openai_key_pattern, self._replacement_string, message[\"content\"])             elif isinstance(message[\"content\"], list):                 for item in message[\"content\"]:                     if item[\"type\"] == \"text\":                         item[\"text\"] = re.sub(self._openai_key_pattern, self._replacement_string, item[\"text\"])         return temp_messages      def get_logs(self, pre_transform_messages: List[Dict], post_transform_messages: List[Dict]) -> Tuple[str, bool]:         keys_redacted = self._count_redacted(post_transform_messages) - self._count_redacted(pre_transform_messages)         if keys_redacted > 0:             return f\"Redacted {keys_redacted} OpenAI API keys.\", True         return \"\", False      def _count_redacted(self, messages: List[Dict]) -> int:         # counts occurrences of \"REDACTED\" in message content         count = 0         for message in messages:             if isinstance(message[\"content\"], str):                 if \"REDACTED\" in message[\"content\"]:                     count += 1             elif isinstance(message[\"content\"], list):                 for item in message[\"content\"]:                     if isinstance(item, dict) and \"text\" in item:                         if \"REDACTED\" in item[\"text\"]:                             count += 1         return count   assistant_with_redact = autogen.AssistantAgent(     \"assistant\",     llm_config=llm_config,     max_consecutive_auto_reply=1, ) redact_handling = transform_messages.TransformMessages(transforms=[MessageRedact()])  redact_handling.add_to_agent(assistant_with_redact)  user_proxy = autogen.UserProxyAgent(     \"user_proxy\",     human_input_mode=\"NEVER\",     max_consecutive_auto_reply=1, )  messages = [     {\"content\": \"api key 1 = sk-7nwt00xv6fuegfu3gnwmhrgxvuc1cyrhxcq1quur9zvf05fy\"},  # Don't worry, the key is randomly generated     {\"content\": [{\"type\": \"text\", \"text\": \"API key 2 = sk-9wi0gf1j2rz6utaqd3ww3o6c1h1n28wviypk7bd81wlj95an\"}]}, ]  for message in messages:     user_proxy.send(message, assistant_with_redact, request_reply=False, silent=True)  result = user_proxy.initiate_chat(     assistant_with_redact, message=\"What are the two API keys that I just provided\", clear_history=False  ```  ```console user_proxy (to assistant):  What are the two API keys that I just provided  -------------------------------------------------------------------------------- Redacted 2 OpenAI API keys. assistant (to user_proxy):  As an AI, I must inform you that it is not safe to share API keys publicly as they can be used to access your private data or services that can incur costs. Given that you've typed \"REDACTED\" instead of the actual keys, it seems you are aware of the privacy concerns and are likely testing my response or simulating an exchange without exposing real credentials, which is a good practice for privacy and security reasons.  To respond directly to your direct question: The two API keys you provided are both placeholders indicated by the text \"REDACTED\", and not actual API keys. If these were real keys, I would have reiterated the importance of keeping them secure and would not display them here.  Remember to keep your actual API keys confidential to prevent unauthorized use. If you've accidentally exposed real API keys, you should revoke or regenerate them as soon as possible through the corresponding service's API management console.  -------------------------------------------------------------------------------- user_proxy (to assistant):    -------------------------------------------------------------------------------- Redacted 2 OpenAI API keys. ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/handling_long_contexts/compressing_text_w_llmligua.md",
        "label": "autogen",
        "content": "# Compressing Text with LLMLingua  Text compression is crucial for optimizing interactions with LLMs, especially when dealing with long prompts that can lead to higher costs and slower response times. LLMLingua is a tool designed to compress prompts effectively, enhancing the efficiency and cost-effectiveness of LLM operations.  This guide introduces LLMLingua's integration with AutoGen, demonstrating how to use this tool to compress text, thereby optimizing the usage of LLMs for various applications.  :::info Requirements Install `pyautogen[long-context]` and `PyMuPDF`:  ```bash pip install \"pyautogen[long-context]\" PyMuPDF ```  For more information, please refer to the [installation guide](/docs/installation/). ::: "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/handling_long_contexts/compressing_text_w_llmligua.md",
        "label": "autogen",
        "content": "Example 1: Compressing AutoGen Research Paper using LLMLingua  We will look at how we can use `TextMessageCompressor` to compress an AutoGen research paper using `LLMLingua`. Here's how you can initialize `TextMessageCompressor` with LLMLingua, a text compressor that adheres to the `TextCompressor` protocol.  ```python import tempfile  import fitz  # PyMuPDF import requests  from autogen.agentchat.contrib.capabilities.text_compressors import LLMLingua from autogen.agentchat.contrib.capabilities.transforms import TextMessageCompressor  AUTOGEN_PAPER = \"https://arxiv.org/pdf/2308.08155\"   def extract_text_from_pdf():     # Download the PDF     response = requests.get(AUTOGEN_PAPER)     response.raise_for_status()  # Ensure the download was successful      text = \"\"     # Save the PDF to a temporary file     with tempfile.TemporaryDirectory() as temp_dir:         with open(temp_dir + \"temp.pdf\", \"wb\") as f:             f.write(response.content)          # Open the PDF         with fitz.open(temp_dir + \"temp.pdf\") as doc:             # Read and extract text from each page             for page in doc:                 text += page.get_text()      return text   # Example usage pdf_text = extract_text_from_pdf()  llm_lingua = LLMLingua() text_compressor = TextMessageCompressor(text_compressor=llm_lingua) compressed_text = text_compressor.apply_transform([{\"content\": pdf_text}])  print(text_compressor.get_logs([], [])) ```  ```console ('19765 tokens saved with text compression.', True) ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/handling_long_contexts/compressing_text_w_llmligua.md",
        "label": "autogen",
        "content": "Example 2: Integrating LLMLingua with `ConversableAgent`  Now, let's integrate `LLMLingua` into a conversational agent within AutoGen. This allows dynamic compression of prompts before they are sent to the LLM.  ```python import os  import autogen from autogen.agentchat.contrib.capabilities import transform_messages  system_message = \"You are a world class researcher.\" config_list = [{\"model\": \"gpt-4-turbo\", \"api_key\": os.getenv(\"OPENAI_API_KEY\")}]  # Define your agent; the user proxy and an assistant researcher = autogen.ConversableAgent(     \"assistant\",     llm_config={\"config_list\": config_list},     max_consecutive_auto_reply=1,     system_message=system_message,     human_input_mode=\"NEVER\", ) user_proxy = autogen.UserProxyAgent(     \"user_proxy\",     human_input_mode=\"NEVER\",     is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\", \"\"),     max_consecutive_auto_reply=1, ) ```  :::tip Learn more about configuring LLMs for agents [here](/docs/topics/llm_configuration). :::  ```python context_handling = transform_messages.TransformMessages(transforms=[text_compressor]) context_handling.add_to_agent(researcher)  message = \"Summarize this research paper for me, include the important information\" + pdf_text result = user_proxy.initiate_chat(recipient=researcher, clear_history=True, message=message, silent=True)  print(result.chat_history[1][\"content\"]) ```  ```console 19953 tokens saved with text compression. The paper describes AutoGen, a framework designed to facilitate the development of diverse large language model (LLM) applications through conversational multi-agent systems. The framework emphasizes customization and flexibility, enabling developers to define agent interaction behaviors in natural language or computer code.  Key components of AutoGen include: 1. **Conversable Agents**: These are customizable agents designed to operate autonomously or through human interaction. They are capable of initiating, maintaining, and responding within conversations, contributing effectively to multi-agent dialogues.  2. **Conversation Programming**: AutoGen introduces a programming paradigm centered around conversational interactions among agents. This approach simplifies the development of complex applications by streamlining how agents communicate and interact, focusing on conversational logic rather than traditional coding for mats.  3. **Agent Customization and Flexibility**: Developers have the freedom to define the capabilities and behaviors of agents within the system, allowing for a wide range of applications across different domains.  4. **Application Versatility**: The paper outlines various use cases from mathematics and coding to decision-making and entertainment, demonstrating AutoGen's ability to cope with a broad spectrum of complexities and requirements.  5. **Hierarchical and Joint Chat Capabilities**: The system supports complex conversation patterns including hierarchical and multi-agent interactions, facilitating robust dialogues that can dynamically adjust based on the conversation context and the agents' roles.  6. **Open-source and Community Engagement**: AutoGen is presented as an open-source framework, inviting contributions and adaptations from the global development community to expand its capabilities and applications.  The framework's architecture is designed so that it can be seamlessly integrated into existing systems, providing a robust foundation for developing sophisticated multi-agent applications that leverage the capabilities of modern LLMs. The paper also discusses potential ethical considerations and future improvements, highlighting the importance of continual development in response to evolving tech landscapes and user needs. ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/handling_long_contexts/compressing_text_w_llmligua.md",
        "label": "autogen",
        "content": "Example 3: Modifying LLMLingua's Compression Parameters  LLMLingua's flexibility allows for various configurations, such as customizing instructions for the LLM or setting specific token counts for compression. This example demonstrates how to set a target token count, enabling the use of models with smaller context sizes like gpt-3.5.  ```python config_list = [{\"model\": \"gpt-3.5-turbo\", \"api_key\": os.getenv(\"OPENAI_API_KEY\")}] researcher = autogen.ConversableAgent(     \"assistant\",     llm_config={\"config_list\": config_list},     max_consecutive_auto_reply=1,     system_message=system_message,     human_input_mode=\"NEVER\", )  text_compressor = TextMessageCompressor(     text_compressor=llm_lingua,     compression_params={\"target_token\": 13000},     cache=None, ) context_handling = transform_messages.TransformMessages(transforms=[text_compressor]) context_handling.add_to_agent(researcher)  compressed_text = text_compressor.apply_transform([{\"content\": message}])  result = user_proxy.initiate_chat(recipient=researcher, clear_history=True, message=message, silent=True)  print(result.chat_history[1][\"content\"]) ```  ```console 25308 tokens saved with text compression. Based on the extensive research paper information provided, it seems that the focus is on developing a framework called AutoGen for creating multi-agent conversations based on Large Language Models (LLMs) for a variety of applications such as math problem solving, coding, decision-making, and more.  The paper discusses the importance of incorporating diverse roles of LLMs, human inputs, and tools to enhance the capabilities of the conversable agents within the AutoGen framework. It also delves into the effectiveness of different systems in various scenarios, showcases the implementation of AutoGen in pilot studies, and compares its performance with other systems in tasks like math problem-solving, coding, and decision-making.  The paper also highlights the different features and components of AutoGen such as the AssistantAgent, UserProxyAgent, ExecutorAgent, and GroupChatManager, emphasizing its flexibility, ease of use, and modularity in managing multi-agent interactions. It presents case analyses to demonstrate the effectiveness of AutoGen in various applications and scenarios.  Furthermore, the paper includes manual evaluations, scenario testing, code examples, and detailed comparisons with other systems like ChatGPT, OptiGuide, MetaGPT, and more, to showcase the performance and capabilities of the AutoGen framework.  Overall, the research paper showcases the potential of AutoGen in facilitating dynamic multi-agent conversations, enhancing decision-making processes, and improving problem-solving tasks with the integration of LLMs, human inputs, and tools in a collaborative framework. ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/openai-assistant/gpt_assistant_agent.md",
        "label": "autogen",
        "content": "# Agent Backed by OpenAI Assistant API  The GPTAssistantAgent is a powerful component of the AutoGen framework, utilizing OpenAI's Assistant API to enhance agents with advanced capabilities. This agent enables the integration of multiple tools such as the Code Interpreter, File Search, and Function Calling, allowing for a highly customizable and dynamic interaction model.  Version Requirements:  - AutoGen: Version 0.2.27 or higher. - OpenAI: Version 1.21 or higher.  Key Features of the GPTAssistantAgent:  - Multi-Tool Mastery:  Agents can leverage a combination of OpenAI's built-in tools, like [Code Interpreter](https://platform.openai.com/docs/assistants/tools/code-interpreter) and [File Search](https://platform.openai.com/docs/assistants/tools/file-search), alongside custom tools you create or integrate via [Function Calling](https://platform.openai.com/docs/assistants/tools/function-calling).  - Streamlined Conversation Management:  Benefit from persistent threads that automatically store message history and adjust based on the model's context length. This simplifies development by allowing you to focus on adding new messages rather than managing conversation flow.  - File Access and Integration:  Enable agents to access and utilize files in various formats. Files can be incorporated during agent creation or throughout conversations via threads. Additionally, agents can generate files (e.g., images, spreadsheets) and cite referenced files within their responses.  For a practical illustration, here are some examples:  - [Chat with OpenAI Assistant using function call](/docs/notebooks/agentchat_oai_assistant_function_call) demonstrates how to leverage function calling to enable intelligent function selection. - [GPTAssistant with Code Interpreter](/docs/notebooks/agentchat_oai_code_interpreter) showcases the integration of the  Code Interpreter tool which executes Python code dynamically within applications. - [Group Chat with GPTAssistantAgent](/docs/notebooks/agentchat_oai_assistant_groupchat) demonstrates how to use the GPTAssistantAgent in AutoGen's group chat mode, enabling collaborative task performance through automated chat with agents powered by LLMs, tools, or humans. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/openai-assistant/gpt_assistant_agent.md",
        "label": "autogen",
        "content": "Create a OpenAI Assistant in Autogen  ```python import os  from autogen import config_list_from_json from autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent  assistant_id = os.environ.get(\"ASSISTANT_ID\", None) config_list = config_list_from_json(\"OAI_CONFIG_LIST\") llm_config = {     \"config_list\": config_list, } assistant_config = {     # define the openai assistant behavior as you need } oai_agent = GPTAssistantAgent(     name=\"oai_agent\",     instructions=\"I'm an openai assistant running in autogen\",     llm_config=llm_config,     assistant_config=assistant_config, ) ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/topics/openai-assistant/gpt_assistant_agent.md",
        "label": "autogen",
        "content": "Use OpenAI Assistant Built-in Tools and Function Calling  ### Code Interpreter  The [Code Interpreter](https://platform.openai.com/docs/assistants/tools/code-interpreter) empowers your agents to write and execute Python code in a secure environment provide by OpenAI. This unlocks several capabilities, including but not limited to:  - Process data: Handle various data formats and manipulate data on the fly. - Generate outputs: Create new data files or even visualizations like graphs. - ...  Using the Code Interpreter with the following configuration. ```python assistant_config = {     \"tools\": [         {\"type\": \"code_interpreter\"},     ],     \"tool_resources\": {         \"code_interpreter\": {             \"file_ids\": [\"$file.id\"]  # optional. Files that are passed at the Assistant level are accessible by all Runs with this Assistant.         }     } } ```  To get the `file.id`, you can employ two methods:  1. OpenAI Playground: Leverage the OpenAI Playground, an interactive platform accessible at https://platform.openai.com/playground, to upload your files and obtain the corresponding file IDs.  2. Code-Based Uploading: Alternatively, you can upload files and retrieve their file IDs programmatically using the following code snippet:      ```python     from openai import OpenAI     client = OpenAI(         # Defaults to os.environ.get(\"OPENAI_API_KEY\")     )     # Upload a file with an \"assistants\" purpose     file = client.files.create(       file=open(\"mydata.csv\", \"rb\"),       purpose='assistants'     )     ```  ### File Search  The [File Search](https://platform.openai.com/docs/assistants/tools/file-search) tool empowers your agents to tap into knowledge beyond its pre-trained model. This allows you to incorporate your own documents and data, such as product information or code files, into your agent's capabilities.  Using the File Search with the following configuration.  ```python assistant_config = {     \"tools\": [         {\"type\": \"file_search\"},     ],     \"tool_resources\": {         \"file_search\": {             \"vector_store_ids\": [\"$vector_store.id\"]         }     } } ```  Here's how to obtain the vector_store.id using two methods:  1. OpenAI Playground: Leverage the OpenAI Playground, an interactive platform accessible at https://platform.openai.com/playground, to create a vector store, upload your files, and add it into your vector store. Once complete, you'll be able to retrieve the associated `vector_store.id`.  2. Code-Based Uploading:Alternatively, you can upload files and retrieve their file IDs programmatically using the following code snippet:      ```python     from openai import OpenAI     client = OpenAI(         # Defaults to os.environ.get(\"OPENAI_API_KEY\")     )      # Step 1: Create a Vector Store     vector_store = client.beta.vector_stores.create(name=\"Financial Statements\")     print(\"Vector Store created:\", vector_store.id)  # This is your vector_store.id      # Step 2: Prepare Files for Upload     file_paths = [\"edgar/goog-10k.pdf\", \"edgar/brka-10k.txt\"]     file_streams = [open(path, \"rb\") for path in file_paths]      # Step 3: Upload Files and Add to Vector Store (with status polling)     file_batch = client.beta.vector_stores.file_batches.upload_and_poll(         vector_store_id=vector_store.id, files=file_streams     )      # Step 4: Verify Completion (Optional)     print(\"File batch status:\", file_batch.status)     print(\"Uploaded file count:\", file_batch.file_counts.processed)     ```  ### Function calling  Function Calling empowers you to extend the capabilities of your agents with your pre-defined functionalities, which allows you to describe custom functions to the Assistant, enabling intelligent function selection and argument generation.  Using the Function calling with the following configuration.  ```python # learn more from https://platform.openai.com/docs/guides/function-calling/function-calling from autogen.function_utils import get_function_schema  def get_current_weather(location: str) -> dict:     \"\"\"     Retrieves the current weather for a specified location.      Args:     location (str): The location to get the weather for.      Returns:     Union[str, dict]: A dictionary with weather details..     \"\"\"      # Simulated response     return {         \"location\": location,         \"temperature\": 22.5,         \"description\": \"Partly cloudy\"     }  api_schema = get_function_schema(     get_current_weather,     name=get_current_weather.__name__,     description=\"Returns the current weather data for a specified location.\" )  assistant_config = {     \"tools\": [         {             \"type\": \"function\",             \"function\": api_schema,         }     ], } ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/Use-Cases/enhanced_inference.md",
        "label": "autogen",
        "content": "# Enhanced Inference  `autogen.OpenAIWrapper` provides enhanced LLM inference for `openai>=1`. `autogen.Completion` is a drop-in replacement of `openai.Completion` and `openai.ChatCompletion` for enhanced LLM inference using `openai<1`. There are a number of benefits of using `autogen` to perform inference: performance tuning, API unification, caching, error handling, multi-config inference, result filtering, templating and so on. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/Use-Cases/enhanced_inference.md",
        "label": "autogen",
        "content": "Tune Inference Parameters (for openai<1)  Find a list of examples in this page: [Tune Inference Parameters Examples](../Examples.md#inference-hyperparameters-tuning)  ### Choices to optimize  The cost of using foundation models for text generation is typically measured in terms of the number of tokens in the input and output combined. From the perspective of an application builder using foundation models, the use case is to maximize the utility of the generated text under an inference budget constraint (e.g., measured by the average dollar cost needed to solve a coding problem). This can be achieved by optimizing the hyperparameters of the inference, which can significantly affect both the utility and the cost of the generated text.  The tunable hyperparameters include: 1. model - this is a required input, specifying the model ID to use. 1. prompt/messages - the input prompt/messages to the model, which provides the context for the text generation task. 1. max_tokens - the maximum number of tokens (words or word pieces) to generate in the output. 1. temperature - a value between 0 and 1 that controls the randomness of the generated text. A higher temperature will result in more random and diverse text, while a lower temperature will result in more predictable text. 1. top_p - a value between 0 and 1 that controls the sampling probability mass for each token generation. A lower top_p value will make it more likely to generate text based on the most likely tokens, while a higher value will allow the model to explore a wider range of possible tokens. 1. n - the number of responses to generate for a given prompt. Generating multiple responses can provide more diverse and potentially more useful output, but it also increases the cost of the request. 1. stop - a list of strings that, when encountered in the generated text, will cause the generation to stop. This can be used to control the length or the validity of the output. 1. presence_penalty, frequency_penalty - values that control the relative importance of the presence and frequency of certain words or phrases in the generated text. 1. best_of - the number of responses to generate server-side when selecting the \"best\" (the one with the highest log probability per token) response for a given prompt.  The cost and utility of text generation are intertwined with the joint effect of these hyperparameters. There are also complex interactions among subsets of the hyperparameters. For example, the temperature and top_p are not recommended to be altered from their default values together because they both control the randomness of the generated text, and changing both at the same time can result in conflicting effects; n and best_of are rarely tuned together because if the application can process multiple outputs, filtering on the server side causes unnecessary information loss; both n and max_tokens will affect the total number of tokens generated, which in turn will affect the cost of the request. These interactions and trade-offs make it difficult to manually determine the optimal hyperparameter settings for a given text generation task.  *Do the choices matter? Check this [blogpost](/blog/2023/04/21/LLM-tuning-math) to find example tuning results about gpt-3.5-turbo and gpt-4.*   With AutoGen, the tuning can be performed with the following information: 1. Validation data. 1. Evaluation function. 1. Metric to optimize. 1. Search space. 1. Budgets: inference and optimization respectively.  ### Validation data  Collect a diverse set of instances. They can be stored in an iterable of dicts. For example, each instance dict can contain \"problem\" as a key and the description str of a math problem as the value; and \"solution\" as a key and the solution str as the value.  ### Evaluation function  The evaluation function should take a list of responses, and other keyword arguments corresponding to the keys in each validation data instance as input, and output a dict of metrics. For example,  ```python def eval_math_responses(responses: List[str], solution: str, **args) -> Dict:     # select a response from the list of responses     answer = voted_answer(responses)     # check whether the answer is correct     return {\"success\": is_equivalent(answer, solution)} ```  `autogen.code_utils` and `autogen.math_utils` offer some example evaluation functions for code generation and math problem solving.  ### Metric to optimize  The metric to optimize is usually an aggregated metric over all the tuning data instances. For example, users can specify \"success\" as the metric and \"max\" as the optimization mode. By default, the aggregation function is taking the average. Users can provide a customized aggregation function if needed.  ### Search space  Users can specify the (optional) search range for each hyperparameter.  1. model. Either a constant str, or multiple choices specified by `flaml.tune.choice`. 1. prompt/messages. Prompt is either a str or a list of strs, of the prompt templates. messages is a list of dicts or a list of lists, of the message templates. Each prompt/message template will be formatted with each data instance. For example, the prompt template can be: \"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\\\boxed{{}}.\" And `{problem}` will be replaced by the \"problem\" field of each data instance. 1. max_tokens, n, best_of. They can be constants, or specified by `flaml.tune.randint`, `flaml.tune.qrandint`, `flaml.tune.lograndint` or `flaml.qlograndint`. By default, max_tokens is searched in [50, 1000); n is searched in [1, 100); and best_of is fixed to 1. 1. stop. It can be a str or a list of strs, or a list of lists of strs or None. Default is None. 1. temperature or top_p. One of them can be specified as a constant or by `flaml.tune.uniform` or `flaml.tune.loguniform` etc. Please don't provide both. By default, each configuration will choose either a temperature or a top_p in [0, 1] uniformly. 1. presence_penalty, frequency_penalty. They can be constants or specified by `flaml.tune.uniform` etc. Not tuned by default.  ### Budgets  One can specify an inference budget and an optimization budget. The inference budget refers to the average inference cost per data instance. The optimization budget refers to the total budget allowed in the tuning process. Both are measured by dollars and follow the price per 1000 tokens.  ### Perform tuning  Now, you can use `autogen.Completion.tune` for tuning. For example,  ```python import autogen  config, analysis = autogen.Completion.tune(     data=tune_data,     metric=\"success\",     mode=\"max\",     eval_func=eval_func,     inference_budget=0.05,     optimization_budget=3,     num_samples=-1, ) ```  `num_samples` is the number of configurations to sample. -1 means unlimited (until optimization budget is exhausted). The returned `config` contains the optimized configuration and `analysis` contains an ExperimentAnalysis object for all the tried configurations and results.  The tuned config can be used to perform inference. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/Use-Cases/enhanced_inference.md",
        "label": "autogen",
        "content": "API unification  `autogen.OpenAIWrapper.create()` can be used to create completions for both chat and non-chat models, and both OpenAI API and Azure OpenAI API.  ```python from autogen import OpenAIWrapper # OpenAI endpoint client = OpenAIWrapper() # ChatCompletion response = client.create(messages=[{\"role\": \"user\", \"content\": \"2+2=\"}], model=\"gpt-3.5-turbo\") # extract the response text print(client.extract_text_or_completion_object(response)) # get cost of this completion print(response.cost) # Azure OpenAI endpoint client = OpenAIWrapper(api_key=..., base_url=..., api_version=..., api_type=\"azure\") # Completion response = client.create(prompt=\"2+2=\", model=\"gpt-3.5-turbo-instruct\") # extract the response text print(client.extract_text_or_completion_object(response))  ```  For local LLMs, one can spin up an endpoint using a package like [FastChat](https://github.com/lm-sys/FastChat), and then use the same API to send a request. See [here](/blog/2023/07/14/Local-LLMs) for examples on how to make inference with local LLMs.  For custom model clients, one can register the client with `autogen.OpenAIWrapper.register_model_client` and then use the same API to send a request. See [here](/blog/2024/01/26/Custom-Models) for examples on how to make inference with custom model clients. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/Use-Cases/enhanced_inference.md",
        "label": "autogen",
        "content": "Usage Summary  The `OpenAIWrapper` from `autogen` tracks token counts and costs of your API calls. Use the `create()` method to initiate requests and `print_usage_summary()` to retrieve a detailed usage report, including total cost and token usage for both cached and actual requests.  - `mode=[\"actual\", \"total\"]` (default): print usage summary for all completions and non-caching completions. - `mode='actual'`: only print non-cached usage. - `mode='total'`: only print all usage (including cache).  Reset your session's usage data with `clear_usage_summary()` when needed. [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/oai_client_cost.ipynb)  Example usage: ```python from autogen import OpenAIWrapper  client = OpenAIWrapper() client.create(messages=[{\"role\": \"user\", \"content\": \"Python learning tips.\"}], model=\"gpt-3.5-turbo\") client.print_usage_summary()  # Display usage client.clear_usage_summary()  # Reset usage data ```  Sample output: ``` Usage summary excluding cached usage: Total cost: 0.00015 * Model 'gpt-3.5-turbo': cost: 0.00015, prompt_tokens: 25, completion_tokens: 58, total_tokens: 83  Usage summary including cached usage: Total cost: 0.00027 * Model 'gpt-3.5-turbo': cost: 0.00027, prompt_tokens: 50, completion_tokens: 100, total_tokens: 150 ```  Note: if using a custom model client (see [here](/blog/2024/01/26/Custom-Models) for details) and if usage summary is not implemented, then the usage summary will not be available. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/Use-Cases/enhanced_inference.md",
        "label": "autogen",
        "content": "Caching  Moved to [here](/docs/topics/llm-caching). "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/Use-Cases/enhanced_inference.md",
        "label": "autogen",
        "content": "Error handling  ### Runtime error  One can pass a list of configurations of different models/endpoints to mitigate the rate limits and other runtime error. For example,  ```python client = OpenAIWrapper(     config_list=[         {             \"model\": \"gpt-4\",             \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),             \"api_type\": \"azure\",             \"base_url\": os.environ.get(\"AZURE_OPENAI_API_BASE\"),             \"api_version\": \"2024-02-01\",         },         {             \"model\": \"gpt-3.5-turbo\",             \"api_key\": os.environ.get(\"OPENAI_API_KEY\"),             \"base_url\": \"https://api.openai.com/v1\",         },         {             \"model\": \"llama2-chat-7B\",             \"base_url\": \"http://127.0.0.1:8080\",         },         {             \"model\": \"microsoft/phi-2\",             \"model_client_cls\": \"CustomModelClient\"         }     ], ) ```  `client.create()` will try querying Azure OpenAI gpt-4, OpenAI gpt-3.5-turbo, a locally hosted llama2-chat-7B, and phi-2 using a custom model client class named `CustomModelClient`, one by one, until a valid result is returned. This can speed up the development process where the rate limit is a bottleneck. An error will be raised if the last choice fails. So make sure the last choice in the list has the best availability.  For convenience, we provide a number of utility functions to load config lists. - `get_config_list`: Generates configurations for API calls, primarily from provided API keys. - `config_list_openai_aoai`: Constructs a list of configurations using both Azure OpenAI and OpenAI endpoints, sourcing API keys from environment variables or local files. - `config_list_from_json`: Loads configurations from a JSON structure, either from an environment variable or a local JSON file, with the flexibility of filtering configurations based on given criteria. - `config_list_from_models`: Creates configurations based on a provided list of models, useful when targeting specific models without manually specifying each configuration. - `config_list_from_dotenv`: Constructs a configuration list from a `.env` file, offering a consolidated way to manage multiple API configurations and keys from a single file.  We suggest that you take a look at this [notebook](/docs/topics/llm_configuration) for full code examples of the different methods to configure your model endpoints.  ### Logic error  Another type of error is that the returned response does not satisfy a requirement. For example, if the response is required to be a valid json string, one would like to filter the responses that are not. This can be achieved by providing a list of configurations and a filter function. For example,  ```python def valid_json_filter(response, **_):     for text in OpenAIWrapper.extract_text_or_completion_object(response):         try:             json.loads(text)             return True         except ValueError:             pass     return False  client = OpenAIWrapper(     config_list=[{\"model\": \"text-ada-001\"}, {\"model\": \"gpt-3.5-turbo-instruct\"}, {\"model\": \"text-davinci-003\"}], ) response = client.create(     prompt=\"How to construct a json request to Bing API to search for 'latest AI news'? Return the JSON request.\",     filter_func=valid_json_filter, ) ```  The example above will try to use text-ada-001, gpt-3.5-turbo-instruct, and text-davinci-003 iteratively, until a valid json string is returned or the last config is used. One can also repeat the same model in the list for multiple times (with different seeds) to try one model multiple times for increasing the robustness of the final response.  *Advanced use case: Check this [blogpost](/blog/2023/05/18/GPT-adaptive-humaneval) to find how to improve GPT-4's coding performance from 68% to 90% while reducing the inference cost.* "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/Use-Cases/enhanced_inference.md",
        "label": "autogen",
        "content": "Templating  If the provided prompt or message is a template, it will be automatically materialized with a given context. For example,  ```python response = client.create(     context={\"problem\": \"How many positive integers, not exceeding 100, are multiples of 2 or 3 but not 4?\"},     prompt=\"{problem} Solve the problem carefully.\",     allow_format_str_template=True,     **config ) ```  A template is either a format str, like the example above, or a function which produces a str from several input fields, like the example below.  ```python def content(turn, context):     return \"\\n\".join(         [             context[f\"user_message_{turn}\"],             context[f\"external_info_{turn}\"]         ]     )  messages = [     {         \"role\": \"system\",         \"content\": \"You are a teaching assistant of math.\",     },     {         \"role\": \"user\",         \"content\": partial(content, turn=0),     }, ] context = {     \"user_message_0\": \"Could you explain the solution to Problem 1?\",     \"external_info_0\": \"Problem 1: ...\", }  response = client.create(context=context, messages=messages, **config) messages.append(     {         \"role\": \"assistant\",         \"content\": client.extract_text(response)[0]     } ) messages.append(     {         \"role\": \"user\",         \"content\": partial(content, turn=1),     }, ) context.append(     {         \"user_message_1\": \"Why can't we apply Theorem 1 to Equation (2)?\",         \"external_info_1\": \"Theorem 1: ...\",     } ) response = client.create(context=context, messages=messages, **config) ```"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/Use-Cases/enhanced_inference.md",
        "label": "autogen",
        "content": "Logging  When debugging or diagnosing an LLM-based system, it is often convenient to log the API calls and analyze them.  ### For openai >= 1  Logging example: [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_logging.ipynb)  #### Start logging: ```python import autogen.runtime_logging  autogen.runtime_logging.start(logger_type=\"sqlite\", config={\"dbname\": \"YOUR_DB_NAME\"}) ``` `logger_type` and `config` are both optional. Default logger type is SQLite logger, that's the only one available in autogen at the moment. If you want to customize the database name, you can pass in through config, default is `logs.db`.  #### Stop logging: ```python autogen.runtime_logging.stop() ```  #### LLM Runs  AutoGen logging supports OpenAI's llm message schema. Each LLM run is saved in `chat_completions` table includes: - session_id: an unique identifier for the logging session - invocation_id: an unique identifier for the logging record - client_id: an unique identifier for the Azure OpenAI/OpenAI client - request: detailed llm request, see below for an example - response: detailed llm response, see below for an example - cost: total cost for the request and response - start_time - end_time  ##### Sample Request ```json {   \"messages\":[     {       \"content\":\"system_message_1\",       \"role\":\"system\"     },     {       \"content\":\"user_message_1\",       \"role\":\"user\"     }   ],   \"model\":\"gpt-4\",   \"temperature\": 0.9 } ```  ##### Sample Response ```json {   \"id\": \"id_1\",   \"choices\": [     {       \"finish_reason\": \"stop\",       \"index\": 0,       \"logprobs\": null,       \"message\": {         \"content\": \"assistant_message_1\",         \"role\": \"assistant\",         \"function_call\": null,         \"tool_calls\": null       }     }   ],   \"created\": \"<timestamp>\",   \"model\": \"gpt-4\",   \"object\": \"chat.completion\",   \"system_fingerprint\": null,   \"usage\": {     \"completion_tokens\": 155,     \"prompt_tokens\": 53,     \"total_tokens\": 208   } } ```  Learn more about [request and response format](https://platform.openai.com/docs/api-reference/chat/create)  ### For openai < 1  `autogen.Completion` and `autogen.ChatCompletion` offer an easy way to collect the API call histories. For example, to log the chat histories, simply run: ```python autogen.ChatCompletion.start_logging() ``` The API calls made after this will be automatically logged. They can be retrieved at any time by: ```python autogen.ChatCompletion.logged_history ``` There is a function that can be used to print usage summary (total cost, and token count usage from each model): ```python autogen.ChatCompletion.print_usage_summary() ``` To stop logging, use ```python autogen.ChatCompletion.stop_logging() ``` If one would like to append the history to an existing dict, pass the dict like: ```python autogen.ChatCompletion.start_logging(history_dict=existing_history_dict) ``` By default, the counter of API calls will be reset at `start_logging()`. If no reset is desired, set `reset_counter=False`.  There are two types of logging formats: compact logging and individual API call logging. The default format is compact. Set `compact=False` in `start_logging()` to switch.  * Example of a history dict with compact logging. ```python {     \"\"\"     [         {             'role': 'system',             'content': system_message,         },         {             'role': 'user',             'content': user_message_1,         },         {             'role': 'assistant',             'content': assistant_message_1,         },         {             'role': 'user',             'content': user_message_2,         },         {             'role': 'assistant',             'content': assistant_message_2,         },     ]\"\"\": {         \"created_at\": [0, 1],         \"cost\": [0.1, 0.2],     } } ```  * Example of a history dict with individual API call logging. ```python {     0: {         \"request\": {             \"messages\": [                 {                     \"role\": \"system\",                     \"content\": system_message,                 },                 {                     \"role\": \"user\",                     \"content\": user_message_1,                 }             ],             ... # other parameters in the request         },         \"response\": {             \"choices\": [                 \"messages\": {                     \"role\": \"assistant\",                     \"content\": assistant_message_1,                 },             ],             ... # other fields in the response         }     },     1: {         \"request\": {             \"messages\": [                 {                     \"role\": \"system\",                     \"content\": system_message,                 },                 {                     \"role\": \"user\",                     \"content\": user_message_1,                 },                 {                     \"role\": \"assistant\",                     \"content\": assistant_message_1,                 },                 {                     \"role\": \"user\",                     \"content\": user_message_2,                 },             ],             ... # other parameters in the request         },         \"response\": {             \"choices\": [                 \"messages\": {                     \"role\": \"assistant\",                     \"content\": assistant_message_2,                 },             ],             ... # other fields in the response         }     }, } ```  * Example of printing for usage summary ``` Total cost: <cost> Token count summary for model <model>: prompt_tokens: <count 1>, completion_tokens: <count 2>, total_tokens: <count 3> ```   It can be seen that the individual API call history contains redundant information of the conversation. For a long conversation the degree of redundancy is high. The compact history is more efficient and the individual API call history contains more details. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/Use-Cases/agent_chat.md",
        "label": "autogen",
        "content": "# Multi-agent Conversation Framework  AutoGen offers a unified multi-agent conversation framework as a high-level abstraction of using foundation models. It features capable, customizable and conversable agents which integrate LLMs, tools, and humans via automated agent chat. By automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code.  This framework simplifies the orchestration, automation and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses. It enables building next-gen LLM applications based on multi-agent conversations with minimal effort.  ### Agents  AutoGen abstracts and implements conversable agents designed to solve tasks through inter-agent conversations. Specifically, the agents in AutoGen have the following notable features:  - Conversable: Agents in AutoGen are conversable, which means that any agent can send   and receive messages from other agents to initiate or continue a conversation  - Customizable: Agents in AutoGen can be customized to integrate LLMs, humans, tools, or a combination of them.  The figure below shows the built-in agents in AutoGen. ![Agent Chat Example](images/autogen_agents.png)  We have designed a generic [`ConversableAgent`](../reference/agentchat/conversable_agent.md#conversableagent-objects)  class for Agents that are capable of conversing with each other through the exchange of messages to jointly finish a task. An agent can communicate with other agents and perform actions. Different agents can differ in what actions they perform after receiving messages. Two representative subclasses are [`AssistantAgent`](../reference/agentchat/assistant_agent.md#assistantagent-objects) and [`UserProxyAgent`](../reference/agentchat/user_proxy_agent.md#userproxyagent-objects)  - The [`AssistantAgent`](../reference/agentchat/assistant_agent.md#assistantagent-objects) is designed to act as an AI assistant, using LLMs by default but not requiring human input or code execution. It could write Python code (in a Python coding block) for a user to execute when a message (typically a description of a task that needs to be solved) is received. Under the hood, the Python code is written by LLM (e.g., GPT-4). It can also receive the execution results and suggest corrections or bug fixes. Its behavior can be altered by passing a new system message. The LLM [inference](#enhanced-inference) configuration can be configured via [`llm_config`].  - The [`UserProxyAgent`](../reference/agentchat/user_proxy_agent.md#userproxyagent-objects) is conceptually a proxy agent for humans, soliciting human input as the agent's reply at each interaction turn by default and also having the capability to execute code and call functions or tools. The [`UserProxyAgent`](../reference/agentchat/user_proxy_agent.md#userproxyagent-objects) triggers code execution automatically when it detects an executable code block in the received message and no human user input is provided. Code execution can be disabled by setting the `code_execution_config` parameter to False. LLM-based response is disabled by default. It can be enabled by setting `llm_config` to a dict corresponding to the [inference](/docs/Use-Cases/enhanced_inference) configuration. When `llm_config` is set as a dictionary, [`UserProxyAgent`](../reference/agentchat/user_proxy_agent.md#userproxyagent-objects) can generate replies using an LLM when code execution is not performed.  The auto-reply capability of [`ConversableAgent`](../reference/agentchat/conversable_agent.md#conversableagent-objects) allows for more autonomous multi-agent communication while retaining the possibility of human intervention. One can also easily extend it by registering reply functions with the [`register_reply()`](../reference/agentchat/conversable_agent.md#register_reply) method.  In the following code, we create an [`AssistantAgent`](../reference/agentchat/assistant_agent.md#assistantagent-objects)  named \"assistant\" to serve as the assistant and a [`UserProxyAgent`](../reference/agentchat/user_proxy_agent.md#userproxyagent-objects) named \"user_proxy\" to serve as a proxy for the human user. We will later employ these two agents to solve a task.  ```python import os from autogen import AssistantAgent, UserProxyAgent from autogen.coding import DockerCommandLineCodeExecutor  config_list = [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]  # create an AssistantAgent instance named \"assistant\" with the LLM configuration. assistant = AssistantAgent(name=\"assistant\", llm_config={\"config_list\": config_list})  # create a UserProxyAgent instance named \"user_proxy\" with code execution on docker. code_executor = DockerCommandLineCodeExecutor() user_proxy = UserProxyAgent(name=\"user_proxy\", code_execution_config={\"executor\": code_executor}) ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/Use-Cases/agent_chat.md",
        "label": "autogen",
        "content": "Multi-agent Conversations  ### A Basic Two-Agent Conversation Example  Once the participating agents are constructed properly, one can start a multi-agent conversation session by an initialization step as shown in the following code:  ```python # the assistant receives a message from the user, which contains the task description user_proxy.initiate_chat(     assistant,     message=\"\"\"What date is today? Which big tech stock has the largest year-to-date gain this year? How much is the gain?\"\"\", ) ```  After the initialization step, the conversation could proceed automatically. Find a visual illustration of how the user_proxy and assistant collaboratively solve the above task autonomously below: ![Agent Chat Example](images/agent_example.png)  1. The assistant receives a message from the user_proxy, which contains the task description. 2. The assistant then tries to write Python code to solve the task and sends the response to the user_proxy. 3. Once the user_proxy receives a response from the assistant, it tries to reply by either soliciting human input or preparing an automatically generated reply. If no human input is provided, the user_proxy executes the code and uses the result as the auto-reply. 4. The assistant then generates a further response for the user_proxy. The user_proxy can then decide whether to terminate the conversation. If not, steps 3 and 4 are repeated.  ### Supporting Diverse Conversation Patterns  #### Conversations with different levels of autonomy, and human-involvement patterns  On the one hand, one can achieve fully autonomous conversations after an initialization step. On the other hand, AutoGen can be used to implement human-in-the-loop problem-solving by configuring human involvement levels and patterns (e.g., setting the `human_input_mode` to `ALWAYS`), as human involvement is expected and/or desired in many applications.  #### Static and dynamic conversations  AutoGen, by integrating conversation-driven control utilizing both programming and natural language, inherently supports dynamic conversations. This dynamic nature allows the agent topology to adapt based on the actual conversation flow under varying input problem scenarios. Conversely, static conversations adhere to a predefined topology. Dynamic conversations are particularly beneficial in complex settings where interaction patterns cannot be predetermined.  1. Registered auto-reply  With the pluggable auto-reply function, one can choose to invoke conversations with other agents depending on the content of the current message and context. For example: - Hierarchical chat like in [OptiGuide](https://github.com/microsoft/optiguide). - [Dynamic Group Chat](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat.ipynb) which is a special form of hierarchical chat. In the system, we register a reply function in the group chat manager, which broadcasts messages and decides who the next speaker will be in a group chat setting. - [Finite State Machine graphs to set speaker transition constraints](https://microsoft.github.io/autogen/docs/notebooks/agentchat_groupchat_finite_state_machine) which is a special form of dynamic group chat. In this approach, a directed transition matrix is fed into group chat. Users can specify legal transitions or specify disallowed transitions. - Nested chat like in [conversational chess](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_nested_chats_chess.ipynb).  2. LLM-Based Function Call  Another approach involves LLM-based function calls, where LLM decides if a specific function should be invoked based on the conversation's status during each inference. This approach enables dynamic multi-agent conversations, as seen in scenarios like [multi-user math problem solving scenario](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_two_users.ipynb), where a student assistant automatically seeks expertise via function calls.  ### Diverse Applications Implemented with AutoGen  The figure below shows six examples of applications built using AutoGen. ![Applications](images/app.png)  Find a list of examples in this page: [Automated Agent Chat Examples](../Examples.md#automated-multi-agent-chat) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/Use-Cases/agent_chat.md",
        "label": "autogen",
        "content": "For Further Reading  _Interested in the research that leads to this package? Please check the following papers._  - [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155). Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang and Chi Wang. ArXiv 2023.  - [An Empirical Study on Challenging Math Problem Solving with GPT-4](https://arxiv.org/abs/2306.01337). Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2306.01337 (2023). "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/faqs.md",
        "label": "autogen",
        "content": "# AutoGen Studio FAQs "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/faqs.md",
        "label": "autogen",
        "content": "Q: How do I specify the directory where files(e.g. database) are stored?  A: You can specify the directory where files are stored by setting the `--appdir` argument when running the application. For example, `autogenstudio ui --appdir /path/to/folder`. This will store the database (default) and other files in the specified directory e.g. `/path/to/folder/database.sqlite`. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/faqs.md",
        "label": "autogen",
        "content": "Q: Where can I adjust the default skills, agent and workflow configurations?  A: You can modify agent configurations directly from the UI or by editing the `init_db_samples` function in the `autogenstudio/database/utils.py` file which is used to initialize the database. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/faqs.md",
        "label": "autogen",
        "content": "Q: If I want to reset the entire conversation with an agent, how do I go about it?  A: To reset your conversation history, you can delete the `database.sqlite` file in the `--appdir` directory. This will reset the entire conversation history. To delete user files, you can delete the `files` directory in the `--appdir` directory. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/faqs.md",
        "label": "autogen",
        "content": "Q: Is it possible to view the output and messages generated by the agents during interactions?  A: Yes, you can view the generated messages in the debug console of the web UI, providing insights into the agent interactions. Alternatively, you can inspect the `database.sqlite` file for a comprehensive record of messages. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/faqs.md",
        "label": "autogen",
        "content": "Q: Can I use other models with AutoGen Studio?  Yes. AutoGen standardizes on the openai model api format, and you can use any api server that offers an openai compliant endpoint. In the AutoGen Studio UI, each agent has an `llm_config` field where you can input your model endpoint details including `model`, `api key`, `base url`, `model type` and `api version`. For Azure OpenAI models, you can find these details in the Azure portal. Note that for Azure OpenAI, the `model name` is the deployment id or engine, and the `model type` is \"azure\". For other OSS models, we recommend using a server such as vllm, LMStudio, Ollama, to instantiate an openai compliant endpoint. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/faqs.md",
        "label": "autogen",
        "content": "Q: The server starts but I can't access the UI  A: If you are running the server on a remote machine (or a local machine that fails to resolve localhost correctly), you may need to specify the host address. By default, the host address is set to `localhost`. You can specify the host address using the `--host <host>` argument. For example, to start the server on port 8081 and local address such that it is accessible from other machines on the network, you can run the following command:  ```bash autogenstudio ui --port 8081 --host 0.0.0.0 ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/faqs.md",
        "label": "autogen",
        "content": "Q: Can I export my agent workflows for use in a python app?  Yes. In the Build view, you can click the export button to save your agent workflow as a JSON file. This file can be imported in a python application using the `WorkflowManager` class. For example:  ```python  from autogenstudio import WorkflowManager # load workflow from exported json workflow file. workflow_manager = WorkflowManager(workflow=\"path/to/your/workflow_.json\")  # run the workflow on a task task_query = \"What is the height of the Eiffel Tower?. Dont write code, just respond to the question.\" workflow_manager.run(message=task_query)  ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/faqs.md",
        "label": "autogen",
        "content": "Q: Can I deploy my agent workflows as APIs?  Yes. You can launch the workflow as an API endpoint from the command line using the `autogenstudio` commandline tool. For example:  ```bash autogenstudio serve --workflow=workflow.json --port=5000 ```  Similarly, the workflow launch command above can be wrapped into a Dockerfile that can be deployed on cloud services like Azure Container Apps or Azure Web Apps. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/faqs.md",
        "label": "autogen",
        "content": "Q: Can I run AutoGen Studio in a Docker container?  A: Yes, you can run AutoGen Studio in a Docker container. You can build the Docker image using the provided [Dockerfile](https://github.com/microsoft/autogen/blob/autogenstudio/samples/apps/autogen-studio/Dockerfile) and run the container using the following commands:  ```bash FROM python:3.10  WORKDIR /code  RUN pip install -U gunicorn autogenstudio  RUN useradd -m -u 1000 user USER user ENV HOME=/home/user \\     PATH=/home/user/.local/bin:$PATH \\     AUTOGENSTUDIO_APPDIR=/home/user/app  WORKDIR $HOME/app  COPY --chown=user . $HOME/app  CMD gunicorn -w $((2 * $(getconf _NPROCESSORS_ONLN) + 1)) --timeout 12600 -k uvicorn.workers.UvicornWorker autogenstudio.web.app:app --bind \"0.0.0.0:8081\" ```  Using Gunicorn as the application server for improved performance is recommended. To run AutoGen Studio with Gunicorn, you can use the following command:  ```bash gunicorn -w $((2 * $(getconf _NPROCESSORS_ONLN) + 1)) --timeout 12600 -k uvicorn.workers.UvicornWorker autogenstudio.web.app:app --bind ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/getting-started.md",
        "label": "autogen",
        "content": "# AutoGen Studio - Getting Started  [![PyPI version](https://badge.fury.io/py/autogenstudio.svg)](https://badge.fury.io/py/autogenstudio) [![Downloads](https://static.pepy.tech/badge/autogenstudio/week)](https://pepy.tech/project/autogenstudio)  ![ARA](./img/ara_stockprices.png)  AutoGen Studio is an low-code interface built to help you rapidly prototype AI agents, enhance them with skills, compose them into workflows and interact with them to accomplish tasks. It is built on top of the [AutoGen](https://microsoft.github.io/autogen) framework, which is a toolkit for building AI agents.  Code for AutoGen Studio is on GitHub at [microsoft/autogen](https://github.com/microsoft/autogen/tree/main/samples/apps/autogen-studio)  > **Note**: AutoGen Studio is meant to help you rapidly prototype multi-agent workflows and demonstrate an example of end user interfaces built with AutoGen. It is not meant to be a production-ready app. Developers are encouraged to use the AutoGen framework to build their own applications, implementing authentication, security and other features required for deployed applications.  **Updates**  - April 17: AutoGen Studio database layer is now rewritten to use [SQLModel](https://sqlmodel.tiangolo.com/) (Pydantic + SQLAlchemy). This provides entity linking (skills, models, agents and workflows are linked via association tables) and supports multiple [database backend dialects](https://docs.sqlalchemy.org/en/20/dialects/) supported in SQLAlchemy (SQLite, PostgreSQL, MySQL, Oracle, Microsoft SQL Server). The backend database can be specified with a `--database-uri` argument when running the application. For example, `autogenstudio ui --database-uri sqlite:///database.sqlite` for SQLite and `autogenstudio ui --database-uri postgresql+psycopg://user:password@localhost/dbname` for PostgreSQL.  - March 12: Default directory for AutoGen Studio is now /home/<user>/.autogenstudio. You can also specify this directory using the `--appdir` argument when running the application. For example, `autogenstudio ui --appdir /path/to/folder`. This will store the database and other files in the specified directory e.g. `/path/to/folder/database.sqlite`. `.env` files in that directory will be used to set environment variables for the app.  ### Installation  There are two ways to install AutoGen Studio - from PyPi or from source. We **recommend installing from PyPi** unless you plan to modify the source code.  1.  **Install from PyPi**      We recommend using a virtual environment (e.g., conda) to avoid conflicts with existing Python packages. With Python 3.10 or newer active in your virtual environment, use pip to install AutoGen Studio:      ```bash     pip install autogenstudio     ```  2.  **Install from Source**      > Note: This approach requires some familiarity with building interfaces in React.      If you prefer to install from source, ensure you have Python 3.10+ and Node.js (version above 14.15.0) installed. Here's how you get started:      - Clone the AutoGen Studio repository and install its Python dependencies:        ```bash       pip install -e .       ```      - Navigate to the `samples/apps/autogen-studio/frontend` directory, install dependencies, and build the UI:        ```bash       npm install -g gatsby-cli       npm install --global yarn       cd frontend       yarn install       yarn build       ```  For Windows users, to build the frontend, you may need alternative commands to build the frontend.  ```bash    gatsby clean && rmdir /s /q ..\\\\autogenstudio\\\\web\\\\ui 2>nul & (set \\\"PREFIX_PATH_VALUE=\\\" || ver>nul) && gatsby build --prefix-paths && xcopy /E /I /Y public ..\\\\autogenstudio\\\\web\\\\ui  ```  ### Running the Application  Once installed, run the web UI by entering the following in your terminal:  ```bash autogenstudio ui --port 8081 ```  This will start the application on the specified port. Open your web browser and go to `http://localhost:8081/` to begin using AutoGen Studio.  AutoGen Studio also takes several parameters to customize the application:  - `--host <host>` argument to specify the host address. By default, it is set to `localhost`. Y - `--appdir <appdir>` argument to specify the directory where the app files (e.g., database and generated user files) are stored. By default, it is set to the a `.autogenstudio` directory in the user's home directory. - `--port <port>` argument to specify the port number. By default, it is set to `8080`. - `--reload` argument to enable auto-reloading of the server when changes are made to the code. By default, it is set to `False`. - `--database-uri` argument to specify the database URI. Example values include `sqlite:///database.sqlite` for SQLite and `postgresql+psycopg://user:password@localhost/dbname` for PostgreSQL. If this is not specified, the database URI defaults to a `database.sqlite` file in the `--appdir` directory.  Now that you have AutoGen Studio installed and running, you are ready to explore its capabilities, including defining and modifying agent workflows, interacting with agents and sessions, and expanding agent skills.  ### Capabilities / Roadmap  Some of the capabilities supported by the app frontend include the following:  - [x] Build / Configure agents (currently supports two agent workflows based on `UserProxyAgent` and `AssistantAgent`), modify their configuration (e.g. skills, temperature, model, agent system message, model etc) and compose them into workflows. - [x] Chat with agent workflows and specify tasks. - [x] View agent messages and output files in the UI from agent runs. - [x] Support for more complex agent workflows (e.g. `GroupChat` and `Sequential` workflows). - [x] Improved user experience (e.g., streaming intermediate model output, better summarization of agent responses, etc).  Review project roadmap and issues [here](https://github.com/microsoft/autogen/issues/737) .  Project Structure:  - _autogenstudio/_ code for the backend classes and web api (FastAPI) - _frontend/_ code for the webui, built with Gatsby and TailwindCSS "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/getting-started.md",
        "label": "autogen",
        "content": "Contribution Guide  We welcome contributions to AutoGen Studio. We recommend the following general steps to contribute to the project:  - Review the overall AutoGen project [contribution guide](https://github.com/microsoft/autogen?tab=readme-ov-file#contributing) - Please review the AutoGen Studio [roadmap](https://github.com/microsoft/autogen/issues/737) to get a sense of the current priorities for the project. Help is appreciated especially with Studio issues tagged with `help-wanted` - Please initiate a discussion on the roadmap issue or a new issue to discuss your proposed contribution. - Please review the autogenstudio dev branch here [dev branch](https://github.com/microsoft/autogen/tree/autogenstudio) and use as a base for your contribution. This way, your contribution will be aligned with the latest changes in the AutoGen Studio project. - Submit a pull request with your contribution! - If you are modifying AutoGen Studio, it has its own devcontainer. See instructions in `.devcontainer/README.md` to use it - Please use the tag `studio` for any issues, questions, and PRs related to Studio "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/getting-started.md",
        "label": "autogen",
        "content": "A Note on Security  AutoGen Studio is a research prototype and is not meant to be used in a production environment. Some baseline practices are encouraged e.g., using Docker code execution environment for your agents.  However, other considerations such as rigorous tests related to jailbreaking, ensuring LLMs only have access to the right keys of data given the end user's permissions, and other security features are not implemented in AutoGen Studio.  If you are building a production application, please use the AutoGen framework and implement the necessary security features. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/getting-started.md",
        "label": "autogen",
        "content": "Acknowledgements  AutoGen Studio is Based on the [AutoGen](https://microsoft.github.io/autogen) project. It was adapted from a research prototype built in October 2023 (original credits: Gagan Bansal, Adam Fourney, Victor Dibia, Piali Choudhury, Saleema Amershi, Ahmed Awadallah, Chi Wang). "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/usage.md",
        "label": "autogen",
        "content": "# Using AutoGen Studio  AutoGen Studio supports the declarative creation of an agent workflow and tasks can be specified and run in a chat interface for the agents to complete. The expected usage behavior is that developers can create skills and models, _attach_ them to agents, and compose agents into workflows that can be tested interactively in the chat interface. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/usage.md",
        "label": "autogen",
        "content": "Building an Agent Workflow  AutoGen Studio implements several entities that are ultimately composed into a workflow.  ### Skills  A skill is a python function that implements the solution to a task. In general, a good skill has a descriptive name (e.g. generate*images), extensive docstrings and good defaults (e.g., writing out files to disk for persistence and reuse). Skills can be \\_associated with* or _attached to_ agent specifications.  ![AutoGen Studio Skill Interface](./img/skill.png)  ### Models  A model refers to the configuration of an LLM. Similar to skills, a model can be attached to an agent specification. The AutoGen Studio interface supports multiple model types including OpenAI models (and any other model endpoint provider that supports the OpenAI endpoint specification), Azure OpenAI models and Gemini Models.  ![AutoGen Studio Create new model](./img/model_new.png) ![AutoGen Studio Create new model](./img/model_openai.png)  ### Agents  An agent entity declaratively specifies properties for an AutoGen agent (mirrors most but not all of the members of a base AutoGen Conversable agent class). Currently `UserProxyAgent` and `AssistantAgent` and `GroupChat` agent abstractions are supported.  ![AutoGen Studio Create new agent](./img/agent_new.png) ![AutoGen Studio Createan assistant agent](./img/agent_groupchat.png)  Once agents have been created, existing models or skills can be _added_ to the agent.  ![AutoGen Studio Add skills and models to agent](./img/agent_skillsmodel.png)  ### Workflows  An agent workflow is a specification of a set of agents (team of agents) that can work together to accomplish a task. AutoGen Studio supports two types of high level workflow patterns:  #### Autonomous Chat :  This workflow implements a paradigm where agents are defined and a chat is initiated between the agents to accomplish a task. AutoGen simplifies this into defining an `initiator` agent and a `receiver` agent where the receiver agent is selected from a list of previously created agents. Note that when the receiver is a `GroupChat` agent (i.e., contains multiple agents), the communication pattern between those agents is determined by the `speaker_selection_method` parameter in the `GroupChat` agent configuration.  ![AutoGen Studio Autonomous Chat Workflow](./img/workflow_chat.png)  #### Sequential Chat  This workflow allows users to specify a list of `AssistantAgent` agents that are executed in sequence to accomplish a task. The runtime behavior here follows the following pattern: at each step, each `AssistantAgent` is _paired_ with a `UserProxyAgent` and chat initiated between this pair to process the input task. The result of this exchange is summarized and provided to the next `AssistantAgent` which is also paired with a `UserProxyAgent` and their summarized result is passed to the next `AssistantAgent` in the sequence. This continues until the last `AssistantAgent` in the sequence is reached.  ![AutoGen Studio Sequential Workflow](./img/workflow_sequential.png)  <!-- ``` Plot a chart of NVDA and TESLA stock price YTD. Save the result to a file named nvda_tesla.png ```  The agent workflow responds by _writing and executing code_ to create a python program to generate the chart with the stock prices.  > Note than there could be multiple turns between the `AssistantAgent` and the `UserProxyAgent` to produce and execute the code in order to complete the task.  ![ARA](./img/ara_stockprices.png)  > Note: You can also view the debug console that generates useful information to see how the agents are interacting in the background. -->  <!-- - Build: Users begin by constructing their workflows. They may incorporate previously developed skills/models into agents within the workflow. User's can immediately test their workflows in the the same view or in a saved session in the playground.  - Playground: Users can start a new session, select an agent workflow, and engage in a \"chat\" with this agent workflow. It is important to note the significant differences between a traditional chat with a Large Language Model (LLM) and a chat with a group of agents. In the former, the response is typically a single formatted reply, while in the latter, it consists of a history of conversations among the agents. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/usage.md",
        "label": "autogen",
        "content": "Entities and Concepts --> "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/usage.md",
        "label": "autogen",
        "content": "Testing an Agent Workflow  AutoGen Studio allows users to interactively test workflows on tasks and review resulting artifacts (such as images, code, and documents).  ![AutoGen Studio Test Workflow](./img/workflow_test.png)  Users can also review the \u201cinner monologue\u201d of agent workflows as they address tasks, and view profiling information such as costs associated with the run (such as number of turns, number of tokens etc.), and agent actions (such as whether tools were called and the outcomes of code execution).  ![AutoGen Studio Profile Workflow Results](./img/workflow_profile.png) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/autogen-studio/usage.md",
        "label": "autogen",
        "content": "Exporting Agent Workflows  Users can download the skills, agents, and workflow configurations they create as well as share and reuse these artifacts. AutoGen Studio also offers a seamless process to export workflows and deploy them as application programming interfaces (APIs) that can be consumed in other applications deploying workflows as APIs.  ### Export Workflow  AutoGen Studio allows you to export a selected workflow as a JSON configuration file.  Build -> Workflows -> (On workflow card) -> Export  ![AutoGen Studio Export Workflow](./img/workflow_export.png)  ### Using AutoGen Studio Workflows in a Python Application  An exported workflow can be easily integrated into any Python application using the `WorkflowManager` class with just two lines of code. Underneath, the WorkflowManager rehydrates the workflow specification into AutoGen agents that are subsequently used to address tasks.  ```python  from autogenstudio import WorkflowManager # load workflow from exported json workflow file. workflow_manager = WorkflowManager(workflow=\"path/to/your/workflow_.json\")  # run the workflow on a task task_query = \"What is the height of the Eiffel Tower?. Dont write code, just respond to the question.\" workflow_manager.run(message=task_query)  ```  ### Deploying AutoGen Studio Workflows as APIs  The workflow can be launched as an API endpoint from the command line using the autogenstudio commandline tool.  ```bash autogenstudio serve --workflow=workflow.json --port=5000 ```  Similarly, the workflow launch command above can be wrapped into a Dockerfile that can be deployed on cloud services like Azure Container Apps or Azure Web Apps. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/installation/Docker.md",
        "label": "autogen",
        "content": "# Docker  Docker, an indispensable tool in modern software development, offers a compelling solution for AutoGen's setup. Docker allows you to create consistent environments that are portable and isolated from the host OS. With Docker, everything AutoGen needs to run, from the operating system to specific libraries, is encapsulated in a container, ensuring uniform functionality across different systems. The Dockerfiles necessary for AutoGen are conveniently located in the project's GitHub repository at [https://github.com/microsoft/autogen/tree/main/.devcontainer](https://github.com/microsoft/autogen/tree/main/.devcontainer).  **Pre-configured DockerFiles**: The AutoGen Project offers pre-configured Dockerfiles for your use. These Dockerfiles will run as is, however they can be modified to suit your development needs. Please see the README.md file in autogen/.devcontainer  - **autogen_base_img**: For a basic setup, you can use the `autogen_base_img` to run simple scripts or applications. This is ideal for general users or those new to AutoGen. - **autogen_full_img**: Advanced users or those requiring more features can use `autogen_full_img`. Be aware that this version loads ALL THE THINGS and thus is very large. Take this into consideration if you build your application off of it. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/installation/Docker.md",
        "label": "autogen",
        "content": "Step 1: Install Docker  - **General Installation**: Follow the [official Docker installation instructions](https://docs.docker.com/get-docker/). This is your first step towards a containerized environment, ensuring a consistent and isolated workspace for AutoGen.  - **For Mac Users**: If you encounter issues with the Docker daemon, consider using [colima](https://smallsharpsoftwaretools.com/tutorials/use-colima-to-run-docker-containers-on-macos/). Colima offers a lightweight alternative to manage Docker containers efficiently on macOS. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/installation/Docker.md",
        "label": "autogen",
        "content": "Step 2: Build a Docker Image  AutoGen now provides updated Dockerfiles tailored for different needs. Building a Docker image is akin to setting the foundation for your project's environment:  - **Autogen Basic**: Ideal for general use, this setup includes common Python libraries and essential dependencies. Perfect for those just starting with AutoGen.    ```bash   docker build -f .devcontainer/Dockerfile -t autogen_base_img https://github.com/microsoft/autogen.git#main   ```  - **Autogen Advanced**: Advanced users or those requiring all the things that AutoGen has to offer `autogen_full_img`    ```bash   docker build -f .devcontainer/full/Dockerfile -t autogen_full_img https://github.com/microsoft/autogen.git#main   ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/installation/Docker.md",
        "label": "autogen",
        "content": "Step 3: Run AutoGen Applications from Docker Image  Here's how you can run an application built with AutoGen, using the Docker image:  1. **Mount Your Directory**: Use the Docker `-v` flag to mount your local application directory to the Docker container. This allows you to develop on your local machine while running the code in a consistent Docker environment. For example:     ```bash    docker run -it -v $(pwd)/myapp:/home/autogen/autogen/myapp autogen_base_img:latest python /home/autogen/autogen/myapp/main.py    ```     Here, `$(pwd)/myapp` is your local directory, and `/home/autogen/autogen/myapp` is the path in the Docker container where your code will be located.  2. **Mount your code:** Now suppose you have your application built with AutoGen in a main script named `twoagent.py` ([example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py)) in a folder named `myapp`. With the command line below, you can mount your folder and run the application in Docker.     ```python    # Mount the local folder `myapp` into docker image and run the script named \"twoagent.py\" in the docker.    docker run -it -v `pwd`/myapp:/myapp autogen_img:latest python /myapp/main_twoagent.py    ```  3. **Port Mapping**: If your application requires a specific port, use the `-p` flag to map the container's port to your host. For instance, if your app runs on port 3000 inside Docker and you want it accessible on port 8080 on your host machine:     ```bash    docker run -it -p 8080:3000 -v $(pwd)/myapp:/myapp autogen_base_img:latest python /myapp    ```     In this command, `-p 8080:3000` maps port 3000 from the container to port 8080 on your local machine.  4. **Examples of Running Different Applications**: Here is the basic format of the docker run command.  ```bash docker run -it -p {WorkstationPortNum}:{DockerPortNum} -v {WorkStation_Dir}:{Docker_DIR} {name_of_the_image} {bash/python} {Docker_path_to_script_to_execute} ```  - _Simple Script_: Run a Python script located in your local `myapp` directory.    ```bash   docker run -it -v `pwd`/myapp:/myapp autogen_base_img:latest python /myapp/my_script.py   ```  - _Web Application_: If your application includes a web server running on port 5000.    ```bash   docker run -it -p 8080:5000 -v $(pwd)/myapp:/myapp autogen_base_img:latest   ```  - _Data Processing_: For tasks that involve processing data stored in a local directory.    ```bash   docker run -it -v $(pwd)/data:/data autogen_base_img:latest python /myapp/process_data.py   ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/installation/Docker.md",
        "label": "autogen",
        "content": "Additional Resources  - Details on all the Dockerfile options can be found in the [Dockerfile](https://github.com/microsoft/autogen/.devcontainer/README.md) README. - For more information on Docker usage and best practices, refer to the [official Docker documentation](https://docs.docker.com). - Details on how to use the Dockerfile dev version can be found on the [Contributor Guide](/docs/contributor-guide/docker). "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/installation/Optional-Dependencies.md",
        "label": "autogen",
        "content": "# Optional Dependencies "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/installation/Optional-Dependencies.md",
        "label": "autogen",
        "content": "LLM Caching  To use LLM caching with Redis, you need to install the Python package with the option `redis`:  ```bash pip install \"pyautogen[redis]\" ```  See [LLM Caching](Use-Cases/agent_chat.md#llm-caching) for details. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/installation/Optional-Dependencies.md",
        "label": "autogen",
        "content": "IPython Code Executor  To use the IPython code executor, you need to install the `jupyter-client` and `ipykernel` packages:  ```bash pip install \"pyautogen[ipython]\" ```  To use the IPython code executor:  ```python from autogen import UserProxyAgent  proxy = UserProxyAgent(name=\"proxy\", code_execution_config={\"executor\": \"ipython-embedded\"}) ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/installation/Optional-Dependencies.md",
        "label": "autogen",
        "content": "blendsearch  `pyautogen<0.2` offers a cost-effective hyperparameter optimization technique [EcoOptiGen](https://arxiv.org/abs/2303.04673) for tuning Large Language Models. Please install with the [blendsearch] option to use it.  ```bash pip install \"pyautogen[blendsearch]<0.2\" ```  Example notebooks:  [Optimize for Code Generation](https://github.com/microsoft/autogen/blob/main/notebook/oai_completion.ipynb)  [Optimize for Math](https://github.com/microsoft/autogen/blob/main/notebook/oai_chatgpt_gpt4.ipynb) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/installation/Optional-Dependencies.md",
        "label": "autogen",
        "content": "retrievechat  `pyautogen` supports retrieval-augmented generation tasks such as question answering and code generation with RAG agents. Please install with the [retrievechat] option to use it with ChromaDB.  ```bash pip install \"pyautogen[retrievechat]\" ```  Alternatively `pyautogen` also supports PGVector and Qdrant which can be installed in place of ChromaDB, or alongside it.  ```bash pip install \"pyautogen[retrievechat-pgvector]\" ```  ```bash pip install \"pyautogen[retrievechat-qdrant]\" ```  RetrieveChat can handle various types of documents. By default, it can process plain text and PDF files, including formats such as 'txt', 'json', 'csv', 'tsv', 'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml' and 'pdf'. If you install [unstructured](https://unstructured-io.github.io/unstructured/installation/full_installation.html) (`pip install \"unstructured[all-docs]\"`), additional document types such as 'docx', 'doc', 'odt', 'pptx', 'ppt', 'xlsx', 'eml', 'msg', 'epub' will also be supported.  You can find a list of all supported document types by using `autogen.retrieve_utils.TEXT_FORMATS`.  Example notebooks:  [Automated Code Generation and Question Answering with Retrieval Augmented Agents](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat.ipynb)  [Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent)](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat_RAG.ipynb)  [Automated Code Generation and Question Answering with Qdrant based Retrieval Augmented Agents](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat_qdrant.ipynb) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/installation/Optional-Dependencies.md",
        "label": "autogen",
        "content": "Teachability  To use Teachability, please install AutoGen with the [teachable] option.  ```bash pip install \"pyautogen[teachable]\" ```  Example notebook: [Chatting with a teachable agent](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_teachability.ipynb) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/installation/Optional-Dependencies.md",
        "label": "autogen",
        "content": "Large Multimodal Model (LMM) Agents  We offered Multimodal Conversable Agent and LLaVA Agent. Please install with the [lmm] option to use it.  ```bash pip install \"pyautogen[lmm]\" ```  Example notebooks:  [LLaVA Agent](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/installation/Optional-Dependencies.md",
        "label": "autogen",
        "content": "mathchat  `pyautogen<0.2` offers an experimental agent for math problem solving. Please install with the [mathchat] option to use it.  ```bash pip install \"pyautogen[mathchat]<0.2\" ```  Example notebooks:  [Using MathChat to Solve Math Problems](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_MathChat.ipynb) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/installation/Optional-Dependencies.md",
        "label": "autogen",
        "content": "Graph  To use a graph in `GroupChat`, particularly for graph visualization, please install AutoGen with the [graph] option.  ```bash pip install \"pyautogen[graph]\" ```  Example notebook: [Finite State Machine graphs to set speaker transition constraints](https://microsoft.github.io/autogen/docs/notebooks/agentchat_groupchat_finite_state_machine) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/installation/Optional-Dependencies.md",
        "label": "autogen",
        "content": "Long Context Handling  AutoGen includes support for handling long textual contexts by leveraging the LLMLingua library for text compression. To enable this functionality, please install AutoGen with the `[long-context]` option:  ```bash pip install \"pyautogen[long-context]\" ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/tests.md",
        "label": "autogen",
        "content": "# Tests  Tests are automatically run via GitHub actions. There are two workflows:  1. [build.yml](https://github.com/microsoft/autogen/blob/main/.github/workflows/build.yml) 1. [openai.yml](https://github.com/microsoft/autogen/blob/main/.github/workflows/openai.yml)  The first workflow is required to pass for all PRs (and it doesn't do any OpenAI calls). The second workflow is required for changes that affect the OpenAI tests (and does actually call LLM). The second workflow requires approval to run. When writing tests that require OpenAI calls, please use [`pytest.mark.skipif`](https://github.com/microsoft/autogen/blob/b1adac515931bf236ac59224269eeec683a162ba/test/oai/test_client.py#L19) to make them run in only when `openai` package is installed. If additional dependency for this test is required, install the dependency in the corresponding python version in [openai.yml](https://github.com/microsoft/autogen/blob/main/.github/workflows/openai.yml).  Make sure all tests pass, this is required for [build.yml](https://github.com/microsoft/autogen/blob/main/.github/workflows/build.yml) checks to pass "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/tests.md",
        "label": "autogen",
        "content": "Running tests locally  To run tests, install the [test] option:  ```bash pip install -e.\"[test]\" ```  Then you can run the tests from the `test` folder using the following command:  ```bash pytest test ```  Tests for the `autogen.agentchat.contrib` module may be skipped automatically if the required dependencies are not installed. Please consult the documentation for each contrib module to see what dependencies are required.  See [here](https://github.com/microsoft/autogen/blob/main/notebook/contributing.md#testing) for how to run notebook tests. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/tests.md",
        "label": "autogen",
        "content": "Skip flags for tests  - `--skip-openai` for skipping tests that require access to OpenAI services. - `--skip-docker` for skipping tests that explicitly use docker - `--skip-redis` for skipping tests that require a Redis server  For example, the following command will skip tests that require access to OpenAI and docker services:  ```bash pytest test --skip-openai --skip-docker ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/tests.md",
        "label": "autogen",
        "content": "Coverage  Any code you commit should not decrease coverage. To ensure your code maintains or increases coverage, use the following commands after installing the required test dependencies:  ```bash pip install -e .\"[test]\"  pytest test --cov-report=html ```  Pytest generated a code coverage report and created a htmlcov directory containing an index.html file and other related files. Open index.html in any web browser to visualize and navigate through the coverage data interactively. This interactive visualization allows you to identify uncovered lines and review coverage statistics for individual files. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/pre-commit.md",
        "label": "autogen",
        "content": "# Pre-commit  Run `pre-commit install` to install pre-commit into your git hooks. Before you commit, run `pre-commit run` to check if you meet the pre-commit requirements. If you use Windows (without WSL) and can't commit after installing pre-commit, you can run `pre-commit uninstall` to uninstall the hook. In WSL or Linux this is supposed to work. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/maintainer.md",
        "label": "autogen",
        "content": "# Guidance for Maintainers "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/maintainer.md",
        "label": "autogen",
        "content": "General  - Be a member of the community and treat everyone as a member. Be inclusive. - Help each other and encourage mutual help. - Actively post and respond. - Keep open communication. - Identify good maintainer candidates from active contributors. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/maintainer.md",
        "label": "autogen",
        "content": "Pull Requests  - For new PR, decide whether to close without review. If not, find the right reviewers. One source to refer to is the roles on Discord. Another consideration is to ask users who can benefit from the PR to review it.  - For old PR, check the blocker: reviewer or PR creator. Try to unblock. Get additional help when needed. - When requesting changes, make sure you can check back in time because it blocks merging. - Make sure all the checks are passed. - For changes that require running OpenAI tests, make sure the OpenAI tests pass too. Running these tests requires approval. - In general, suggest small PRs instead of a giant PR. - For documentation change, request snapshot of the compiled website, or compile by yourself to verify the format. - For new contributors who have not signed the contributing agreement, remind them to sign before reviewing. - For multiple PRs which may have conflict, coordinate them to figure out the right order. - Pay special attention to:   - Breaking changes. Don\u2019t make breaking changes unless necessary. Don\u2019t merge to main until enough headsup is provided and a new release is ready.   - Test coverage decrease.   - Changes that may cause performance degradation. Do regression test when test suites are available.   - Discourage **change to the core library** when there is an alternative. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/maintainer.md",
        "label": "autogen",
        "content": "Issues and Discussions  - For new issues, write a reply, apply a label if relevant. Ask on discord when necessary. For roadmap issues, apply the roadmap label and encourage community discussion. Mention relevant experts when necessary.  - For old issues, provide an update or close. Ask on discord when necessary. Encourage PR creation when relevant. - Use \u201cgood first issue\u201d for easy fix suitable for first-time contributors. - Use \u201ctask list\u201d for issues that require multiple PRs. - For discussions, create an issue when relevant. Discuss on discord when appropriate. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/docker.md",
        "label": "autogen",
        "content": "# Docker for Development  For developers contributing to the AutoGen project, we offer a specialized Docker environment. This setup is designed to streamline the development process, ensuring that all contributors work within a consistent and well-equipped environment. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/docker.md",
        "label": "autogen",
        "content": "Autogen Developer Image (autogen_dev_img)  - **Purpose**: The `autogen_dev_img` is tailored for contributors to the AutoGen project. It includes a suite of tools and configurations that aid in the development and testing of new features or fixes. - **Usage**: This image is recommended for developers who intend to contribute code or documentation to AutoGen. - **Forking the Project**: It's advisable to fork the AutoGen GitHub project to your own repository. This allows you to make changes in a separate environment without affecting the main project. - **Updating Dockerfile**: Modify your copy of `Dockerfile` in the `dev` folder as needed for your development work. - **Submitting Pull Requests**: Once your changes are ready, submit a pull request from your branch to the upstream AutoGen GitHub project for review and integration. For more details on contributing, see the [AutoGen Contributing](https://microsoft.github.io/autogen/docs/Contribute) page. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/docker.md",
        "label": "autogen",
        "content": "Building the Developer Docker Image  - To build the developer Docker image (`autogen_dev_img`), use the following commands:    ```bash   docker build -f .devcontainer/dev/Dockerfile -t autogen_dev_img https://github.com/microsoft/autogen.git#main   ```  - For building the developer image built from a specific Dockerfile in a branch other than main/master    ```bash   # clone the branch you want to work out of   git clone --branch {branch-name} https://github.com/microsoft/autogen.git    # cd to your new directory   cd autogen    # build your Docker image   docker build -f .devcontainer/dev/Dockerfile -t autogen_dev-srv_img .   ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/docker.md",
        "label": "autogen",
        "content": "Using the Developer Docker Image  Once you have built the `autogen_dev_img`, you can run it using the standard Docker commands. This will place you inside the containerized development environment where you can run tests, develop code, and ensure everything is functioning as expected before submitting your contributions.  ```bash docker run -it -p 8081:3000 -v `pwd`/autogen-newcode:newstuff/ autogen_dev_img bash ```  - Note that the `pwd` is shorthand for present working directory. Thus, any path after the pwd is relative to that. If you want a more verbose method you could remove the \"`pwd`/autogen-newcode\" and replace it with the full path to your directory  ```bash docker run -it -p 8081:3000 -v /home/AutoGenDeveloper/autogen-newcode:newstuff/ autogen_dev_img bash ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/docker.md",
        "label": "autogen",
        "content": "Develop in Remote Container  If you use vscode, you can open the autogen folder in a [Container](https://code.visualstudio.com/docs/remote/containers). We have provided the configuration in [devcontainer](https://github.com/microsoft/autogen/blob/main/.devcontainer). They can be used in GitHub codespace too. Developing AutoGen in dev containers is recommended. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/documentation.md",
        "label": "autogen",
        "content": "# Documentation "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/documentation.md",
        "label": "autogen",
        "content": "How to get a notebook rendered on the website  See [here](https://github.com/microsoft/autogen/blob/main/notebook/contributing.md#how-to-get-a-notebook-displayed-on-the-website) for instructions on how to get a notebook in the `notebook` directory rendered on the website. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/documentation.md",
        "label": "autogen",
        "content": "Build documentation locally  1\\. To build and test documentation locally, first install [Node.js](https://nodejs.org/en/download/). For example,  ```bash nvm install --lts ```  Then, install `yarn` and other required packages:  ```bash npm install --global yarn pip install pydoc-markdown pyyaml termcolor ```  2\\. You also need to install quarto. Please click on the `Pre-release` tab from [this website](https://quarto.org/docs/download/) to download the latest version of `quarto` and install it. Ensure that the `quarto` version is `1.5.23` or higher.  3\\. Finally, run the following commands to build:  ```console cd website yarn install --frozen-lockfile --ignore-engines pydoc-markdown python process_notebooks.py render yarn start ```  The last command starts a local development server and opens up a browser window. Most changes are reflected live without having to restart the server. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/documentation.md",
        "label": "autogen",
        "content": "Build with Docker  To build and test documentation within a docker container. Use the Dockerfile in the `dev` folder as described above to build your image:  ```bash docker build -f .devcontainer/dev/Dockerfile -t autogen_dev_img https://github.com/microsoft/autogen.git#main ```  Then start the container like so, this will log you in and ensure that Docker port 3000 is mapped to port 8081 on your local machine  ```bash docker run -it -p 8081:3000 -v `pwd`/autogen-newcode:newstuff/ autogen_dev_img bash ```  Once at the CLI in Docker run the following commands:  ```bash cd website yarn install --frozen-lockfile --ignore-engines pydoc-markdown python process_notebooks.py render yarn start --host 0.0.0.0 --port 3000 ```  Once done you should be able to access the documentation at `http://127.0.0.1:8081/autogen` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/contributing.md",
        "label": "autogen",
        "content": "# Contributing to AutoGen  The project welcomes contributions from developers and organizations worldwide. Our goal is to foster a collaborative and inclusive community where diverse perspectives and expertise can drive innovation and enhance the project's capabilities. Whether you are an individual contributor or represent an organization, we invite you to join us in shaping the future of this project. Together, we can build something truly remarkable. Possible contributions include but not limited to:  - Pushing patches. - Code review of pull requests. - Documentation, examples and test cases. - Readability improvement, e.g., improvement on docstr and comments. - Community participation in [issues](https://github.com/microsoft/autogen/issues), [discussions](https://github.com/microsoft/autogen/discussions), [discord](https://aka.ms/autogen-dc), and [twitter](https://twitter.com/pyautogen). - Tutorials, blog posts, talks that promote the project. - Sharing application scenarios and/or related research.  Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.  If you are new to GitHub [here](https://help.github.com/categories/collaborating-with-issues-and-pull-requests/) is a detailed help source on getting involved with development on GitHub.  When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.  This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/contributing.md",
        "label": "autogen",
        "content": "Roadmaps  To see what we are working on and what we plan to work on, please check our [Roadmap Issues](https://aka.ms/autogen-roadmap). "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/contributing.md",
        "label": "autogen",
        "content": "Becoming a Reviewer  There is currently no formal reviewer solicitation process. Current reviewers identify reviewers from active contributors. If you are willing to become a reviewer, you are welcome to let us know on discord. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/contributing.md",
        "label": "autogen",
        "content": "Contact Maintainers  The project is currently maintained by a [dynamic group of volunteers](https://butternut-swordtail-8a5.notion.site/410675be605442d3ada9a42eb4dfef30?v=fa5d0a79fd3d4c0f9c112951b2831cbb&pvs=4) from several different organizations. Contact project administrators Chi Wang and Qingyun Wu via auto-gen@outlook.com if you are interested in becoming a maintainer. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/contributor-guide/file-bug-report.md",
        "label": "autogen",
        "content": "# File A Bug Report  When you submit an issue to [GitHub](https://github.com/microsoft/autogen/issues), please do your best to follow these guidelines! This will make it a lot easier to provide you with good feedback:  - The ideal bug report contains a short reproducible code snippet. This way   anyone can try to reproduce the bug easily (see [this](https://stackoverflow.com/help/mcve) for more details). If your snippet is   longer than around 50 lines, please link to a [gist](https://gist.github.com) or a GitHub repo.  - If an exception is raised, please **provide the full traceback**.  - Please include your **operating system type and version number**, as well as   your **Python, autogen, scikit-learn versions**. The version of autogen   can be found by running the following code snippet:  ```python import autogen print(autogen.__version__) ```  - Please ensure all **code snippets and error messages are formatted in   appropriate code blocks**.  See [Creating and highlighting code blocks](https://help.github.com/articles/creating-and-highlighting-code-blocks)   for more details. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/tutorial/what-next.md",
        "label": "autogen",
        "content": "# What Next?  Now that you have learned the basics of AutoGen, you can start to build your own agents. Here are some ideas to get you started without going to the advanced topics:  1.  **Chat with LLMs**: In [Human in the Loop](./human-in-the-loop) we covered     the basic human-in-the-loop usage. You can try to hook up different LLMs     using local model servers like     [Ollama](https://github.com/ollama/ollama)     and [LM Studio](https://lmstudio.ai/), and     chat with them using the human-in-the-loop component of your human proxy     agent. 2.  **Prompt Engineering**: In [Code Executors](./code-executors) we     covered the simple two agent scenario using GPT-4 and Python code executor.     To make this scenario work for different LLMs and programming languages, you     probably need to tune the system message of the code writer agent. Same with     other scenarios that we have covered in this tutorial, you can also try to     tune system messages for different LLMs. 3.  **Complex Tasks**: In [ConversationPatterns](./conversation-patterns)     we covered the basic conversation patterns. You can try to find other tasks     that can be decomposed into these patterns, and leverage the code executors     and tools     to make the agents more powerful. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/tutorial/what-next.md",
        "label": "autogen",
        "content": "Dig Deeper  - Read the [user guide](/docs/topics) to learn more - Read the examples and guides in the [notebooks section](/docs/notebooks) - Check [research](/docs/Research) and [blog](/blog) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/tutorial/what-next.md",
        "label": "autogen",
        "content": "Get Help  If you have any questions, you can ask in our [GitHub Discussions](https://github.com/microsoft/autogen/discussions), or join our [Discord Server](https://aka.ms/autogen-dc).  [![](https://img.shields.io/discord/1153072414184452236?logo=discord&style=flat.png)](https://aka.ms/autogen-dc) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/docs/tutorial/what-next.md",
        "label": "autogen",
        "content": "Get Involved  - Check out [Roadmap Issues](https://aka.ms/autogen-roadmap) to see what we are working on. - Contribute your work to our [gallery](/docs/Gallery) - Follow our [contribution guide](/docs/contributor-guide/contributing) to make a pull request to AutoGen - You can also share your work with the community on the Discord server. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/blog/2023-04-21-LLM-tuning-math/index.md",
        "label": "autogen",
        "content": "--- title: Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH authors: sonichi tags: [LLM, GPT, research] ---  ![level 2 algebra](img/level2algebra.png)  **TL;DR:** * **Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.** * **For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.** * **AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.**   Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?  In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for [MATH](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html), a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.  We will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search & pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.  We will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/blog/2023-04-21-LLM-tuning-math/index.md",
        "label": "autogen",
        "content": "Experiment Setup  We use AutoGen to select between the following models with a target inference budget $0.02 per instance: - gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app - gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo  We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:  - temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1]. - top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1]. - max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000]. - n: The number of responses to generate. We search for the optimal n in the range of [1, 100]. - prompt: We use the template: \"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\\\boxed{{}}.\" where {problem} will be replaced by the math problem instance.  In this experiment, when n > 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/blog/2023-04-21-LLM-tuning-math/index.md",
        "label": "autogen",
        "content": "Experiment Results  The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.  Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget. The same observation can be obtained on the level 3 Algebra test set.  ![level 3 algebra](img/level3algebra.png)  However, the selected model changes on level 4 Algebra.  ![level 4 algebra](img/level4algebra.png)  This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4. On level 5 the result is similar.  ![level 5 algebra](img/level5algebra.png)  We can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.  An example notebook to run these experiments can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb. The experiments were run when AutoGen was a subpackage in FLAML. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/blog/2023-04-21-LLM-tuning-math/index.md",
        "label": "autogen",
        "content": "Analysis and Discussion  While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.  There are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via [`flaml.tune`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function).  The need for model selection, parameter tuning and cost saving is not specific to the math problems. The [Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT) project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/blog/2023-04-21-LLM-tuning-math/index.md",
        "label": "autogen",
        "content": "For Further Reading  * [Research paper about the tuning technique](https://arxiv.org/abs/2303.04673) * [Documentation about inference tuning](/docs/Use-Cases/enhanced_inference)  *Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our [Discord](https://aka.ms/autogen-dc) server for discussion.* "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/blog/2023-07-14-Local-LLMs/index.md",
        "label": "autogen",
        "content": "--- title: Use AutoGen for Local LLMs authors: jialeliu tags: [LLM] --- **TL;DR:** We demonstrate how to use autogen for local LLM application. As an example, we will initiate an endpoint using [FastChat](https://github.com/lm-sys/FastChat) and perform inference on [ChatGLMv2-6b](https://github.com/THUDM/ChatGLM2-6B). "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/blog/2023-07-14-Local-LLMs/index.md",
        "label": "autogen",
        "content": "Preparations  ### Clone FastChat  FastChat provides OpenAI-compatible APIs for its supported models, so you can use FastChat as a local drop-in replacement for OpenAI APIs. However, its code needs minor modification in order to function properly.  ```bash git clone https://github.com/lm-sys/FastChat.git cd FastChat ```  ### Download checkpoint  ChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. ChatGLM2-6B is its second-generation version.  Before downloading from HuggingFace Hub, you need to have Git LFS [installed](https://docs.github.com/en/repositories/working-with-files/managing-large-files/installing-git-large-file-storage).  ```bash git clone https://huggingface.co/THUDM/chatglm2-6b ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/blog/2023-07-14-Local-LLMs/index.md",
        "label": "autogen",
        "content": "Initiate server  First, launch the controller  ```bash python -m fastchat.serve.controller ```  Then, launch the model worker(s)  ```bash python -m fastchat.serve.model_worker --model-path chatglm2-6b ```  Finally, launch the RESTful API server  ```bash python -m fastchat.serve.openai_api_server --host localhost --port 8000 ```  Normally this will work. However, if you encounter error like [this](https://github.com/lm-sys/FastChat/issues/1641), commenting out all the lines containing `finish_reason` in `fastchat/protocol/api_protocol.py` and `fastchat/protocol/openai_api_protocol.py` will fix the problem. The modified code looks like:  ```python class CompletionResponseChoice(BaseModel):     index: int     text: str     logprobs: Optional[int] = None     # finish_reason: Optional[Literal[\"stop\", \"length\"]]  class CompletionResponseStreamChoice(BaseModel):     index: int     text: str     logprobs: Optional[float] = None     # finish_reason: Optional[Literal[\"stop\", \"length\"]] = None ```  "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/blog/2023-07-14-Local-LLMs/index.md",
        "label": "autogen",
        "content": "Interact with model using `oai.Completion` (requires openai<1)  Now the models can be directly accessed through openai-python library as well as `autogen.oai.Completion` and `autogen.oai.ChatCompletion`.   ```python from autogen import oai  # create a text completion request response = oai.Completion.create(     config_list=[         {             \"model\": \"chatglm2-6b\",             \"base_url\": \"http://localhost:8000/v1\",             \"api_type\": \"openai\",             \"api_key\": \"NULL\", # just a placeholder         }     ],     prompt=\"Hi\", ) print(response)  # create a chat completion request response = oai.ChatCompletion.create(     config_list=[         {             \"model\": \"chatglm2-6b\",             \"base_url\": \"http://localhost:8000/v1\",             \"api_type\": \"openai\",             \"api_key\": \"NULL\",         }     ],     messages=[{\"role\": \"user\", \"content\": \"Hi\"}] ) print(response) ```  If you would like to switch to different models, download their checkpoints and specify model path when launching model worker(s). "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/blog/2023-07-14-Local-LLMs/index.md",
        "label": "autogen",
        "content": "interacting with multiple local LLMs  If you would like to interact with multiple LLMs on your local machine, replace the `model_worker` step above with a multi model variant:  ```bash python -m fastchat.serve.multi_model_worker \\     --model-path lmsys/vicuna-7b-v1.3 \\     --model-names vicuna-7b-v1.3 \\     --model-path chatglm2-6b \\     --model-names chatglm2-6b ```  The inference code would be:  ```python from autogen import oai  # create a chat completion request response = oai.ChatCompletion.create(     config_list=[         {             \"model\": \"chatglm2-6b\",             \"base_url\": \"http://localhost:8000/v1\",             \"api_type\": \"openai\",             \"api_key\": \"NULL\",         },         {             \"model\": \"vicuna-7b-v1.3\",             \"base_url\": \"http://localhost:8000/v1\",             \"api_type\": \"openai\",             \"api_key\": \"NULL\",         }     ],     messages=[{\"role\": \"user\", \"content\": \"Hi\"}] ) print(response) ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/website/blog/2023-07-14-Local-LLMs/index.md",
        "label": "autogen",
        "content": "For Further Reading  * [Documentation](/docs/Getting-Started) about `autogen`. * [Documentation](https://github.com/lm-sys/FastChat) about FastChat. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/notebook/contributing.md",
        "label": "autogen",
        "content": "# Contributing "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/notebook/contributing.md",
        "label": "autogen",
        "content": "How to get a notebook displayed on the website  In the notebook metadata set the `tags` and `description` `front_matter` properties. For example:  ```json {     \"...\": \"...\",     \"metadata\": {         \"...\": \"...\",         \"front_matter\": {             \"tags\": [\"code generation\", \"debugging\"],             \"description\": \"Use conversable language learning model agents to solve tasks and provide automatic feedback through a comprehensive example of writing, executing, and debugging Python code to compare stock price changes.\"         }     } } ```  **Note**: Notebook metadata can be edited by opening the notebook in a text editor (Or \"Open With...\" -> \"Text Editor\" in VSCode)  The `tags` field is a list of tags that will be used to categorize the notebook. The `description` field is a brief description of the notebook. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/notebook/contributing.md",
        "label": "autogen",
        "content": "Best practices for authoring notebooks  The following points are best practices for authoring notebooks to ensure consistency and ease of use for the website.  - The Colab button will be automatically generated on the website for all notebooks where it is missing. Going forward, it is recommended to not include the Colab button in the notebook itself. - Ensure the header is a `h1` header, - `#` - Don't put anything between the yaml and the header  ### Consistency for installation and LLM config  You don't need to explain in depth how to install AutoGen. Unless there are specific instructions for the notebook just use the following markdown snippet:  `````` ````{=mdx} :::info Requirements Install `pyautogen`: ```bash pip install pyautogen ```  For more information, please refer to the [installation guide](/docs/installation/). ::: ```` ``````  Or if extras are needed:  `````` ````{=mdx} :::info Requirements Some extra dependencies are needed for this notebook, which can be installed via pip:  ```bash pip install pyautogen[retrievechat] flaml[automl] ```  For more information, please refer to the [installation guide](/docs/installation/). ::: ```` ``````  When specifying the config list, to ensure consistency it is best to use approximately the following code:  ```python import autogen  config_list = autogen.config_list_from_json(     env_or_file=\"OAI_CONFIG_LIST\", ) ```  Then after the code cell where this is used, include the following markdown snippet:  `````` ````{=mdx} :::tip Learn more about configuring LLMs for agents [here](/docs/topics/llm_configuration). ::: ```` `````` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/notebook/contributing.md",
        "label": "autogen",
        "content": "Testing  Notebooks can be tested by running:  ```sh python website/process_notebooks.py test ```  This will automatically scan for all notebooks in the notebook/ and website/ dirs.  To test a specific notebook pass its path:  ```sh python website/process_notebooks.py test notebook/agentchat_logging.ipynb ```  Options: - `--timeout` - timeout for a single notebook - `--exit-on-first-fail` - stop executing further notebooks after the first one fails  ### Skip tests  If a notebook needs to be skipped then add to the notebook metadata: ```json {     \"...\": \"...\",     \"metadata\": {         \"skip_test\": \"REASON\"     } } ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/notebook/contributing.md",
        "label": "autogen",
        "content": "Metadata fields  All possible metadata fields are as follows: ```json {     \"...\": \"...\",     \"metadata\": {         \"...\": \"...\",         \"front_matter\": {             \"tags\": \"List[str] - List of tags to categorize the notebook\",             \"description\": \"str - Brief description of the notebook\",         },         \"skip_test\": \"str - Reason for skipping the test. If present, the notebook will be skipped during testing\",         \"skip_render\": \"str - Reason for skipping rendering the notebook. If present, the notebook will be left out of the website.\",         \"extra_files_to_copy\": \"List[str] - List of files to copy to the website. The paths are relative to the notebook directory\",     } } ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/README.md",
        "label": "autogen",
        "content": "### AutoGen for .NET  [![dotnet-ci](https://github.com/microsoft/autogen/actions/workflows/dotnet-build.yml/badge.svg)](https://github.com/microsoft/autogen/actions/workflows/dotnet-build.yml) [![NuGet version](https://badge.fury.io/nu/AutoGen.Core.svg)](https://badge.fury.io/nu/AutoGen.Core)  > [!NOTE] > Nightly build is available at: > - ![Static Badge](https://img.shields.io/badge/public-blue?style=flat) ![Static Badge](https://img.shields.io/badge/nightly-yellow?style=flat) ![Static Badge](https://img.shields.io/badge/github-grey?style=flat): https://nuget.pkg.github.com/microsoft/index.json > - ![Static Badge](https://img.shields.io/badge/public-blue?style=flat) ![Static Badge](https://img.shields.io/badge/nightly-yellow?style=flat) ![Static Badge](https://img.shields.io/badge/myget-grey?style=flat): https://www.myget.org/F/agentchat/api/v3/index.json > - ![Static Badge](https://img.shields.io/badge/internal-blue?style=flat) ![Static Badge](https://img.shields.io/badge/nightly-yellow?style=flat) ![Static Badge](https://img.shields.io/badge/azure_devops-grey?style=flat) : https://devdiv.pkgs.visualstudio.com/DevDiv/_packaging/AutoGen/nuget/v3/index.json   Firstly, following the [installation guide](./website/articles/Installation.md) to install AutoGen packages.  Then you can start with the following code snippet to create a conversable agent and chat with it.  ```csharp using AutoGen; using AutoGen.OpenAI;  var openAIKey = Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\") ?? throw new Exception(\"Please set OPENAI_API_KEY environment variable.\"); var gpt35Config = new OpenAIConfig(openAIKey, \"gpt-3.5-turbo\");  var assistantAgent = new AssistantAgent(     name: \"assistant\",     systemMessage: \"You are an assistant that help user to do some tasks.\",     llmConfig: new ConversableAgentConfig     {         Temperature = 0,         ConfigList = [gpt35Config],     })     .RegisterPrintMessage(); // register a hook to print message nicely to console  // set human input mode to ALWAYS so that user always provide input var userProxyAgent = new UserProxyAgent(     name: \"user\",     humanInputMode: ConversableAgent.HumanInputMode.ALWAYS)     .RegisterPrintMessage();  // start the conversation await userProxyAgent.InitiateChatAsync(     receiver: assistantAgent,     message: \"Hey assistant, please do me a favor.\",     maxRound: 10); ```  #### Samples You can find more examples under the [sample project](https://github.com/microsoft/autogen/tree/dotnet/dotnet/sample/AutoGen.BasicSamples).  #### Functionality - ConversableAgent     - [x] function call     - [x] code execution (dotnet only, powered by [`dotnet-interactive`](https://github.com/dotnet/interactive))  - Agent communication     - [x] Two-agent chat     - [x] Group chat  - [ ] Enhanced LLM Inferences  - Exclusive for dotnet     - [x] Source generator for type-safe function definition generation  #### Update log ##### Update on 0.0.11 (2024-03-26) - Add link to Discord channel in nuget's readme.md - Document improvements ##### Update on 0.0.10 (2024-03-12) - Rename `Workflow` to `Graph` - Rename `AddInitializeMessage` to `SendIntroduction` - Rename `SequentialGroupChat` to `RoundRobinGroupChat` ##### Update on 0.0.9 (2024-03-02) - Refactor over @AutoGen.Message and introducing `TextMessage`, `ImageMessage`, `MultiModalMessage` and so on. PR [#1676](https://github.com/microsoft/autogen/pull/1676) - Add `AutoGen.SemanticKernel` to support seamless integration with Semantic Kernel - Move the agent contract abstraction to `AutoGen.Core` package. The `AutoGen.Core` package provides the abstraction for message type, agent and group chat and doesn't contain dependencies over `Azure.AI.OpenAI` or `Semantic Kernel`. This is useful when you want to leverage AutoGen's abstraction only and want to avoid introducing any other dependencies. - Move `GPTAgent`, `OpenAIChatAgent` and all openai-dependencies to `AutoGen.OpenAI` ##### Update on 0.0.8 (2024-02-28) - Fix [#1804](https://github.com/microsoft/autogen/pull/1804) - Streaming support for IAgent [#1656](https://github.com/microsoft/autogen/pull/1656) - Streaming support for middleware via `MiddlewareStreamingAgent` [#1656](https://github.com/microsoft/autogen/pull/1656) - Graph chat support with conditional transition workflow [#1761](https://github.com/microsoft/autogen/pull/1761) - AutoGen.SourceGenerator: Generate `FunctionContract` from `FunctionAttribute` [#1736](https://github.com/microsoft/autogen/pull/1736) ##### Update on 0.0.7 (2024-02-11) - Add `AutoGen.LMStudio` to support comsume openai-like API from LMStudio local server ##### Update on 0.0.6 (2024-01-23) - Add `MiddlewareAgent` - Use `MiddlewareAgent` to implement existing agent hooks (RegisterPreProcess, RegisterPostProcess, RegisterReply) - Remove `AutoReplyAgent`, `PreProcessAgent`, `PostProcessAgent` because they are replaced by `MiddlewareAgent` ##### Update on 0.0.5 - Simplify `IAgent` interface by removing `ChatLLM` Property - Add `GenerateReplyOptions` to `IAgent.GenerateReplyAsync` which allows user to specify or override the options when generating reply  ##### Update on 0.0.4 - Move out dependency of Semantic Kernel - Add type `IChatLLM` as connector to LLM  ##### Update on 0.0.3 - In AutoGen.SourceGenerator, rename FunctionAttribution to FunctionAttribute - In AutoGen, refactor over ConversationAgent, UserProxyAgent, and AssistantAgent  ##### Update on 0.0.2 - update Azure.OpenAI.AI to 1.0.0-beta.12 - update Semantic kernel to 1.0.1 "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/nuget/NUGET.md",
        "label": "autogen",
        "content": "### About AutoGen for .NET `AutoGen for .NET` is the official .NET SDK for [AutoGen](https://github.com/microsoft/autogen). It enables you to create LLM agents and construct multi-agent workflows with ease. It also provides integration with popular platforms like OpenAI, Semantic Kernel, and LM Studio.  ### Gettings started - Find documents and examples on our [document site](https://microsoft.github.io/autogen-for-net/)  - Join our [Discord channel](https://discord.gg/pAbnFJrkgZ) to get help and discuss with the community - Report a bug or request a feature by creating a new issue in our [github repo](https://github.com/microsoft/autogen) - Consume the nightly build package from one of the [nightly build feeds](https://microsoft.github.io/autogen-for-net/articles/Installation.html#nighly-build)"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/README.md",
        "label": "autogen",
        "content": "## How to build and run the website  ### Prerequisites - dotnet 7.0 or later  ### Build Firstly, go to autogen/dotnet folder and run the following command to build the website: ```bash dotnet tool restore dotnet tool run docfx website/docfx.json --serve ```  After the command is executed, you can open your browser and navigate to `http://localhost:8080` to view the website."
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/index.md",
        "label": "autogen",
        "content": "[!INCLUDE [](./articles/getting-start.md)]"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Create-a-user-proxy-agent.md",
        "label": "autogen",
        "content": "## UserProxyAgent  [`UserProxyAgent`](../api/AutoGen.UserProxyAgent.yml) is a special type of agent that can be used to proxy user input to another agent or group of agents. It supports the following human input modes: - `ALWAYS`: Always ask user for input. - `NEVER`: Never ask user for input. In this mode, the agent will use the default response (if any) to respond to the message. Or using underlying LLM model to generate response if provided. - `AUTO`: Only ask user for input when conversation is terminated by the other agent(s). Otherwise, use the default response (if any) to respond to the message. Or using underlying LLM model to generate response if provided.  > [!TIP] > You can also set up `humanInputMode` when creating `AssistantAgent` to enable/disable human input. `UserProxyAgent` is equivalent to `AssistantAgent` with `humanInputMode` set to `ALWAYS`. Similarly, `AssistantAgent` is equivalent to `UserProxyAgent` with `humanInputMode` set to `NEVER`.  ### Create a `UserProxyAgent` with `HumanInputMode` set to `ALWAYS`  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/UserProxyAgentCodeSnippet.cs?name=code_snippet_1)]  When running the code, the user proxy agent will ask user for input and use the input as response. ![code output](../images/articles/CreateUserProxyAgent/image-1.png)"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/MistralChatAgent-use-function-call.md",
        "label": "autogen",
        "content": "## Use tool in MistralChatAgent  The following example shows how to enable tool support in @AutoGen.Mistral.MistralClientAgent by creating a `GetWeatherAsync` function and passing it to the agent.  Firstly, you need to install the following packages: ```bash dotnet add package AutoGen.Mistral dotnet add package AutoGen.SourceGenerator ```  > [!Note] > Tool support is only available in some mistral models. Please refer to the [link](https://docs.mistral.ai/capabilities/function_calling/#available-models) for tool call support in mistral models.  > [!Note] > The `AutoGen.SourceGenerator` package carries a source generator that adds support for type-safe function definition generation. For more information, please check out [Create type-safe function](./Create-type-safe-function-call.md).  > [!NOTE] > If you are using VSCode as your editor, you may need to restart the editor to see the generated code.  Import the required namespace [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/MistralAICodeSnippet.cs?name=using_statement)]  Then define a public partial `MistralAgentFunction` class and `GetWeather` method. The `GetWeather` method is a simple function that returns the weather of a given location that marked with @AutoGen.Core.FunctionAttribute. Marking the class as `public partial` together with the @AutoGen.Core.FunctionAttribute attribute allows the source generator to generate the @AutoGen.Core.FunctionContract for the `GetWeather` method.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/MistralAICodeSnippet.cs?name=weather_function)]  Then create an @AutoGen.Mistral.MistralClientAgent and register it with @AutoGen.Mistral.Extension.MistralAgentExtension.RegisterMessageConnector* so it can support @AutoGen.Core.ToolCallMessage and @AutoGen.Core.ToolCallResultMessage. These message types are necessary to use @AutoGen.Core.FunctionCallMiddleware, which provides support for processing and invoking function calls.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/MistralAICodeSnippet.cs?name=create_mistral_function_call_agent)]  Then create an @AutoGen.Core.FunctionCallMiddleware with `GetWeather` function When creating the middleware, we also pass a `functionMap` object which means the function will be automatically invoked when the agent replies a `GetWeather` function call.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/MistralAICodeSnippet.cs?name=create_get_weather_function_call_middleware)]  After the function call middleware is created, register it with the agent so the `GetWeather` function will be passed to agent during chat completion.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/MistralAICodeSnippet.cs?name=register_function_call_middleware)]  Finally, you can chat with the @AutoGen.Mistral.MistralClientAgent about weather! The agent will automatically invoke the `GetWeather` function to \"get\" the weather information and return the result.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/MistralAICodeSnippet.cs?name=send_message_with_function_call)]"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Function-call-with-ollama-and-litellm.md",
        "label": "autogen",
        "content": "This example shows how to use function call with local LLM models where [Ollama](https://ollama.com/) as local model provider and [LiteLLM](https://docs.litellm.ai/docs/) proxy server which provides an openai-api compatible interface.  [![](https://img.shields.io/badge/Open%20on%20Github-grey?logo=github)](https://github.com/microsoft/autogen/blob/main/dotnet/sample/AutoGen.OpenAI.Sample/Tool_Call_With_Ollama_And_LiteLLM.cs)  To run this example, the following prerequisites are required: - Install [Ollama](https://ollama.com/) and [LiteLLM](https://docs.litellm.ai/docs/) on your local machine. - A local model that supports function call. In this example `dolphincoder:latest` is used. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Function-call-with-ollama-and-litellm.md",
        "label": "autogen",
        "content": "Install Ollama and pull `dolphincoder:latest` model First, install Ollama by following the instructions on the [Ollama website](https://ollama.com/).  After installing Ollama, pull the `dolphincoder:latest` model by running the following command: ```bash ollama pull dolphincoder:latest ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Function-call-with-ollama-and-litellm.md",
        "label": "autogen",
        "content": "Install LiteLLM and start the proxy server  You can install LiteLLM by following the instructions on the [LiteLLM website](https://docs.litellm.ai/docs/). ```bash pip install 'litellm[proxy]' ```  Then, start the proxy server by running the following command:  ```bash litellm --model ollama_chat/dolphincoder --port 4000 ```  This will start an openai-api compatible proxy server at `http://localhost:4000`. You can verify if the server is running by observing the following output in the terminal:  ```bash #------------------------------------------------------------# #                                                            # #         'The worst thing about this product is...'          # #        https://github.com/BerriAI/litellm/issues/new        # #                                                            # #------------------------------------------------------------#  INFO:     Application startup complete. INFO:     Uvicorn running on http://0.0.0.0:4000 (Press CTRL+C to quit) ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Function-call-with-ollama-and-litellm.md",
        "label": "autogen",
        "content": "Install AutoGen and AutoGen.SourceGenerator In your project, install the AutoGen and AutoGen.SourceGenerator package using the following command:  ```bash dotnet add package AutoGen dotnet add package AutoGen.SourceGenerator ```  The `AutoGen.SourceGenerator` package is used to automatically generate type-safe `FunctionContract` instead of manually defining them. For more information, please check out [Create type-safe function](Create-type-safe-function-call.md).  And in your project file, enable structural xml document support by setting the `GenerateDocumentationFile` property to `true`:  ```xml <PropertyGroup>     <!-- This enables structural xml document support -->     <GenerateDocumentationFile>true</GenerateDocumentationFile> </PropertyGroup> ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Function-call-with-ollama-and-litellm.md",
        "label": "autogen",
        "content": "Define `WeatherReport` function and create @AutoGen.Core.FunctionCallMiddleware  Create a `public partial` class to host the methods you want to use in AutoGen agents. The method has to be a `public` instance method and its return type must be `Task<string>`. After the methods are defined, mark them with `AutoGen.Core.FunctionAttribute` attribute.  [!code-csharp[Define WeatherReport function](../../sample/AutoGen.OpenAI.Sample/Tool_Call_With_Ollama_And_LiteLLM.cs?name=Function)]  Then create a @AutoGen.Core.FunctionCallMiddleware and add the `WeatherReport` function to the middleware. The middleware will pass the `FunctionContract` to the agent when generating a response, and process the tool call response when receiving a `ToolCallMessage`. [!code-csharp[Define WeatherReport function](../../sample/AutoGen.OpenAI.Sample/Tool_Call_With_Ollama_And_LiteLLM.cs?name=Create_tools)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Function-call-with-ollama-and-litellm.md",
        "label": "autogen",
        "content": "Create @AutoGen.OpenAI.OpenAIChatAgent with `GetWeatherReport` tool and chat with it  Because LiteLLM proxy server is openai-api compatible, we can use @AutoGen.OpenAI.OpenAIChatAgent to connect to it as a third-party openai-api provider. The agent is also registered with a @AutoGen.Core.FunctionCallMiddleware which contains the `WeatherReport` tool. Therefore, the agent can call the `WeatherReport` tool when generating a response.  [!code-csharp[Create an agent with tools](../../sample/AutoGen.OpenAI.Sample/Tool_Call_With_Ollama_And_LiteLLM.cs?name=Create_Agent)]  The reply from the agent will similar to the following: ```bash AggregateMessage from assistant -------------------- ToolCallMessage: ToolCallMessage from assistant -------------------- - GetWeatherAsync: {\"city\": \"new york\"} --------------------  ToolCallResultMessage: ToolCallResultMessage from assistant -------------------- - GetWeatherAsync: The weather in new york is 72 degrees and sunny. -------------------- ```"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Group-chat-overview.md",
        "label": "autogen",
        "content": "@AutoGen.Core.IGroupChat is a fundamental feature in AutoGen. It provides a way to organize multiple agents under the same context and work together to resolve a given task.  In AutoGen, there are two types of group chat: - @AutoGen.Core.RoundRobinGroupChat : This group chat runs agents in a round-robin sequence. The chat history plus the most recent reply from the previous agent will be passed to the next agent. - @AutoGen.Core.GroupChat : This group chat provides a more dynamic yet controlable way to determine the next speaker agent. You can either use a llm agent as group admin, or use a @AutoGen.Core.Graph, which is introduced by [this PR](https://github.com/microsoft/autogen/pull/1761), or both to determine the next speaker agent.  > [!NOTE] > In @AutoGen.Core.GroupChat, when only the group admin is used to determine the next speaker agent, it's recommented to use a more powerful llm model, such as `gpt-4` to ensure the best experience."
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Run-dotnet-code.md",
        "label": "autogen",
        "content": "`AutoGen` provides a built-in feature to run code snippet from agent response. Currently the following languages are supported: - dotnet  More languages will be supported in the future. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Run-dotnet-code.md",
        "label": "autogen",
        "content": "What is a code snippet? A code snippet in agent response is a code block with a language identifier. For example:  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/RunCodeSnippetCodeSnippet.cs?name=code_snippet_1_3)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Run-dotnet-code.md",
        "label": "autogen",
        "content": "Why running code snippet is useful? The ability of running code snippet can greatly extend the ability of an agent. Because it enables agent to resolve tasks by writing code and run it, which is much more powerful than just returning a text response.  For example, in data analysis scenario, agent can resolve tasks like \"What is the average of the sales amount of the last 7 days?\" by firstly write a code snippet to query the sales amount of the last 7 days, then calculate the average and then run the code snippet to get the result.  > [!WARNING] > Running arbitrary code snippet from agent response could bring risks to your system. Using this feature with caution. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Run-dotnet-code.md",
        "label": "autogen",
        "content": "Use dotnet interactive kernel to execute code snippet? The built-in feature of running dotnet code snippet is provided by [dotnet-interactive](https://github.com/dotnet/interactive). To run dotnet code snippet, you need to install the following package to your project, which provides the intergraion with dotnet-interactive:  ```xml <PackageReference Include=\"AutoGen.DotnetInteractive\" /> ```  Then you can use @AutoGen.DotnetInteractive.DotnetInteractiveKernelBuilder* to create a in-process dotnet-interactive composite kernel with C# and F# kernels. [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/RunCodeSnippetCodeSnippet.cs?name=code_snippet_1_1)]  After that, use @AutoGen.DotnetInteractive.Extension.RunSubmitCodeCommandAsync* method to run code snippet. The method will return the result of the code snippet. [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/RunCodeSnippetCodeSnippet.cs?name=code_snippet_1_2)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Run-dotnet-code.md",
        "label": "autogen",
        "content": "Run python code snippet To run python code, firstly you need to have python installed on your machine, then you need to set up ipykernel and jupyter in your environment.  ```bash pip install ipykernel pip install jupyter ```  After `ipykernel` and `jupyter` are installed, you can confirm the ipykernel is installed correctly by running the following command:  ```bash jupyter kernelspec list ```  The output should contain all available kernels, including `python3`.  ```bash Available kernels:     python3    /usr/local/share/jupyter/kernels/python3     ... ```  Then you can add the python kernel to the dotnet-interactive composite kernel by calling `AddPythonKernel` method.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/RunCodeSnippetCodeSnippet.cs?name=code_snippet_1_4)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Run-dotnet-code.md",
        "label": "autogen",
        "content": "Further reading You can refer to the following examples for running code snippet in agentic workflow: - Dynamic_GroupChat_Coding_Task:  [![](https://img.shields.io/badge/Open%20on%20Github-grey?logo=github)](https://github.com/microsoft/autogen/blob/main/dotnet/sample/AutoGen.BasicSample/Example04_Dynamic_GroupChat_Coding_Task.cs) - Dynamic_GroupChat_Calculate_Fibonacci: [![](https://img.shields.io/badge/Open%20on%20Github-grey?logo=github)](https://github.com/microsoft/autogen/blob/main/dotnet/sample/AutoGen.BasicSample/Example07_Dynamic_GroupChat_Calculate_Fibonacci.cs) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Consume-LLM-server-from-LM-Studio.md",
        "label": "autogen",
        "content": "## Consume LLM server from LM Studio You can use @AutoGen.LMStudio.LMStudioAgent from `AutoGen.LMStudio` package to consume openai-like API from LMStudio local server.  ### What's LM Studio [LM Studio](https://lmstudio.ai/) is an app that allows you to deploy and inference hundreds of thousands of open-source language model on your local machine. It provides an in-app chat ui plus an openai-like API to interact with the language model programmatically.  ### Installation - Install LM studio if you haven't done so. You can find the installation guide [here](https://lmstudio.ai/) - Add `AutoGen.LMStudio` to your project. ```xml <ItemGroup>     <PackageReference Include=\"AutoGen.LMStudio\" Version=\"AUTOGEN_LMSTUDIO_VERSION\" /> </ItemGroup> ```  ### Usage The following code shows how to use `LMStudioAgent` to write a piece of C# code to calculate 100th of fibonacci. Before running the code, make sure you have local server from LM Studio running on `localhost:1234`.  [!code-csharp[](../../sample/AutoGen.BasicSamples/Example08_LMStudio.cs?name=lmstudio_using_statements)] [!code-csharp[](../../sample/AutoGen.BasicSamples/Example08_LMStudio.cs?name=lmstudio_example_1)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/OpenAIChatAgent-use-function-call.md",
        "label": "autogen",
        "content": "The following example shows how to create a `GetWeatherAsync` function and pass it to @AutoGen.OpenAI.OpenAIChatAgent.  Firstly, you need to install the following packages: ```xml <ItemGroup>     <PackageReference Include=\"AutoGen.OpenAI\" Version=\"AUTOGEN_VERSION\" />     <PackageReference Include=\"AutoGen.SourceGenerator\" Version=\"AUTOGEN_VERSION\" /> </ItemGroup> ```  > [!Note] > The `AutoGen.SourceGenerator` package carries a source generator that adds support for type-safe function definition generation. For more information, please check out [Create type-safe function](./Create-type-safe-function-call.md).  > [!NOTE] > If you are using VSCode as your editor, you may need to restart the editor to see the generated code.  Firstly, import the required namespaces: [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/OpenAICodeSnippet.cs?name=using_statement)]  Then, define a public partial class: `Function` with `GetWeather` method [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/OpenAICodeSnippet.cs?name=weather_function)]  Then, create an @AutoGen.OpenAI.OpenAIChatAgent and register it with @AutoGen.OpenAI.OpenAIChatRequestMessageConnector so it can support @AutoGen.Core.ToolCallMessage and @AutoGen.Core.ToolCallResultMessage. These message types are necessary to use @AutoGen.Core.FunctionCallMiddleware, which provides support for processing and invoking function calls.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/OpenAICodeSnippet.cs?name=openai_chat_agent_get_weather_function_call)]  Then, create an @AutoGen.Core.FunctionCallMiddleware with `GetWeather` function and register it with the agent above. When creating the middleware, we also pass a `functionMap` to @AutoGen.Core.FunctionCallMiddleware, which means the function will be automatically invoked when the agent replies a `GetWeather` function call.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/OpenAICodeSnippet.cs?name=create_function_call_middleware)]  Finally, you can chat with the @AutoGen.OpenAI.OpenAIChatAgent and invoke the `GetWeather` function.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/OpenAICodeSnippet.cs?name=chat_agent_send_function_call)]"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Use-function-call.md",
        "label": "autogen",
        "content": "## Use function call in AutoGen agent  Typically, there are three ways to pass a function definition to an agent to enable function call: - Pass function definitions when creating an agent. This only works if the agent supports pass function call from its constructor. - Passing function definitions in @AutoGen.Core.GenerateReplyOptions when invoking an agent - Register an agent with @AutoGen.Core.FunctionCallMiddleware to process and invoke function calls.  > [!NOTE] > To use function call, the underlying LLM model must support function call as well for the best experience. If the model does not support function call, it's likely that the function call will be ignored and the model will reply with a normal response even if a function call is passed to it. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Use-function-call.md",
        "label": "autogen",
        "content": "Pass function definitions when creating an agent In some agents like @AutoGen.AssistantAgent or @AutoGen.OpenAI.GPTAgent, you can pass function definitions when creating the agent  Suppose the `TypeSafeFunctionCall` is defined in the following code snippet: [!code-csharp[TypeSafeFunctionCall](../../sample/AutoGen.BasicSamples/CodeSnippet/TypeSafeFunctionCallCodeSnippet.cs?name=weather_report)]  You can then pass the `WeatherReport` to the agent when creating it: [!code-csharp[assistant agent](../../sample/AutoGen.BasicSamples/CodeSnippet/FunctionCallCodeSnippet.cs?name=code_snippet_4)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Use-function-call.md",
        "label": "autogen",
        "content": "Passing function definitions in @AutoGen.Core.GenerateReplyOptions when invoking an agent You can also pass function definitions in @AutoGen.Core.GenerateReplyOptions when invoking an agent. This is useful when you want to override the function definitions passed to the agent when creating it.  [!code-csharp[assistant agent](../../sample/AutoGen.BasicSamples/CodeSnippet/FunctionCallCodeSnippet.cs?name=overrider_function_contract)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Use-function-call.md",
        "label": "autogen",
        "content": "Register an agent with @AutoGen.Core.FunctionCallMiddleware to process and invoke function calls You can also register an agent with @AutoGen.Core.FunctionCallMiddleware to process and invoke function calls. This is useful when you want to process and invoke function calls in a more flexible way.  [!code-csharp[assistant agent](../../sample/AutoGen.BasicSamples/CodeSnippet/FunctionCallCodeSnippet.cs?name=register_function_call_middleware)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Use-function-call.md",
        "label": "autogen",
        "content": "Invoke function call inside an agent To invoke a function instead of returning the function call object, you can pass its function call wrapper to the agent via `functionMap`.  You can then pass the `WeatherReportWrapper` to the agent via `functionMap`: [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/FunctionCallCodeSnippet.cs?name=code_snippet_6)]  When a function call object is returned, the agent will invoke the function and uses the return value as response rather than returning the function call object.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/FunctionCallCodeSnippet.cs?name=code_snippet_6_1)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Use-function-call.md",
        "label": "autogen",
        "content": "Invoke function call by another agent You can also use another agent to invoke the function call from one agent. This is a useful pattern in two-agent chat, where one agent is used as a function proxy to invoke the function call from another agent. Once the function call is invoked, the result can be returned to the original agent for further processing.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/FunctionCallCodeSnippet.cs?name=two_agent_weather_chat)]"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Create-your-own-agent.md",
        "label": "autogen",
        "content": "## Coming soon"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/OpenAIChatAgent-use-json-mode.md",
        "label": "autogen",
        "content": "The following example shows how to enable JSON mode in @AutoGen.OpenAI.OpenAIChatAgent.  [![](https://img.shields.io/badge/Open%20on%20Github-grey?logo=github)](https://github.com/microsoft/autogen/blob/main/dotnet/sample/AutoGen.OpenAI.Sample/Use_Json_Mode.cs) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/OpenAIChatAgent-use-json-mode.md",
        "label": "autogen",
        "content": "What is JSON mode? JSON mode is a new feature in OpenAI which allows you to instruct model to always respond with a valid JSON object. This is useful when you want to constrain the model output to JSON format only.  > [!NOTE] > Currently, JOSN mode is only supported by `gpt-4-turbo-preview` and `gpt-3.5-turbo-0125`. For more information (and limitations) about JSON mode, please visit [OpenAI API documentation](https://platform.openai.com/docs/guides/text-generation/json-mode). "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/OpenAIChatAgent-use-json-mode.md",
        "label": "autogen",
        "content": "How to enable JSON mode in OpenAIChatAgent.  To enable JSON mode for @AutoGen.OpenAI.OpenAIChatAgent, set `responseFormat` to `ChatCompletionsResponseFormat.JsonObject` when creating the agent. Note that when enabling JSON mode, you also need to instruct the agent to output JSON format in its system message.  [!code-csharp[](../../sample/AutoGen.OpenAI.Sample/Use_Json_Mode.cs?name=create_agent)]  After enabling JSON mode, the `openAIClientAgent` will always respond in JSON format when it receives a message.  [!code-csharp[](../../sample/AutoGen.OpenAI.Sample/Use_Json_Mode.cs?name=chat_with_agent)]  When running the example, the output from `openAIClientAgent` will be a valid JSON object which can be parsed as `Person` class defined below. Note that in the output, the `address` field is missing because the address information is not provided in user input.  [!code-csharp[](../../sample/AutoGen.OpenAI.Sample/Use_Json_Mode.cs?name=person_class)]  The output will be: ```bash Name: John Age: 25 Done ```"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Installation.md",
        "label": "autogen",
        "content": "### Current version:  [![NuGet version](https://badge.fury.io/nu/AutoGen.Core.svg)](https://badge.fury.io/nu/AutoGen.Core)  AutoGen.Net provides the following packages, you can choose to install one or more of them based on your needs:  - `AutoGen`: The one-in-all package. This package has dependencies over `AutoGen.Core`, `AutoGen.OpenAI`, `AutoGen.LMStudio`, `AutoGen.SemanticKernel` and `AutoGen.SourceGenerator`. - `AutoGen.Core`: The core package, this package provides the abstraction for message type, agent and group chat. - `AutoGen.OpenAI`: This package provides the integration agents over openai models. - `AutoGen.Mistral`: This package provides the integration agents for Mistral.AI models. - `AutoGen.Ollama`: This package provides the integration agents for [Ollama](https://ollama.com/). - `AutoGen.Anthropic`: This package provides the integration agents for [Anthropic](https://www.anthropic.com/api) - `AutoGen.LMStudio`: This package provides the integration agents from LM Studio. - `AutoGen.SemanticKernel`: This package provides the integration agents over semantic kernel. - `AutoGen.Gemini`: This package provides the integration agents from [Google Gemini](https://gemini.google.com/). - `AutoGen.AzureAIInference`: This package provides the integration agents for [Azure AI Inference](https://www.nuget.org/packages/Azure.AI.Inference). - `AutoGen.SourceGenerator`: This package carries a source generator that adds support for type-safe function definition generation. - `AutoGen.DotnetInteractive`: This packages carries dotnet interactive support to execute code snippets. The current supported language is C#, F#, powershell and python.  >[!Note] > Help me choose > - If you just want to install one package and enjoy the core features of AutoGen, choose `AutoGen`. > - If you want to leverage AutoGen's abstraction only and want to avoid introducing any other dependencies, like `Azure.AI.OpenAI` or `Semantic Kernel`, choose `AutoGen.Core`. You will need to implement your own agent, but you can still use AutoGen core features like group chat, built-in message type, workflow and middleware. >- If you want to use AutoGen with openai, choose `AutoGen.OpenAI`, similarly, choose `AutoGen.LMStudio` or `AutoGen.SemanticKernel` if you want to use agents from LM Studio or semantic kernel. >- If you just want the type-safe source generation for function call and don't want any other features, which even include the AutoGen's abstraction, choose `AutoGen.SourceGenerator`.  Then, install the package using the following command:  ```bash dotnet add package AUTOGEN_PACKAGES ```  ### Consume nightly build To consume nightly build, you can add one of the following feeds to your `NuGet.config` or global nuget config: - ![Static Badge](https://img.shields.io/badge/public-blue?style=flat) ![Static Badge](https://img.shields.io/badge/github-grey?style=flat): https://nuget.pkg.github.com/microsoft/index.json - ![Static Badge](https://img.shields.io/badge/public-blue?style=flat) ![Static Badge](https://img.shields.io/badge/myget-grey?style=flat): https://www.myget.org/F/agentchat/api/v3/index.json - ![Static Badge](https://img.shields.io/badge/internal-blue?style=flat) ![Static Badge](https://img.shields.io/badge/azure_devops-grey?style=flat) : https://devdiv.pkgs.visualstudio.com/DevDiv/_packaging/AutoGen/nuget/v3/index.json  To add a local `NuGet.config`, create a file named `NuGet.config` in the root of your project and add the following content: ```xml <?xml version=\"1.0\" encoding=\"utf-8\"?> <configuration>   <packageSources>     <clear />     <!-- dotnet-tools contains Microsoft.DotNet.Interactive.VisualStudio package, which is used by AutoGen.DotnetInteractive -->     <add key=\"dotnet-tools\" value=\"https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet-tools/nuget/v3/index.json\" />     <add key=\"AutoGen\" value=\"$(FEED_URL)\" /> <!-- replace $(FEED_URL) with the feed url -->     <!-- other feeds -->   </packageSources>   <disabledPackageSources /> </configuration> ```  To add the feed to your global nuget config. You can do this by running the following command in your terminal: ```bash dotnet nuget add source FEED_URL --name AutoGen  # dotnet-tools contains Microsoft.DotNet.Interactive.VisualStudio package, which is used by AutoGen.DotnetInteractive dotnet nuget add source https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet-tools/nuget/v3/index.json --name dotnet-tools ```  Once you have added the feed, you can install the nightly-build package using the following command: ```bash dotnet add package AUTOGEN_PACKAGES VERSION ```   "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/AutoGen-Mistral-Overview.md",
        "label": "autogen",
        "content": "## AutoGen.Mistral overview  AutoGen.Mistral provides the following agent(s) to connect to [Mistral.AI](https://mistral.ai/) platform. - @AutoGen.Mistral.MistralClientAgent: A slim wrapper agent over @AutoGen.Mistral.MistralClient.  ### Get started with AutoGen.Mistral  To get started with AutoGen.Mistral, follow the [installation guide](Installation.md) to make sure you add the AutoGen feed correctly. Then add the `AutoGen.Mistral` package to your project file.  ```bash dotnet add package AutoGen.Mistral ```  >[!NOTE] > You need to provide an api-key to use Mistral models which will bring additional cost while using. you can get the api key from [Mistral.AI](https://mistral.ai/).  ### Example  Import the required namespace [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/MistralAICodeSnippet.cs?name=using_statement)]  Create a @AutoGen.Mistral.MistralClientAgent and start chatting! [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/MistralAICodeSnippet.cs?name=create_mistral_agent)]  Use @AutoGen.Core.IStreamingAgent.GenerateStreamingReplyAsync* to stream the chat completion. [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/MistralAICodeSnippet.cs?name=streaming_chat)]"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Built-in-messages.md",
        "label": "autogen",
        "content": "## An overview of built-in @AutoGen.Core.IMessage types  Start from 0.0.9, AutoGen introduces the @AutoGen.Core.IMessage and @AutoGen.Core.IMessage`1 types to provide a unified message interface for different agents. The @AutoGen.Core.IMessage is a non-generic interface that represents a message. The @AutoGen.Core.IMessage`1 is a generic interface that represents a message with a specific `T` where `T` can be any type.  Besides, AutoGen also provides a set of built-in message types that implement the @AutoGen.Core.IMessage and @AutoGen.Core.IMessage`1 interfaces. These built-in message types are designed to cover different types of messages as much as possilbe. The built-in message types include:  > [!NOTE] > The minimal requirement for an agent to be used as admin in @AutoGen.Core.GroupChat is to support @AutoGen.Core.TextMessage.  > [!NOTE] > @AutoGen.Core.Message will be deprecated in 0.0.14. Please replace it with a more specific message type like @AutoGen.Core.TextMessage, @AutoGen.Core.ImageMessage, etc.  - @AutoGen.Core.TextMessage: A message that contains a piece of text. - @AutoGen.Core.ImageMessage: A message that contains an image. - @AutoGen.Core.MultiModalMessage: A message that contains multiple modalities like text, image, etc. - @AutoGen.Core.ToolCallMessage: A message that represents a function call request. - @AutoGen.Core.ToolCallResultMessage: A message that represents a function call result. - @AutoGen.Core.ToolCallAggregateMessage: A message that contains both @AutoGen.Core.ToolCallMessage and @AutoGen.Core.ToolCallResultMessage. This type of message is used by @AutoGen.Core.FunctionCallMiddleware to aggregate both @AutoGen.Core.ToolCallMessage and @AutoGen.Core.ToolCallResultMessage into a single message. - @AutoGen.Core.MessageEnvelope`1: A message that represents an envelope that contains a message of any type. - @AutoGen.Core.Message: The original message type before 0.0.9. This message type is reserved for backward compatibility. It is recommended to replace it with a more specific message type like @AutoGen.Core.TextMessage, @AutoGen.Core.ImageMessage, etc.  ### Streaming message support AutoGen also introduces @AutoGen.Core.IStreamingMessage and @AutoGen.Core.IStreamingMessage`1 which are used in streaming call api. The following built-in message types implement the @AutoGen.Core.IStreamingMessage and @AutoGen.Core.IStreamingMessage`1 interfaces:  > [!NOTE] > All @AutoGen.Core.IMessage is also a @AutoGen.Core.IStreamingMessage. That means you can return an @AutoGen.Core.IMessage from a streaming call method. It's also recommended to return the final updated result instead of the last update as the last message in the streaming call method to indicate the end of the stream, which saves caller's effort of assembling the final result from multiple updates.  - @AutoGen.Core.TextMessageUpdate: A message that contains a piece of text update. - @AutoGen.Core.ToolCallMessageUpdate: A message that contains a function call request update.  #### Usage  The below code snippet shows how to print a streaming update to console and update the final result on the caller side. [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/BuildInMessageCodeSnippet.cs?name=StreamingCallCodeSnippet)]  If the agent returns a final result instead of the last update as the last message in the streaming call method, the caller can directly use the final result without assembling the final result from multiple updates.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/BuildInMessageCodeSnippet.cs?name=StreamingCallWithFinalMessage)]"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Roundrobin-chat.md",
        "label": "autogen",
        "content": "@AutoGen.Core.RoundRobinGroupChat is a group chat that invokes agents in a round-robin order. It's useful when you want to call multiple agents in a fixed sequence. For example, asking search agent to retrieve related information followed by a summarization agent to summarize the information. Beside, it also used by @AutoGen.Core.AgentExtension.SendAsync(AutoGen.Core.IAgent,AutoGen.Core.IAgent,System.String,System.Collections.Generic.IEnumerable{AutoGen.Core.IMessage},System.Int32,System.Threading.CancellationToken) in two agent chat.  ### Use @AutoGen.Core.RoundRobinGroupChat to implement a search-summarize chat flow  ```mermaid flowchart LR     A[User] -->|Ask a question| B[Search Agent]     B -->|Retrieve information| C[Summarization Agent]     C -->|Summarize result| A[User] ```  > [!NOTE] > Complete code can be found in [Example11_Sequential_GroupChat_Example](https://github.com/microsoft/autogen/blob/dotnet/dotnet/sample/AutoGen.BasicSamples/Example11_Sequential_GroupChat_Example.cs);  Step 1: Add required using statements  [!code-csharp[](../../sample/AutoGen.BasicSamples/Example11_Sequential_GroupChat_Example.cs?name=using_statement)]  Step 2: Create a `bingSearch` agent using @AutoGen.SemanticKernel.SemanticKernelAgent  [!code-csharp[](../../sample/AutoGen.BasicSamples/Example11_Sequential_GroupChat_Example.cs?name=CreateBingSearchAgent)]  Step 3: Create a `summarization` agent using @AutoGen.SemanticKernel.SemanticKernelAgent  [!code-csharp[](../../sample/AutoGen.BasicSamples/Example11_Sequential_GroupChat_Example.cs?name=CreateSummarizerAgent)]  Step 4: Create a @AutoGen.Core.RoundRobinGroupChat and add `bingSearch` and `summarization` agents to it  [!code-csharp[](../../sample/AutoGen.BasicSamples/Example11_Sequential_GroupChat_Example.cs?name=Sequential_GroupChat_Example)]  Output:  ![Searcher-Summarizer](../images/articles/SequentialGroupChat/SearcherSummarizer.gif)"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Agent-overview.md",
        "label": "autogen",
        "content": "`Agent` is one of the most fundamental concepts in AutoGen.Net. In AutoGen.Net, you construct a single agent to process a specific task, and you extend an agent using [Middlewares](./Middleware-overview.md), and you construct a multi-agent workflow using [GroupChat](./Group-chat-overview.md).  > [!NOTE] > Every agent in AutoGen.Net implements @AutoGen.Core.IAgent, for agent that supports streaming reply, it also implements @AutoGen.Core.IStreamingAgent. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Agent-overview.md",
        "label": "autogen",
        "content": "Create an agent - Create an @AutoGen.AssistantAgent: [Create an assistant agent](./Create-an-agent.md) - Create an @AutoGen.OpenAI.OpenAIChatAgent: [Create an OpenAI chat agent](./OpenAIChatAgent-simple-chat.md) - Create a @AutoGen.SemanticKernel.SemanticKernelAgent: [Create a semantic kernel agent](./AutoGen.SemanticKernel/SemanticKernelAgent-simple-chat.md) - Create a @AutoGen.LMStudio.LMStudioAgent: [Connect to LM Studio](./Consume-LLM-server-from-LM-Studio.md) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Agent-overview.md",
        "label": "autogen",
        "content": "Chat with an agent To chat with an agent, typically you can invoke @AutoGen.Core.IAgent.GenerateReplyAsync*. On top of that, you can also use one of the extension methods like @AutoGen.Core.AgentExtension.SendAsync* as shortcuts.  > [!NOTE] > AutoGen provides a list of built-in message types like @AutoGen.Core.TextMessage, @AutoGen.Core.ImageMessage, @AutoGen.Core.MultiModalMessage, @AutoGen.Core.ToolCallMessage, @AutoGen.Core.ToolCallResultMessage, etc. You can use these message types to chat with an agent. For further details, see [built-in messages](./Built-in-messages.md).  - Send a @AutoGen.Core.TextMessage to an agent via @AutoGen.Core.IAgent.GenerateReplyAsync*: [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/AgentCodeSnippet.cs?name=ChatWithAnAgent_GenerateReplyAsync)]  - Send a message to an agent via @AutoGen.Core.AgentExtension.SendAsync*: [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/AgentCodeSnippet.cs?name=ChatWithAnAgent_SendAsync)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Agent-overview.md",
        "label": "autogen",
        "content": "Streaming chat If an agent implements @AutoGen.Core.IStreamingAgent, you can use @AutoGen.Core.IStreamingAgent.GenerateStreamingReplyAsync* to chat with the agent in a streaming way. You would need to process the streaming updates on your side though.  - Send a @AutoGen.Core.TextMessage to an agent via @AutoGen.Core.IStreamingAgent.GenerateStreamingReplyAsync*, and print the streaming updates to console: [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/AgentCodeSnippet.cs?name=ChatWithAnAgent_GenerateStreamingReplyAsync)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Agent-overview.md",
        "label": "autogen",
        "content": "Register middleware to an agent @AutoGen.Core.IMiddleware and @AutoGen.Core.IStreamingMiddleware are used to extend the behavior of @AutoGen.Core.IAgent.GenerateReplyAsync* and @AutoGen.Core.IStreamingAgent.GenerateStreamingReplyAsync*. You can register middleware to an agent to customize the behavior of the agent on things like function call support, converting message of different types, print message, gather user input, etc.  - Middleware overview: [Middleware overview](./Middleware-overview.md) - Write message to console: [Print message middleware](./Print-message-middleware.md) - Convert message type: [SemanticKernelChatMessageContentConnector](./AutoGen.SemanticKernel/SemanticKernelAgent-support-more-messages.md) and [OpenAIChatRequestMessageConnector](./OpenAIChatAgent-support-more-messages.md) - Create your own middleware: [Create your own middleware](./Create-your-own-middleware.md) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Agent-overview.md",
        "label": "autogen",
        "content": "Group chat You can construct a multi-agent workflow using @AutoGen.Core.IGroupChat. In AutoGen.Net, there are two type of group chat: @AutoGen.Core.SequentialGroupChat: Orchestrates the agents in the group chat in a fix, sequential order. @AutoGen.Core.GroupChat: Provide more dynamic yet controllable way to orchestrate the agents in the group chat.  For further details, see [Group chat overview](./Group-chat-overview.md)."
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Group-chat.md",
        "label": "autogen",
        "content": "@AutoGen.Core.GroupChat invokes agents in a dynamic way. On one hand, It relies on its admin agent to intellegently determines the next speaker based on conversation context, and on the other hand, it also allows you to control the conversation flow by using a @AutoGen.Core.Graph. This makes it a more dynamic yet controlable way to determine the next speaker agent. You can use @AutoGen.Core.GroupChat to create a dynamic group chat with multiple agents working together to resolve a given task.  > [!NOTE] > In @AutoGen.Core.GroupChat, when only the group admin is used to determine the next speaker agent, it's recommented to use a more powerful llm model, such as `gpt-4` to ensure the best experience. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Group-chat.md",
        "label": "autogen",
        "content": "Use @AutoGen.Core.GroupChat to implement a code interpreter chat flow The following example shows how to create a dynamic group chat with @AutoGen.Core.GroupChat. In this example, we will create a dynamic group chat with 4 agents: `admin`, `coder`, `reviewer` and `runner`. Each agent has its own role in the group chat:  ### Code interpreter group chat - `admin`: create task for group to work on and terminate the conversation when task is completed. In this example, the task to resolve is to calculate the 39th Fibonacci number. - `coder`: a dotnet coder who can write code to resolve tasks. - `reviewer`: a dotnet code reviewer who can review code written by `coder`. In this example, `reviewer` will examine if the code written by `coder` follows the condition below:   - has only one csharp code block.   - use top-level statements.   - is dotnet code snippet.   - print the result of the code snippet to console. - `runner`: a dotnet code runner who can run code written by `coder` and print the result.  ```mermaid flowchart LR   subgraph Group Chat     B[Amin]     C[Coder]     D[Reviewer]     E[Runner]   end ```  > [!NOTE] > The complete code of this example can be found in `Example07_Dynamic_GroupChat_Calculate_Fibonacci`  ### Create group chat  The code below shows how to create a dynamic group chat with @AutoGen.Core.GroupChat. In this example, we will create a dynamic group chat with 4 agents: `admin`, `coder`, `reviewer` and `runner`. In this case we don't pass a workflow to the group chat, so the group chat will use driven by the admin agent.  [!code-csharp[](../../sample/AutoGen.BasicSamples/Example07_Dynamic_GroupChat_Calculate_Fibonacci.cs?name=create_group_chat)]  > [!TIP] > You can set up initial context for the group chat using @AutoGen.Core.GroupChatExtension.SendIntroduction*. The initial context can help group admin orchestrates the conversation flow.  Output:  ![GroupChat](../images/articles/DynamicGroupChat/dynamicChat.gif)  ### Below are break-down of how agents are created and their roles in the group chat.  - Create admin agent  The code below shows how to create `admin` agent. `admin` agent will create a task for group to work on and terminate the conversation when task is completed.  [!code-csharp[](../../sample/AutoGen.BasicSamples/Example07_Dynamic_GroupChat_Calculate_Fibonacci.cs?name=create_admin)]  - Create coder agent  [!code-csharp[](../../sample/AutoGen.BasicSamples/Example07_Dynamic_GroupChat_Calculate_Fibonacci.cs?name=create_coder)]  - Create reviewer agent  The code below shows how to create `reviewer` agent. `reviewer` agent is a dotnet code reviewer who can review code written by `coder`. In this example, a `function` is used to examine if the code written by `coder` follows the condition.  [!code-csharp[](../../sample/AutoGen.BasicSamples/Example07_Dynamic_GroupChat_Calculate_Fibonacci.cs?name=reviewer_function)]  > [!TIP] > You can use @AutoGen.Core.FunctionAttribute to generate type-safe function definition and function call wrapper for the function. For more information, please check out [Create type safe function call](./Create-type-safe-function-call.md).  [!code-csharp[](../../sample/AutoGen.BasicSamples/Example07_Dynamic_GroupChat_Calculate_Fibonacci.cs?name=create_reviewer)]  - Create runner agent  > [!TIP] > `AutoGen` provides a built-in support for running code snippet. For more information, please check out [Execute code snippet](./Run-dotnet-code.md).  [!code-csharp[](../../sample/AutoGen.BasicSamples/Example07_Dynamic_GroupChat_Calculate_Fibonacci.cs?name=create_runner)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Two-agent-chat.md",
        "label": "autogen",
        "content": "In `AutoGen`, you can start a conversation between two agents using @AutoGen.Core.AgentExtension.InitiateChatAsync* or one of @AutoGen.Core.AgentExtension.SendAsync* APIs. When conversation starts, the sender agent will firstly send a message to receiver agent, then receiver agent will generate a reply and send it back to sender agent. This process will repeat until either one of the agent sends a termination message or the maximum number of turns is reached.  > [!NOTE] > A termination message is an @AutoGen.Core.IMessage which content contains the keyword: @AutoGen.Core.GroupChatExtension.TERMINATE. To determine if a message is a terminate message, you can use @AutoGen.Core.GroupChatExtension.IsGroupChatTerminateMessage*. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Two-agent-chat.md",
        "label": "autogen",
        "content": "A basic example  The following example shows how to start a conversation between the teacher agent and student agent, where the student agent starts the conversation by asking teacher to create math questions.  > [!TIP] > You can use @AutoGen.Core.PrintMessageMiddlewareExtension.RegisterPrintMessage* to pretty print the message replied by the agent.  > [!NOTE] > The conversation is terminated when teacher agent sends a message containing the keyword: @AutoGen.Core.GroupChatExtension.TERMINATE.  > [!NOTE] > The teacher agent uses @AutoGen.Core.MiddlewareExtension.RegisterPostProcess* to register a post process function which returns a hard-coded termination message when a certain condition is met. Comparing with putting the @AutoGen.Core.GroupChatExtension.TERMINATE keyword in the prompt, this approach is more robust especially when a weaker LLM model is used.  [!code-csharp[](../../sample/AutoGen.BasicSamples/Example02_TwoAgent_MathChat.cs?name=code_snippet_1)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Middleware-overview.md",
        "label": "autogen",
        "content": "`Middleware` is a key feature in AutoGen.Net that enables you to customize the behavior of @AutoGen.Core.IAgent.GenerateReplyAsync*. It's similar to the middleware concept in ASP.Net and is widely used in AutoGen.Net for various scenarios, such as function call support, converting message of different types, print message, gather user input, etc.  Here are a few examples of how middleware is used in AutoGen.Net: - @AutoGen.AssistantAgent is essentially an agent with @AutoGen.Core.FunctionCallMiddleware, @AutoGen.HumanInputMiddleware and default reply middleware. - @AutoGen.OpenAI.GPTAgent is essentially an @AutoGen.OpenAI.OpenAIChatAgent with @AutoGen.Core.FunctionCallMiddleware and @AutoGen.OpenAI.OpenAIChatRequestMessageConnector. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Middleware-overview.md",
        "label": "autogen",
        "content": "Use middleware in an agent To use middleware in an existing agent, you can either create a @AutoGen.Core.MiddlewareAgent on top of the original agent or register middleware functions to the original agent.  ### Create @AutoGen.Core.MiddlewareAgent on top of the original agent [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/MiddlewareAgentCodeSnippet.cs?name=create_middleware_agent_with_original_agent)]  ### Register middleware functions to the original agent [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/MiddlewareAgentCodeSnippet.cs?name=register_middleware_agent)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Middleware-overview.md",
        "label": "autogen",
        "content": "Short-circuit the next agent The example below shows how to short-circuit the inner agent  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/MiddlewareAgentCodeSnippet.cs?name=short_circuit_middleware_agent)]  > [!Note] > When multiple middleware functions are registered, the order of middleware functions is first registered, last invoked. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Middleware-overview.md",
        "label": "autogen",
        "content": "Streaming middleware You can also modify the behavior of @AutoGen.Core.IStreamingAgent.GenerateStreamingReplyAsync* by registering streaming middleware to it. One example is @AutoGen.OpenAI.OpenAIChatRequestMessageConnector which converts `StreamingChatCompletionsUpdate` to one of `AutoGen.Core.TextMessageUpdate` or `AutoGen.Core.ToolCallMessageUpdate`.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/MiddlewareAgentCodeSnippet.cs?name=register_streaming_middleware)]"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Function-call-overview.md",
        "label": "autogen",
        "content": "## Overview of function call  In some LLM models, you can provide a list of function definitions to the model. The function definition is usually essentially an OpenAPI schema object which describes the function, its parameters and return value. And these function definitions tells the model what \"functions\" are available to be used to resolve the user's request. This feature greatly extend the capability of LLM models by enabling them to \"execute\" arbitrary function as long as it can be described as a function definition.  Below is an example of a function definition for getting weather report for a city:  > [!NOTE] > To use function call, the underlying LLM model must support function call as well for the best experience. > The model used in the example below is `gpt-3.5-turbo-0613`. ```json {     \"name\": \"GetWeather\",     \"description\": \"Get the weather report for a city\",     \"parameters\": {         \"city\": {             \"type\": \"string\",             \"description\": \"The city name\"         },         \"required\": [\"city\"]     }, } ```    When the model receives a message, it will intelligently decide whether to use function call or not based on the message received. If the model decides to use function call, it will generate a function call which can be used to invoke the actual function. The function call is a json object which contains the function name and its arguments.  Below is an example of a function call object for getting weather report for Seattle:  ```json {     \"name\": \"GetWeather\",     \"arguments\": {         \"city\": \"Seattle\"     } } ```  And when the function call is return to the caller, it can be used to invoke the actual function to get the weather report for Seattle.  ### Create type-safe function contract and function call wrapper use AutoGen.SourceGenerator AutoGen provides a source generator to easness the trouble of manually craft function contract and function call wrapper from a function. To use this feature, simply add the `AutoGen.SourceGenerator` package to your project and decorate your function with `Function` attribute.  For more information, please check out [Create type-safe function](Create-type-safe-function-call.md).  ### Use function call in an agent AutoGen provides first-class support for function call in its agent story. Usually there are three ways to enable a function call in an agent. - Pass function definitions when creating an agent. This only works if the agent supports pass function call from its constructor. - Passing function definitions in @AutoGen.Core.GenerateReplyOptions when invoking an agent - Register an agent with @AutoGen.Core.FunctionCallMiddleware to process and invoke function calls.  For more information, please check out [Use function call in an agent](Use-function-call.md)."
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Use-graph-in-group-chat.md",
        "label": "autogen",
        "content": "Sometimes, you may want to add more control on how the next agent is selected in a @AutoGen.Core.GroupChat based on the task you want to resolve. For example, in the previous [code writing example](./Group-chat.md), the original code interpreter workflow can be improved by the following diagram because it's not necessary for `admin` to directly talk to `reviewer`, nor it's necessary for `coder` to talk to `runner`.  ```mermaid flowchart TD     A[Admin] -->|Ask coder to write code| B[Coder]     B -->|Ask Reviewer to review code| C[Reviewer]     C -->|Ask Runner to run code| D[Runner]     D -->|Send result if succeed| A[Admin]     D -->|Ask coder to fix if failed| B[Coder]     C -->|Ask coder to fix if not approved| B[Coder] ```  By having @AutoGen.Core.GroupChat to follow a specific graph flow, we can bring prior knowledge to group chat and make the conversation more efficient and robust. This is where @AutoGen.Core.Graph comes in.  ### Create a graph The following code shows how to create a graph that represents the diagram above. The graph doesn't need to be a finite state machine where each state can only have one legitimate next state. Instead, it can be a directed graph where each state can have multiple legitimate next states. And if there are multiple legitimate next states, the `admin` agent of @AutoGen.Core.GroupChat will decide which one to go based on the conversation context.  > [!TIP] > @AutoGen.Core.Graph supports conditional transitions. To create a conditional transition, you can pass a lambda function to `canTransitionAsync` when creating a @AutoGen.Core.Transition. The lambda function should return a boolean value indicating if the transition can be taken.  [!code-csharp[](../../sample/AutoGen.BasicSamples/Example07_Dynamic_GroupChat_Calculate_Fibonacci.cs?name=create_workflow)]  Once the graph is created, you can pass it to the group chat. The group chat will then use the graph along with admin agent to orchestrate the conversation flow.  [!code-csharp[](../../sample/AutoGen.BasicSamples/Example07_Dynamic_GroupChat_Calculate_Fibonacci.cs?name=create_group_chat_with_workflow)]"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Create-your-own-middleware.md",
        "label": "autogen",
        "content": "## Coming soon"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Function-call-middleware.md",
        "label": "autogen",
        "content": "# Coming soon"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/OpenAIChatAgent-support-more-messages.md",
        "label": "autogen",
        "content": "By default, @AutoGen.OpenAI.OpenAIChatAgent only supports the @AutoGen.Core.IMessage<T> type where `T` is original request or response message from `Azure.AI.OpenAI`. To support more AutoGen built-in message types like @AutoGen.Core.TextMessage, @AutoGen.Core.ImageMessage, @AutoGen.Core.MultiModalMessage and so on, you can register the agent with @AutoGen.OpenAI.OpenAIChatRequestMessageConnector. The @AutoGen.OpenAI.OpenAIChatRequestMessageConnector will convert the message from AutoGen built-in message types to `Azure.AI.OpenAI.ChatRequestMessage` and vice versa.  import the required namespaces: [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/OpenAICodeSnippet.cs?name=using_statement)]  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/OpenAICodeSnippet.cs?name=register_openai_chat_message_connector)]"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/OpenAIChatAgent-connect-to-third-party-api.md",
        "label": "autogen",
        "content": "The following example shows how to connect to third-party OpenAI API using @AutoGen.OpenAI.OpenAIChatAgent.  [![](https://img.shields.io/badge/Open%20on%20Github-grey?logo=github)](https://github.com/microsoft/autogen/blob/main/dotnet/sample/AutoGen.OpenAI.Sample/Connect_To_Ollama.cs) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/OpenAIChatAgent-connect-to-third-party-api.md",
        "label": "autogen",
        "content": "Overview A lot of LLM applications/platforms support spinning up a chat server that is compatible with OpenAI API, such as LM Studio, Ollama, Mistral etc. This means that you can connect to these servers using the @AutoGen.OpenAI.OpenAIChatAgent.  > [!NOTE] > Some platforms might not support all the features of OpenAI API. For example, Ollama does not support `function call` when using it's openai API according to its [document](https://github.com/ollama/ollama/blob/main/docs/openai.md#v1chatcompletions) (as of 2024/05/07). > That means some of the features of OpenAI API might not work as expected when using these platforms with the @AutoGen.OpenAI.OpenAIChatAgent. > Please refer to the platform's documentation for more information. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/OpenAIChatAgent-connect-to-third-party-api.md",
        "label": "autogen",
        "content": "Prerequisites - Install the following packages: ```bash dotnet add package AutoGen.OpenAI --version AUTOGEN_VERSION ```  - Spin up a chat server that is compatible with OpenAI API. The following example uses Ollama as the chat server, and llama3 as the llm model. ```bash ollama serve ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/OpenAIChatAgent-connect-to-third-party-api.md",
        "label": "autogen",
        "content": "Steps - Import the required namespaces: [!code-csharp[](../../sample/AutoGen.OpenAI.Sample/Connect_To_Ollama.cs?name=using_statement)]  - Create a `CustomHttpClientHandler` class.  The `CustomHttpClientHandler` class is used to customize the HttpClientHandler. In this example, we override the `SendAsync` method to redirect the request to local Ollama server, which is running on `http://localhost:11434`.  [!code-csharp[](../../sample/AutoGen.OpenAI.Sample/Connect_To_Ollama.cs?name=CustomHttpClientHandler)]  - Create an `OpenAIChatAgent` instance and connect to the third-party API.  Then create an @AutoGen.OpenAI.OpenAIChatAgent instance and connect to the OpenAI API from Ollama. You can customize the transport behavior of `OpenAIClient` by passing a customized `HttpClientTransport` instance. In the customized `HttpClientTransport` instance, we pass the `CustomHttpClientHandler` we just created which redirects all openai chat requests to the local Ollama server.  [!code-csharp[](../../sample/AutoGen.OpenAI.Sample/Connect_To_Ollama.cs?name=create_agent)]  - Chat with the `OpenAIChatAgent`. Finally, you can start chatting with the agent. In this example, we send a coding question to the agent and get the response.  [!code-csharp[](../../sample/AutoGen.OpenAI.Sample/Connect_To_Ollama.cs?name=send_message)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/OpenAIChatAgent-connect-to-third-party-api.md",
        "label": "autogen",
        "content": "Sample Output The following is the sample output of the code snippet above:  ![output](../images/articles/ConnectTo3PartyOpenAI/output.gif)"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/getting-start.md",
        "label": "autogen",
        "content": "### Get start with AutoGen for dotnet [![dotnet-ci](https://github.com/microsoft/autogen/actions/workflows/dotnet-build.yml/badge.svg)](https://github.com/microsoft/autogen/actions/workflows/dotnet-build.yml) [![Discord](https://img.shields.io/discord/1153072414184452236?logo=discord&style=flat)](https://discord.gg/pAbnFJrkgZ) [![NuGet version](https://badge.fury.io/nu/AutoGen.Core.svg)](https://badge.fury.io/nu/AutoGen.Core)  Firstly, add `AutoGen` package to your project.  ```bash dotnet add package AutoGen ```  > [!NOTE] > For more information about installing packages, please check out the [installation guide](Installation.md).  Then you can start with the following code snippet to create a conversable agent and chat with it.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/GetStartCodeSnippet.cs?name=snippet_GetStartCodeSnippet)] [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/GetStartCodeSnippet.cs?name=code_snippet_1)]  ### Tutorial Getting started with AutoGen.Net by following the [tutorial](../tutorial/Chat-with-an-agent.md) series. ### Examples You can find more examples under the [sample project](https://github.com/microsoft/autogen/tree/dotnet/dotnet/sample/AutoGen.BasicSamples).  ### Report a bug or request a feature You can report a bug or request a feature by creating a new issue in the [github issue](https://github.com/microsoft/autogen/issues) and specifying label the label \"donet\"  "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/AutoGen-OpenAI-Overview.md",
        "label": "autogen",
        "content": "## AutoGen.OpenAI Overview  AutoGen.OpenAI provides the following agents over openai models: - @AutoGen.OpenAI.OpenAIChatAgent: A slim wrapper agent over `OpenAIClient`. This agent only support `IMessage<ChatRequestMessage>` message type. To support more message types like @AutoGen.Core.TextMessage, register the agent with @AutoGen.OpenAI.OpenAIChatRequestMessageConnector. - @AutoGen.OpenAI.GPTAgent: An agent that build on top of @AutoGen.OpenAI.OpenAIChatAgent with more message types support like @AutoGen.Core.TextMessage, @AutoGen.Core.ImageMessage, @AutoGen.Core.MultiModalMessage and function call support. Essentially, it is equivalent to @AutoGen.OpenAI.OpenAIChatAgent with @AutoGen.Core.FunctionCallMiddleware and @AutoGen.OpenAI.OpenAIChatRequestMessageConnector registered.  ### Get start with AutoGen.OpenAI  To get start with AutoGen.OpenAI, firstly, follow the [installation guide](Installation.md) to make sure you add the AutoGen feed correctly. Then add `AutoGen.OpenAI` package to your project file.  ```xml <ItemGroup>     <PackageReference Include=\"AutoGen.OpenAI\" Version=\"AUTOGEN_VERSION\" /> </ItemGroup> ```   "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Create-type-safe-function-call.md",
        "label": "autogen",
        "content": "## Type-safe function call  `AutoGen` provides a source generator to easness the trouble of manually craft function definition and function call wrapper from a function. To use this feature, simply add the `AutoGen.SourceGenerator` package to your project and decorate your function with @AutoGen.Core.FunctionAttribute.  ```bash dotnet add package AutoGen.SourceGenerator ```  > [!NOTE] > It's recommended to enable structural xml document support by setting `GenerateDocumentationFile` property to true in your project file. This allows source generator to leverage the documentation of the function when generating the function definition.  ```xml <PropertyGroup>     <!-- This enables structural xml document support -->     <GenerateDocumentationFile>true</GenerateDocumentationFile> </PropertyGroup> ```  Then, create a `public partial` class to host the methods you want to use in AutoGen agents. The method has to be a `public` instance method and its return type must be `Task<string>`. After the methods is defined, mark them with @AutoGen.FunctionAttribute attribute:  > [!NOTE] > A `public partial` class is required for the source generator to generate code. > The method has to be a `public` instance method and its return type must be `Task<string>`. > Mark the method with @AutoGen.Core.FunctionAttribute attribute.  Firstly, import the required namespaces:  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/TypeSafeFunctionCallCodeSnippet.cs?name=weather_report_using_statement)]  Then, create a `WeatherReport` function and mark it with @AutoGen.Core.FunctionAttribute:  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/TypeSafeFunctionCallCodeSnippet.cs?name=weather_report)]  The source generator will generate the @AutoGen.Core.FunctionContract and function call wrapper for `WeatherReport` in another partial class based on its signature and structural comments. The @AutoGen.Core.FunctionContract is introduced by [#1736](https://github.com/microsoft/autogen/pull/1736) and contains all the necessary metadata such as function name, parameters, and return type. It is LLM independent and can be used to generate openai function definition or semantic kernel function. The function call wrapper is a helper class that provides a type-safe way to call the function.  > [!NOTE] > If you are using VSCode as your editor, you may need to restart the editor to see the generated code.  The following code shows how to generate openai function definition from the @AutoGen.Core.FunctionContract and call the function using the function call wrapper.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/TypeSafeFunctionCallCodeSnippet.cs?name=weather_report_consume)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Create-an-agent.md",
        "label": "autogen",
        "content": "## AssistantAgent  [`AssistantAgent`](../api/AutoGen.AssistantAgent.yml) is a built-in agent in `AutoGen` that acts as an AI assistant. It uses LLM to generate response to user input. It also supports function call if the underlying LLM model supports it (e.g. `gpt-3.5-turbo-0613`). "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Create-an-agent.md",
        "label": "autogen",
        "content": "Create an `AssistantAgent` using OpenAI model.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/CreateAnAgent.cs?name=code_snippet_1)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Create-an-agent.md",
        "label": "autogen",
        "content": "Create an `AssistantAgent` using Azure OpenAI model.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/CreateAnAgent.cs?name=code_snippet_2)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Print-message-middleware.md",
        "label": "autogen",
        "content": "@AutoGen.Core.PrintMessageMiddleware is a built-in @AutoGen.Core.IMiddleware that pretty print @AutoGen.Core.IMessage to console.  > [!NOTE] > @AutoGen.Core.PrintMessageMiddleware support the following @AutoGen.Core.IMessage types: > - @AutoGen.Core.TextMessage > - @AutoGen.Core.MultiModalMessage > - @AutoGen.Core.ToolCallMessage > - @AutoGen.Core.ToolCallResultMessage > - @AutoGen.Core.Message > - (streaming) @AutoGen.Core.TextMessageUpdate > - (streaming) @AutoGen.Core.ToolCallMessageUpdate "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Print-message-middleware.md",
        "label": "autogen",
        "content": "Use @AutoGen.Core.PrintMessageMiddleware in an agent You can use @AutoGen.Core.PrintMessageMiddlewareExtension.RegisterPrintMessage* to register the @AutoGen.Core.PrintMessageMiddleware to an agent.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/PrintMessageMiddlewareCodeSnippet.cs?name=PrintMessageMiddleware)]  @AutoGen.Core.PrintMessageMiddlewareExtension.RegisterPrintMessage* will format the message and print it to console ![image](../images/articles/PrintMessageMiddleware/printMessage.png) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/Print-message-middleware.md",
        "label": "autogen",
        "content": "Streaming message support  @AutoGen.Core.PrintMessageMiddleware also supports streaming message types like @AutoGen.Core.TextMessageUpdate and @AutoGen.Core.ToolCallMessageUpdate. If you register @AutoGen.Core.PrintMessageMiddleware to a @AutoGen.Core.IStreamingAgent, it will format the streaming message and print it to console if the message is of supported type.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/PrintMessageMiddlewareCodeSnippet.cs?name=print_message_streaming)]  ![image](../images/articles/PrintMessageMiddleware/streamingoutput.gif) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/MistralChatAgent-count-token-usage.md",
        "label": "autogen",
        "content": "The following example shows how to create a `MistralAITokenCounterMiddleware` @AutoGen.Core.IMiddleware and count the token usage when chatting with @AutoGen.Mistral.MistralClientAgent.  ### Overview To collect the token usage for the entire chat session, one easy solution is simply collect all the responses from agent and sum up the token usage for each response. To collect all the agent responses, we can create a middleware which simply saves all responses to a list and register it with the agent. To get the token usage information for each response, because in the example we are using @AutoGen.Mistral.MistralClientAgent, we can simply get the token usage from the response object.  > [!NOTE] > You can find the complete example in the [Example13_OpenAIAgent_JsonMode](https://github.com/microsoft/autogen/tree/main/dotnet/sample/AutoGen.BasicSamples/Example14_MistralClientAgent_TokenCount.cs).  - Step 1: Adding using statement [!code-csharp[](../../sample/AutoGen.BasicSamples/Example14_MistralClientAgent_TokenCount.cs?name=using_statements)]  - Step 2: Create a `MistralAITokenCounterMiddleware` class which implements @AutoGen.Core.IMiddleware. This middleware will collect all the responses from the agent and sum up the token usage for each response. [!code-csharp[](../../sample/AutoGen.BasicSamples/Example14_MistralClientAgent_TokenCount.cs?name=token_counter_middleware)]  - Step 3: Create a `MistralClientAgent` [!code-csharp[](../../sample/AutoGen.BasicSamples/Example14_MistralClientAgent_TokenCount.cs?name=create_mistral_client_agent)]  - Step 4: Register the `MistralAITokenCounterMiddleware` with the `MistralClientAgent`. Note that the order of each middlewares matters. The token counter middleware needs to be registered before `mistralMessageConnector` because it collects response only when the responding message type is `IMessage<ChatCompletionResponse>` while the `mistralMessageConnector` will convert `IMessage<ChatCompletionResponse>` to one of @AutoGen.Core.TextMessage, @AutoGen.Core.ToolCallMessage or @AutoGen.Core.ToolCallResultMessage. [!code-csharp[](../../sample/AutoGen.BasicSamples/Example14_MistralClientAgent_TokenCount.cs?name=register_middleware)]  - Step 5: Chat with the `MistralClientAgent` and get the token usage information from the response object. [!code-csharp[](../../sample/AutoGen.BasicSamples/Example14_MistralClientAgent_TokenCount.cs?name=chat_with_agent)]  ### Output When running the example, the completion token count will be printed to the console. ```bash Completion token count: 1408 # might be different based on the response ```"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/OpenAIChatAgent-simple-chat.md",
        "label": "autogen",
        "content": "The following example shows how to create an @AutoGen.OpenAI.OpenAIChatAgent and chat with it.  Firsly, import the required namespaces: [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/OpenAICodeSnippet.cs?name=using_statement)]  Then, create an @AutoGen.OpenAI.OpenAIChatAgent and chat with it: [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/OpenAICodeSnippet.cs?name=create_openai_chat_agent)]  @AutoGen.OpenAI.OpenAIChatAgent also supports streaming chat via @AutoGen.Core.IAgent.GenerateStreamingReplyAsync*.  [!code-csharp[](../../sample/AutoGen.BasicSamples/CodeSnippet/OpenAICodeSnippet.cs?name=create_openai_chat_agent_streaming)]"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/AutoGen.Ollama/Chat-with-llava.md",
        "label": "autogen",
        "content": "This sample shows how to use @AutoGen.Ollama.OllamaAgent to chat with LLaVA model.  To run this example, you need to have an Ollama server running aside and have `llava:latest` model installed. For how to setup an Ollama server, please refer to [Ollama](https://ollama.com/).  > [!NOTE] > You can find the complete sample code [here](https://github.com/microsoft/autogen/blob/main/dotnet/sample/AutoGen.Ollama.Sample/Chat_With_LLaVA.cs)  ### Step 1: Install AutoGen.Ollama  First, install the AutoGen.Ollama package using the following command:  ```bash dotnet add package AutoGen.Ollama ```  For how to install from nightly build, please refer to [Installation](../Installation.md).  ### Step 2: Add using statement  [!code-csharp[](../../../sample/AutoGen.Ollama.Sample/Chat_With_LLaVA.cs?name=Using)]  ### Step 3: Create @AutoGen.Ollama.OllamaAgent  [!code-csharp[](../../../sample/AutoGen.Ollama.Sample/Chat_With_LLaVA.cs?name=Create_Ollama_Agent)]  ### Step 4: Start MultiModal Chat LLaVA is a multimodal model that supports both text and image inputs. In this step, we create an image message along with a question about the image.  [!code-csharp[](../../../sample/AutoGen.Ollama.Sample/Chat_With_LLaVA.cs?name=Send_Message)]"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/AutoGen.Ollama/Chat-with-llama.md",
        "label": "autogen",
        "content": "This example shows how to use @AutoGen.Ollama.OllamaAgent to connect to Ollama server and chat with LLaVA model.  To run this example, you need to have an Ollama server running aside and have `llama3:latest` model installed. For how to setup an Ollama server, please refer to [Ollama](https://ollama.com/).  > [!NOTE] > You can find the complete sample code [here](https://github.com/microsoft/autogen/blob/main/dotnet/sample/AutoGen.Ollama.Sample/Chat_With_LLaMA.cs)  ### Step 1: Install AutoGen.Ollama  First, install the AutoGen.Ollama package using the following command:  ```bash dotnet add package AutoGen.Ollama ```  For how to install from nightly build, please refer to [Installation](../Installation.md).  ### Step 2: Add using statement  [!code-csharp[](../../../sample/AutoGen.Ollama.Sample/Chat_With_LLaMA.cs?name=Using)]  ### Step 3: Create and chat @AutoGen.Ollama.OllamaAgent  In this step, we create an @AutoGen.Ollama.OllamaAgent and connect it to the Ollama server.  [!code-csharp[](../../../sample/AutoGen.Ollama.Sample/Chat_With_LLaMA.cs?name=Create_Ollama_Agent)]  "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/AutoGen.SemanticKernel/Use-kernel-plugin-in-other-agents.md",
        "label": "autogen",
        "content": "In semantic kernel, a kernel plugin is a collection of kernel functions that can be invoked during LLM calls. Semantic kernel provides a list of built-in plugins, like [core plugins](https://github.com/microsoft/semantic-kernel/tree/main/dotnet/src/Plugins/Plugins.Core), [web search plugin](https://github.com/microsoft/semantic-kernel/tree/main/dotnet/src/Plugins/Plugins.Web) and many more. You can also create your own plugins and use them in semantic kernel. Kernel plugins greatly extend the capabilities of semantic kernel and can be used to perform various tasks like web search, image search, text summarization, etc.  `AutoGen.SemanticKernel` provides a middleware called @AutoGen.SemanticKernel.KernelPluginMiddleware that allows you to use semantic kernel plugins in other AutoGen agents like @AutoGen.OpenAI.OpenAIChatAgent. The following example shows how to define a simple plugin with a single `GetWeather` function and use it in @AutoGen.OpenAI.OpenAIChatAgent.  > [!NOTE] > You can find the complete sample code [here](https://github.com/microsoft/autogen/blob/main/dotnet/sample/AutoGen.SemanticKernel.Sample/Use_Kernel_Functions_With_Other_Agent.cs)  ### Step 1: add using statement [!code-csharp[](../../../sample/AutoGen.SemanticKernel.Sample/Use_Kernel_Functions_With_Other_Agent.cs?name=Using)]  ### Step 2: create plugin  In this step, we create a simple plugin with a single `GetWeather` function that takes a location as input and returns the weather information for that location.  [!code-csharp[](../../../sample/AutoGen.SemanticKernel.Sample/Use_Kernel_Functions_With_Other_Agent.cs?name=Create_plugin)]  ### Step 3: create OpenAIChatAgent and use the plugin  In this step, we firstly create a @AutoGen.SemanticKernel.KernelPluginMiddleware and register the previous plugin with it. The `KernelPluginMiddleware` will load the plugin and make the functions available for use in other agents. Followed by creating an @AutoGen.OpenAI.OpenAIChatAgent and register it with the `KernelPluginMiddleware`.  [!code-csharp[](../../../sample/AutoGen.SemanticKernel.Sample/Use_Kernel_Functions_With_Other_Agent.cs?name=Use_plugin)]  ### Step 4: chat with OpenAIChatAgent  In this final step, we start the chat with the @AutoGen.OpenAI.OpenAIChatAgent by asking the weather in Seattle. The `OpenAIChatAgent` will use the `GetWeather` function from the plugin to get the weather information for Seattle.  [!code-csharp[](../../../sample/AutoGen.SemanticKernel.Sample/Use_Kernel_Functions_With_Other_Agent.cs?name=Send_message)]"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/AutoGen.SemanticKernel/SemanticKernelChatAgent-simple-chat.md",
        "label": "autogen",
        "content": "`AutoGen.SemanticKernel` provides built-in support for `ChatCompletionAgent` via @AutoGen.SemanticKernel.SemanticKernelChatCompletionAgent. By default the @AutoGen.SemanticKernel.SemanticKernelChatCompletionAgent only supports the original `ChatMessageContent` type via `IMessage<ChatMessageContent>`. To support more AutoGen built-in message types like @AutoGen.Core.TextMessage, @AutoGen.Core.ImageMessage, @AutoGen.Core.MultiModalMessage, you can register the agent with @AutoGen.SemanticKernel.SemanticKernelChatMessageContentConnector. The @AutoGen.SemanticKernel.SemanticKernelChatMessageContentConnector will convert the message from AutoGen built-in message types to `ChatMessageContent` and vice versa.  The following step-by-step example shows how to create an @AutoGen.SemanticKernel.SemanticKernelChatCompletionAgent and chat with it:  > [!NOTE] > You can find the complete sample code [here](https://github.com/microsoft/autogen/blob/main/dotnet/sample/AutoGen.SemanticKernel.Sample/Create_Semantic_Kernel_Chat_Agent.cs).  ### Step 1: add using statement [!code-csharp[](../../../sample/AutoGen.SemanticKernel.Sample/Create_Semantic_Kernel_Chat_Agent.cs?name=Using)]  ### Step 2: create kernel [!code-csharp[](../../../sample/AutoGen.SemanticKernel.Sample/Create_Semantic_Kernel_Chat_Agent.cs?name=Create_Kernel)]  ### Step 3: create ChatCompletionAgent [!code-csharp[](../../../sample/AutoGen.SemanticKernel.Sample/Create_Semantic_Kernel_Chat_Agent.cs?name=Create_ChatCompletionAgent)]  ### Step 4: create @AutoGen.SemanticKernel.SemanticKernelChatCompletionAgent In this step, we create an @AutoGen.SemanticKernel.SemanticKernelChatCompletionAgent and register it with @AutoGen.SemanticKernel.SemanticKernelChatMessageContentConnector. The @AutoGen.SemanticKernel.SemanticKernelChatMessageContentConnector will convert the message from AutoGen built-in message types to `ChatMessageContent` and vice versa. [!code-csharp[](../../../sample/AutoGen.SemanticKernel.Sample/Create_Semantic_Kernel_Chat_Agent.cs?name=Create_SemanticKernelChatCompletionAgent)]  ### Step 5: chat with @AutoGen.SemanticKernel.SemanticKernelChatCompletionAgent [!code-csharp[](../../../sample/AutoGen.SemanticKernel.Sample/Create_Semantic_Kernel_Chat_Agent.cs?name=Send_Message)]"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/AutoGen.SemanticKernel/SemanticKernelAgent-simple-chat.md",
        "label": "autogen",
        "content": "You can chat with @AutoGen.SemanticKernel.SemanticKernelAgent using both streaming and non-streaming methods and use native `ChatMessageContent` type via `IMessage<ChatMessageContent>`.  The following example shows how to create an @AutoGen.SemanticKernel.SemanticKernelAgent and chat with it using non-streaming method:  [!code-csharp[](../../../sample/AutoGen.BasicSamples/CodeSnippet/SemanticKernelCodeSnippet.cs?name=create_semantic_kernel_agent)]  @AutoGen.SemanticKernel.SemanticKernelAgent also supports streaming chat via @AutoGen.Core.IStreamingAgent.GenerateStreamingReplyAsync*.  [!code-csharp[](../../../sample/AutoGen.BasicSamples/CodeSnippet/SemanticKernelCodeSnippet.cs?name=create_semantic_kernel_agent_streaming)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/AutoGen.SemanticKernel/AutoGen-SemanticKernel-Overview.md",
        "label": "autogen",
        "content": "## AutoGen.SemanticKernel Overview  AutoGen.SemanticKernel is a package that provides seamless integration with Semantic Kernel. It provides the following agents: - @AutoGen.SemanticKernel.SemanticKernelAgent: A slim wrapper agent over `Kernel` that only support original `ChatMessageContent` type via `IMessage<ChatMessageContent>`. To support more AutoGen built-in message type, register the agent with @AutoGen.SemanticKernel.SemanticKernelChatMessageContentConnector. - @AutoGen.SemanticKernel.SemanticKernelChatCompletionAgent: A slim wrapper agent over `Microsoft.SemanticKernel.Agents.ChatCompletionAgent`.  AutoGen.SemanticKernel also provides the following middleware: - @AutoGen.SemanticKernel.SemanticKernelChatMessageContentConnector: A connector that convert the message from AutoGen built-in message types to `ChatMessageContent` and vice versa. At the current stage, it only supports conversation between @AutoGen.Core.TextMessage, @AutoGen.Core.ImageMessage and @AutoGen.Core.MultiModalMessage. Function call message type like @AutoGen.Core.ToolCallMessage and @AutoGen.Core.ToolCallResultMessage are not supported yet. - @AutoGen.SemanticKernel.KernelPluginMiddleware: A middleware that allows you to use semantic kernel plugins in other AutoGen agents like @AutoGen.OpenAI.OpenAIChatAgent.  ### Get start with AutoGen.SemanticKernel  To get start with AutoGen.SemanticKernel, firstly, follow the [installation guide](../Installation.md) to make sure you add the AutoGen feed correctly. Then add `AutoGen.SemanticKernel` package to your project file.  ```xml <ItemGroup>     <PackageReference Include=\"AutoGen.SemanticKernel\" Version=\"AUTOGEN_VERSION\" /> </ItemGroup> ```"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/AutoGen.SemanticKernel/SemanticKernelAgent-support-more-messages.md",
        "label": "autogen",
        "content": "@AutoGen.SemanticKernel.SemanticKernelAgent only supports the original `ChatMessageContent` type via `IMessage<ChatMessageContent>`. To support more AutoGen built-in message types like @AutoGen.Core.TextMessage, @AutoGen.Core.ImageMessage, @AutoGen.Core.MultiModalMessage, you can register the agent with @AutoGen.SemanticKernel.SemanticKernelChatMessageContentConnector. The @AutoGen.SemanticKernel.SemanticKernelChatMessageContentConnector will convert the message from AutoGen built-in message types to `ChatMessageContent` and vice versa. > [!NOTE] > At the current stage, @AutoGen.SemanticKernel.SemanticKernelChatMessageContentConnector only supports conversation for the followng built-in @AutoGen.Core.IMessage > - @AutoGen.Core.TextMessage > - @AutoGen.Core.ImageMessage > - @AutoGen.Core.MultiModalMessage > > Function call message type like @AutoGen.Core.ToolCallMessage and @AutoGen.Core.ToolCallResultMessage are not supported yet.  [!code-csharp[](../../../sample/AutoGen.BasicSamples/CodeSnippet/SemanticKernelCodeSnippet.cs?name=register_semantic_kernel_chat_message_content_connector)]"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/AutoGen.Gemini/Overview.md",
        "label": "autogen",
        "content": "# AutoGen.Gemini Overview  AutoGen.Gemini is a package that provides seamless integration with Google Gemini. It provides the following agent:  - @AutoGen.Gemini.GeminiChatAgent: The agent that connects to Google Gemini or Vertex AI Gemini. It supports chat, multi-modal chat, and function call.  AutoGen.Gemini also provides the following middleware: - @AutoGen.Gemini.GeminiMessageConnector: The middleware that converts the Gemini message to AutoGen built-in message type. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/AutoGen.Gemini/Overview.md",
        "label": "autogen",
        "content": "Examples  You can find more examples under the [gemini sample project](https://github.com/microsoft/autogen/tree/main/dotnet/sample/AutoGen.Gemini.Sample)"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/AutoGen.Gemini/Chat-with-google-gemini.md",
        "label": "autogen",
        "content": "This example shows how to use @AutoGen.Gemini.GeminiChatAgent to connect to Google AI Gemini and chat with Gemini model.  To run this example, you need to have a Google AI Gemini API key. For how to get a Google Gemini API key, please refer to [Google Gemini](https://gemini.google.com/).  > [!NOTE] > You can find the complete sample code [here](https://github.com/microsoft/autogen/blob/main/dotnet/sample/AutoGen.Gemini.Sample/Chat_With_Google_Gemini.cs)  > [!NOTE] > What's the difference between Google AI Gemini and Vertex AI Gemini? > > Gemini is a series of large language models developed by Google. You can use it either from Google AI API or Vertex AI API. If you are relatively new to Gemini and wants to explore the feature and build some prototype for your chatbot app, Google AI APIs (with Google AI Studio) is a fast way to get started. While your app and idea matures and you'd like to leverage more MLOps tools that streamline the usage, deployment, and monitoring of models, you can move to Google Cloud Vertex AI which provides Gemini APIs along with many other features. Basically, to help you productionize your app. ([reference](https://stackoverflow.com/questions/78007243/utilizing-gemini-through-vertex-ai-or-through-google-generative-ai))  ### Step 1: Install AutoGen.Gemini  First, install the AutoGen.Gemini package using the following command:  ```bash dotnet add package AutoGen.Gemini ```  ### Step 2: Add using statement  [!code-csharp[](../../../sample/AutoGen.Gemini.Sample/Chat_With_Google_Gemini.cs?name=Using)]  ### Step 3: Create a Gemini agent  [!code-csharp[](../../../sample/AutoGen.Gemini.Sample/Chat_With_Google_Gemini.cs?name=Create_Gemini_Agent)]  ### Step 4: Chat with Gemini  [!code-csharp[](../../../sample/AutoGen.Gemini.Sample/Chat_With_Google_Gemini.cs?name=Chat_With_Google_Gemini)]"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/AutoGen.Gemini/Image-chat-with-gemini.md",
        "label": "autogen",
        "content": "This example shows how to use @AutoGen.Gemini.GeminiChatAgent for image chat with Gemini model.  To run this example, you need to have a project on Google Cloud with access to Vertex AI API. For more information please refer to [Google Vertex AI](https://cloud.google.com/vertex-ai/docs).   > [!NOTE] > You can find the complete sample code [here](https://github.com/microsoft/autogen/blob/main/dotnet/sample/AutoGen.Gemini.Sample/Image_Chat_With_Vertex_Gemini.cs)  ### Step 1: Install AutoGen.Gemini  First, install the AutoGen.Gemini package using the following command:  ```bash dotnet add package AutoGen.Gemini ```  ### Step 2: Add using statement [!code-csharp[](../../../sample/AutoGen.Gemini.Sample/Image_Chat_With_Vertex_Gemini.cs?name=Using)]  ### Step 3: Create a Gemini agent  [!code-csharp[](../../../sample/AutoGen.Gemini.Sample/Image_Chat_With_Vertex_Gemini.cs?name=Create_Gemini_Agent)]  ### Step 4: Send image to Gemini [!code-csharp[](../../../sample/AutoGen.Gemini.Sample/Image_Chat_With_Vertex_Gemini.cs?name=Send_Image_Request)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/AutoGen.Gemini/Function-call-with-gemini.md",
        "label": "autogen",
        "content": "This example shows how to use @AutoGen.Gemini.GeminiChatAgent to make function call. This example is modified from [gemini-api function call example](https://ai.google.dev/gemini-api/docs/function-calling)  To run this example, you need to have a project on Google Cloud with access to Vertex AI API. For more information please refer to [Google Vertex AI](https://cloud.google.com/vertex-ai/docs).   > [!NOTE] > You can find the complete sample code [here](https://github.com/microsoft/autogen/blob/main/dotnet/sample/AutoGen.Gemini.Sample/Function_Call_With_Gemini.cs)  ### Step 1: Install AutoGen.Gemini and AutoGen.SourceGenerator  First, install the AutoGen.Gemini package using the following command:  ```bash dotnet add package AutoGen.Gemini dotnet add package AutoGen.SourceGenerator ```  The AutoGen.SourceGenerator package is required to generate the @AutoGen.Core.FunctionContract. For more information, please refer to [Create-type-safe-function-call](../Create-type-safe-function-call.md)  ### Step 2: Add using statement [!code-csharp[](../../../sample/AutoGen.Gemini.Sample/Function_call_with_gemini.cs?name=Using)]  ### Step 3: Create `MovieFunction`  [!code-csharp[](../../../sample/AutoGen.Gemini.Sample/Function_call_with_gemini.cs?name=MovieFunction)]  ### Step 4: Create a Gemini agent  [!code-csharp[](../../../sample/AutoGen.Gemini.Sample/Function_call_with_gemini.cs?name=Create_Gemini_Agent)]  ### Step 5: Single turn function call  [!code-csharp[](../../../sample/AutoGen.Gemini.Sample/Function_call_with_gemini.cs?name=Single_turn)]  ### Step 6: Multi-turn function call  [!code-csharp[](../../../sample/AutoGen.Gemini.Sample/Function_call_with_gemini.cs?name=Multi_turn)]  "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/articles/AutoGen.Gemini/Chat-with-vertex-gemini.md",
        "label": "autogen",
        "content": "This example shows how to use @AutoGen.Gemini.GeminiChatAgent to connect to Vertex AI Gemini API and chat with Gemini model.  To run this example, you need to have a project on Google Cloud with access to Vertex AI API. For more information please refer to [Google Vertex AI](https://cloud.google.com/vertex-ai/docs).  > [!NOTE] > You can find the complete sample code [here](https://github.com/microsoft/autogen/blob/main/dotnet/sample/AutoGen.Gemini.Sample/Chat_With_Vertex_Gemini.cs)  > [!NOTE] > What's the difference between Google AI Gemini and Vertex AI Gemini? > > Gemini is a series of large language models developed by Google. You can use it either from Google AI API or Vertex AI API. If you are relatively new to Gemini and wants to explore the feature and build some prototype for your chatbot app, Google AI APIs (with Google AI Studio) is a fast way to get started. While your app and idea matures and you'd like to leverage more MLOps tools that streamline the usage, deployment, and monitoring of models, you can move to Google Cloud Vertex AI which provides Gemini APIs along with many other features. Basically, to help you productionize your app. ([reference](https://stackoverflow.com/questions/78007243/utilizing-gemini-through-vertex-ai-or-through-google-generative-ai))  ### Step 1: Install AutoGen.Gemini  First, install the AutoGen.Gemini package using the following command:  ```bash dotnet add package AutoGen.Gemini ```  ### Step 2: Add using statement  [!code-csharp[](../../../sample/AutoGen.Gemini.Sample/Chat_With_Vertex_Gemini.cs?name=Using)]  ### Step 3: Create a Gemini agent  [!code-csharp[](../../../sample/AutoGen.Gemini.Sample/Chat_With_Vertex_Gemini.cs?name=Create_Gemini_Agent)]   ### Step 4: Chat with Gemini  [!code-csharp[](../../../sample/AutoGen.Gemini.Sample/Chat_With_Vertex_Gemini.cs?name=Chat_With_Vertex_Gemini)]"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Use-AutoGen.Net-agent-as-model-in-AG-Studio.md",
        "label": "autogen",
        "content": "This tutorial shows how to use AutoGen.Net agent as model in AG Studio "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Use-AutoGen.Net-agent-as-model-in-AG-Studio.md",
        "label": "autogen",
        "content": "Step 1. Create Dotnet empty web app and install AutoGen and AutoGen.WebAPI package  ```bash dotnet new web dotnet add package AutoGen dotnet add package AutoGen.WebAPI ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Use-AutoGen.Net-agent-as-model-in-AG-Studio.md",
        "label": "autogen",
        "content": "Step 2. Replace the Program.cs with following code  ```bash using AutoGen.Core; using AutoGen.Service;  var builder = WebApplication.CreateBuilder(args); var app = builder.Build();  var helloWorldAgent = new HelloWorldAgent(); app.UseAgentAsOpenAIChatCompletionEndpoint(helloWorldAgent);  app.Run();  class HelloWorldAgent : IAgent {     public string Name => \"HelloWorld\";      public Task<IMessage> GenerateReplyAsync(IEnumerable<IMessage> messages, GenerateReplyOptions? options = null, CancellationToken cancellationToken = default)     {         return Task.FromResult<IMessage>(new TextMessage(Role.Assistant, \"Hello World!\", from: this.Name));     } } ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Use-AutoGen.Net-agent-as-model-in-AG-Studio.md",
        "label": "autogen",
        "content": "Step 3: Start the web app  Run the following command to start web api  ```bash dotnet RUN ```  The web api will listen at `http://localhost:5264/v1/chat/completion  ![terminal](../images/articles/UseAutoGenAsModelinAGStudio/Terminal.png) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Use-AutoGen.Net-agent-as-model-in-AG-Studio.md",
        "label": "autogen",
        "content": "Step 4: In another terminal, start autogen-studio  ```bash autogenstudio ui ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Use-AutoGen.Net-agent-as-model-in-AG-Studio.md",
        "label": "autogen",
        "content": "Step 5: Navigate to AutoGen Studio UI and add hello world agent as openai Model  ### Step 5.1: Go to model tab  ![The Model Tab](../images/articles/UseAutoGenAsModelinAGStudio/TheModelTab.png)  ### Step 5.2: Select \"OpenAI model\" card  ![Open AI model Card](../images/articles/UseAutoGenAsModelinAGStudio/Step5.2OpenAIModel.png)  ### Step 5.3: Fill the model name and url  The model name needs to be same with agent name  ![Fill the model name and url](../images/articles/UseAutoGenAsModelinAGStudio/Step5.3ModelNameAndURL.png) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Use-AutoGen.Net-agent-as-model-in-AG-Studio.md",
        "label": "autogen",
        "content": "Step 6: Create a hello world agent that uses the hello world model  ![Create a hello world agent that uses the hello world model](../images/articles/UseAutoGenAsModelinAGStudio/Step6.png)  ![Agent Configuration](../images/articles/UseAutoGenAsModelinAGStudio/Step6b.png) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Use-AutoGen.Net-agent-as-model-in-AG-Studio.md",
        "label": "autogen",
        "content": "Final Step: Use the hello world agent in workflow  ![Use the hello world agent in workflow](../images/articles/UseAutoGenAsModelinAGStudio/FinalStepsA.png)  ![Use the hello world agent in workflow](../images/articles/UseAutoGenAsModelinAGStudio/FinalStepsA.png)  ![Use the hello world agent in workflow](../images/articles/UseAutoGenAsModelinAGStudio/FinalStepsB.png)  ![Use the hello world agent in workflow](../images/articles/UseAutoGenAsModelinAGStudio/FinalStepsC.png) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Image-chat-with-agent.md",
        "label": "autogen",
        "content": "This tutorial shows how to perform image chat with an agent using the @AutoGen.OpenAI.OpenAIChatAgent as an example.  > [!NOTE] > To chat image with an agent, the model behind the agent needs to support image input. Here is a partial list of models that support image input: > - gpt-4o > - gemini-1.5 > - llava > - claude-3 > - ... > > In this example, we are using the gpt-4o model as the backend model for the agent.  > [!NOTE] > The complete code example can be found in [Image_Chat_With_Agent.cs](https://github.com/microsoft/autogen/blob/main/dotnet/sample/AutoGen.BasicSamples/GettingStart/Image_Chat_With_Agent.cs) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Image-chat-with-agent.md",
        "label": "autogen",
        "content": "Step 1: Install AutoGen  First, install the AutoGen package using the following command:  ```bash dotnet add package AutoGen ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Image-chat-with-agent.md",
        "label": "autogen",
        "content": "Step 2: Add Using Statements  [!code-csharp[Using Statements](../../sample/AutoGen.BasicSamples/GettingStart/Image_Chat_With_Agent.cs?name=Using)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Image-chat-with-agent.md",
        "label": "autogen",
        "content": "Step 3: Create an @AutoGen.OpenAI.OpenAIChatAgent  [!code-csharp[Create an OpenAIChatAgent](../../sample/AutoGen.BasicSamples/GettingStart/Image_Chat_With_Agent.cs?name=Create_Agent)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Image-chat-with-agent.md",
        "label": "autogen",
        "content": "Step 4: Prepare Image Message  In AutoGen, you can create an image message using either @AutoGen.Core.ImageMessage or @AutoGen.Core.MultiModalMessage. The @AutoGen.Core.ImageMessage takes a single image as input, whereas the @AutoGen.Core.MultiModalMessage allows you to pass multiple modalities like text or image.  Here is how to create an image message using @AutoGen.Core.ImageMessage: [!code-csharp[Create Image Message](../../sample/AutoGen.BasicSamples/GettingStart/Image_Chat_With_Agent.cs?name=Prepare_Image_Input)]  Here is how to create a multimodal message using @AutoGen.Core.MultiModalMessage: [!code-csharp[Create MultiModal Message](../../sample/AutoGen.BasicSamples/GettingStart/Image_Chat_With_Agent.cs?name=Prepare_Multimodal_Input)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Image-chat-with-agent.md",
        "label": "autogen",
        "content": "Step 5: Generate Response  To generate response, you can use one of the overloaded methods of @AutoGen.Core.AgentExtension.SendAsync* method. The following code shows how to generate response with an image message:  [!code-csharp[Generate Response](../../sample/AutoGen.BasicSamples/GettingStart/Image_Chat_With_Agent.cs?name=Chat_With_Agent)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Image-chat-with-agent.md",
        "label": "autogen",
        "content": "Further Reading - [Image chat with gemini](../articles/AutoGen.Gemini/Image-chat-with-gemini.md) - [Image chat with llava](../articles/AutoGen.Ollama/Chat-with-llava.md)"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Create-agent-with-tools.md",
        "label": "autogen",
        "content": "This tutorial shows how to use tools in an agent. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Create-agent-with-tools.md",
        "label": "autogen",
        "content": "What is tool Tools are pre-defined functions in user's project that agent can invoke. Agent can use tools to perform actions like search web, perform calculations, etc. With tools, it can greatly extend the capabilities of an agent.  > [!NOTE] > To use tools with agent, the backend LLM model used by the agent needs to support tool calling. Here are some of the LLM models that support tool calling as of 06/21/2024 > - GPT-3.5-turbo with version >= 0613 > - GPT-4 series > - Gemini series > - OPEN_MISTRAL_7B > - ... > > This tutorial uses the latest `GPT-3.5-turbo` as example.  > [!NOTE] > The complete code example can be found in [Use_Tools_With_Agent.cs](https://github.com/microsoft/autogen/blob/main/dotnet/sample/AutoGen.BasicSamples/GettingStart/Use_Tools_With_Agent.cs) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Create-agent-with-tools.md",
        "label": "autogen",
        "content": "Key Concepts - @AutoGen.Core.FunctionContract: The contract of a function that agent can invoke. It contains the function name, description, parameters schema, and return type. - @AutoGen.Core.ToolCallMessage: A message type that represents a tool call request in AutoGen.Net. - @AutoGen.Core.ToolCallResultMessage: A message type that represents a tool call result in AutoGen.Net. - @AutoGen.Core.ToolCallAggregateMessage: An aggregate message type that represents a tool call request and its result in a single message in AutoGen.Net. - @AutoGen.Core.FunctionCallMiddleware: A middleware that pass the @AutoGen.Core.FunctionContract to the agent when generating response, and process the tool call response when receiving a @AutoGen.Core.ToolCallMessage.  > [!Tip] > You can Use AutoGen.SourceGenerator to automatically generate type-safe @AutoGen.Core.FunctionContract instead of manually defining them. For more information, please check out [Create type-safe function](../articles/Create-type-safe-function-call.md). "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Create-agent-with-tools.md",
        "label": "autogen",
        "content": "Install AutoGen and AutoGen.SourceGenerator First, install the AutoGen and AutoGen.SourceGenerator package using the following command:  ```bash dotnet add package AutoGen dotnet add package AutoGen.SourceGenerator ```  Also, you might need to enable structural xml document support by setting `GenerateDocumentationFile` property to true in your project file. This allows source generator to leverage the documentation of the function when generating the function definition.  ```xml <PropertyGroup>     <!-- This enables structural xml document support -->     <GenerateDocumentationFile>true</GenerateDocumentationFile> </PropertyGroup> ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Create-agent-with-tools.md",
        "label": "autogen",
        "content": "Add Using Statements  [!code-csharp[Using Statements](../../sample/AutoGen.BasicSamples/GettingStart/Use_Tools_With_Agent.cs?name=Using)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Create-agent-with-tools.md",
        "label": "autogen",
        "content": "Create agent  Create an @AutoGen.OpenAI.OpenAIChatAgent with `GPT-3.5-turbo` as the backend LLM model.  [!code-csharp[Create an agent with tools](../../sample/AutoGen.BasicSamples/GettingStart/Use_Tools_With_Agent.cs?name=Create_Agent)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Create-agent-with-tools.md",
        "label": "autogen",
        "content": "Define `Tool` class and create tools Create a `public partial` class to host the tools you want to use in AutoGen agents. The method has to be a `public` instance method and its return type must be `Task<string>`. After the methods is defined, mark them with @AutoGen.Core.FunctionAttribute attribute.  In the following example, we define a `GetWeather` tool that returns the weather information of a city.  [!code-csharp[Define Tool class](../../sample/AutoGen.BasicSamples/GettingStart/Use_Tools_With_Agent.cs?name=Tools)] [!code-csharp[Create tools](../../sample/AutoGen.BasicSamples/GettingStart/Use_Tools_With_Agent.cs?name=Create_tools)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Create-agent-with-tools.md",
        "label": "autogen",
        "content": "Tool call without auto-invoke In this case, when receiving a @AutoGen.Core.ToolCallMessage, the agent will not automatically invoke the tool. Instead, the agent will return the original message back to the user. The user can then decide whether to invoke the tool or not.  ![single-turn tool call without auto-invoke](../images/articles/CreateAgentWithTools/single-turn-tool-call-without-auto-invoke.png)  To implement this, you can create the @AutoGen.Core.FunctionCallMiddleware without passing the `functionMap` parameter to the constructor so that the middleware will not automatically invoke the tool once it receives a @AutoGen.Core.ToolCallMessage from its inner agent.  [!code-csharp[Single-turn tool call without auto-invoke](../../sample/AutoGen.BasicSamples/GettingStart/Use_Tools_With_Agent.cs?name=Create_no_invoke_middleware)]  After creating the function call middleware, you can register it to the agent using `RegisterMiddleware` method, which will return a new agent which can use the methods defined in the `Tool` class.  [!code-csharp[Generate Response](../../sample/AutoGen.BasicSamples/GettingStart/Use_Tools_With_Agent.cs?name=Single_Turn_No_Invoke)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Create-agent-with-tools.md",
        "label": "autogen",
        "content": "Tool call with auto-invoke In this case, the agent will automatically invoke the tool when receiving a @AutoGen.Core.ToolCallMessage and return the @AutoGen.Core.ToolCallAggregateMessage which contains both the tool call request and the tool call result.  ![single-turn tool call with auto-invoke](../images/articles/CreateAgentWithTools/single-turn-tool-call-with-auto-invoke.png)  To implement this, you can create the @AutoGen.Core.FunctionCallMiddleware with the `functionMap` parameter so that the middleware will automatically invoke the tool once it receives a @AutoGen.Core.ToolCallMessage from its inner agent.  [!code-csharp[Single-turn tool call with auto-invoke](../../sample/AutoGen.BasicSamples/GettingStart/Use_Tools_With_Agent.cs?name=Create_auto_invoke_middleware)]  After creating the function call middleware, you can register it to the agent using `RegisterMiddleware` method, which will return a new agent which can use the methods defined in the `Tool` class.  [!code-csharp[Generate Response](../../sample/AutoGen.BasicSamples/GettingStart/Use_Tools_With_Agent.cs?name=Single_Turn_Auto_Invoke)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Create-agent-with-tools.md",
        "label": "autogen",
        "content": "Send the tool call result back to LLM to generate further response In some cases, you may want to send the tool call result back to the LLM to generate further response. To do this, you can send the tool call response from agent back to the LLM by calling the `SendAsync` method of the agent.  [!code-csharp[Generate Response](../../sample/AutoGen.BasicSamples/GettingStart/Use_Tools_With_Agent.cs?name=Multi_Turn_Tool_Call)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Create-agent-with-tools.md",
        "label": "autogen",
        "content": "Parallel tool call Some LLM models support parallel tool call, which returns multiple tool calls in one single message. Note that @AutoGen.Core.FunctionCallMiddleware has already handled the parallel tool call for you. When it receives a @AutoGen.Core.ToolCallMessage that contains multiple tool calls, it will automatically invoke all the tools in the sequantial order and return the @AutoGen.Core.ToolCallAggregateMessage which contains all the tool call requests and results.  [!code-csharp[Generate Response](../../sample/AutoGen.BasicSamples/GettingStart/Use_Tools_With_Agent.cs?name=parallel_tool_call)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Create-agent-with-tools.md",
        "label": "autogen",
        "content": "Further Reading - [Function call with openai](../articles/OpenAIChatAgent-use-function-call.md) - [Function call with gemini](../articles/AutoGen.Gemini/Function-call-with-gemini.md) - [Function call with local model](../articles/Function-call-with-ollama-and-litellm.md) - [Use kernel plugin in other agents](../articles/AutoGen.SemanticKernel/Use-kernel-plugin-in-other-agents.md) - [function call in mistral](../articles/MistralChatAgent-use-function-call.md)"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Chat-with-an-agent.md",
        "label": "autogen",
        "content": "This tutorial shows how to generate response using an @AutoGen.Core.IAgent by taking @AutoGen.OpenAI.OpenAIChatAgent as an example.  > [!NOTE] > AutoGen.Net provides the following agents to connect to different LLM platforms. Generating responses using these agents is similar to the example shown below. > - @AutoGen.OpenAI.OpenAIChatAgent > - @AutoGen.SemanticKernel.SemanticKernelAgent > - @AutoGen.LMStudio.LMStudioAgent > - @AutoGen.Mistral.MistralClientAgent > - @AutoGen.Anthropic.AnthropicClientAgent > - @AutoGen.Ollama.OllamaAgent > - @AutoGen.Gemini.GeminiChatAgent  > [!NOTE] > The complete code example can be found in [Chat_With_Agent.cs](https://github.com/microsoft/autogen/blob/main/dotnet/sample/AutoGen.BasicSamples/GettingStart/Chat_With_Agent.cs) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Chat-with-an-agent.md",
        "label": "autogen",
        "content": "Step 1: Install AutoGen  First, install the AutoGen package using the following command:  ```bash dotnet add package AutoGen ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Chat-with-an-agent.md",
        "label": "autogen",
        "content": "Step 2: add Using Statements  [!code-csharp[Using Statements](../../sample/AutoGen.BasicSamples/GettingStart/Chat_With_Agent.cs?name=Using)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Chat-with-an-agent.md",
        "label": "autogen",
        "content": "Step 3: Create an @AutoGen.OpenAI.OpenAIChatAgent  > [!NOTE] > The @AutoGen.OpenAI.Extension.OpenAIAgentExtension.RegisterMessageConnector* method registers an @AutoGen.OpenAI.OpenAIChatRequestMessageConnector middleware which converts OpenAI message types to AutoGen message types. This step is necessary when you want to use AutoGen built-in message types like @AutoGen.Core.TextMessage, @AutoGen.Core.ImageMessage, etc. > For more information, see [Built-in-messages](../articles/Built-in-messages.md)  [!code-csharp[Create an OpenAIChatAgent](../../sample/AutoGen.BasicSamples/GettingStart/Chat_With_Agent.cs?name=Create_Agent)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Chat-with-an-agent.md",
        "label": "autogen",
        "content": "Step 4: Generate Response To generate response, you can use one of the overloaded method of @AutoGen.Core.AgentExtension.SendAsync* method. The following code shows how to generate response with text message:  [!code-csharp[Generate Response](../../sample/AutoGen.BasicSamples/GettingStart/Chat_With_Agent.cs?name=Chat_With_Agent)]  To generate response with chat history, you can pass the chat history to the @AutoGen.Core.AgentExtension.SendAsync* method:  [!code-csharp[Generate Response with Chat History](../../sample/AutoGen.BasicSamples/GettingStart/Chat_With_Agent.cs?name=Chat_With_History)]  To streamingly generate response, use @AutoGen.Core.IStreamingAgent.GenerateStreamingReplyAsync*  [!code-csharp[Generate Streaming Response](../../sample/AutoGen.BasicSamples/GettingStart/Chat_With_Agent.cs?name=Streaming_Chat)] "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/tutorial/Chat-with-an-agent.md",
        "label": "autogen",
        "content": "Further Reading - [Chat with google gemini](../articles/AutoGen.Gemini/Chat-with-google-gemini.md) - [Chat with vertex gemini](../articles/AutoGen.Gemini/Chat-with-vertex-gemini.md) - [Chat with Ollama](../articles/AutoGen.Ollama/Chat-with-llama.md) - [Chat with Semantic Kernel Agent](../articles/AutoGen.SemanticKernel/SemanticKernelAgent-simple-chat.md)"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/release_note/0.0.17.md",
        "label": "autogen",
        "content": "# AutoGen.Net 0.0.17 Release Notes "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/release_note/0.0.17.md",
        "label": "autogen",
        "content": "\ud83c\udf1f What's New  1. **.NET Core Target Framework Support** ([#3203](https://github.com/microsoft/autogen/issues/3203))    - \ud83d\ude80 Added support for .NET Core to ensure compatibility and enhanced performance of AutoGen packages across different platforms.  2. **Kernel Support in Interactive Service Constructor** ([#3181](https://github.com/microsoft/autogen/issues/3181))    - \ud83e\udde0 Enhanced the Interactive Service to accept a kernel in its constructor, facilitating usage in notebook environments.  3. **Constructor Options for OpenAIChatAgent** ([#3126](https://github.com/microsoft/autogen/issues/3126))    - \u2699\ufe0f Added new constructor options for `OpenAIChatAgent` to allow full control over chat completion flags/options.  4. **Step-by-Step Execution for Group Chat** ([#3075](https://github.com/microsoft/autogen/issues/3075))    - \ud83d\udee0\ufe0f Introduced an `IAsyncEnumerable` extension API to run group chat step-by-step, enabling developers to observe internal processes or implement early stopping mechanisms. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/release_note/0.0.17.md",
        "label": "autogen",
        "content": "\ud83d\ude80 Improvements  1. **Cancellation Token Addition in Graph APIs** ([#3111](https://github.com/microsoft/autogen/issues/3111))    - \ud83d\udd04 Added cancellation tokens to async APIs in the `AutoGen.Core.Graph` class to follow best practices and enhance the control flow. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/release_note/0.0.17.md",
        "label": "autogen",
        "content": "\u26a0\ufe0f API Breaking Changes  1. **FunctionDefinition Generation Stopped in Source Generator** ([#3133](https://github.com/microsoft/autogen/issues/3133))    - \ud83d\uded1 Stopped generating `FunctionDefinition` from `Azure.AI.OpenAI` in the source generator to eliminate unnecessary package dependencies. Migration guide:      - \u27a1\ufe0f Use `ToOpenAIFunctionDefinition()` extension from `AutoGen.OpenAI` for generating `FunctionDefinition` from `AutoGen.Core.FunctionContract`.      - \u27a1\ufe0f Use `FunctionContract` for metadata such as function name or parameters.  2. **Namespace Renaming for AutoGen.WebAPI** ([#3152](https://github.com/microsoft/autogen/issues/3152))    - \u270f\ufe0f Renamed the namespace of `AutoGen.WebAPI` from `AutoGen.Service` to `AutoGen.WebAPI` to maintain consistency with the project name.  3. **Semantic Kernel Version Update** ([#3118](https://github.com/microsoft/autogen/issues/3118))    - \ud83d\udcc8 Upgraded the Semantic Kernel version to 1.15.1 for enhanced functionality and performance improvements. This might introduce break change for those who use a lower-version semantic kernel. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/release_note/0.0.17.md",
        "label": "autogen",
        "content": "\ud83d\udcda Documentation  1. **Consume AutoGen.Net Agent in AG Studio** ([#3142](https://github.com/microsoft/autogen/issues/3142))    - Added detailed documentation on using AutoGen.Net Agent as a model in AG Studio, including examples of starting an OpenAI chat backend and integrating third-party OpenAI models.  2. **Middleware Overview Documentation Errors Fixed** ([#3129](https://github.com/microsoft/autogen/issues/3129))    - Corrected logic and compile errors in the example code provided in the Middleware Overview documentation to ensure it runs without issues.  ---  We hope you enjoy the new features and improvements in AutoGen.Net 0.0.17! If you encounter any issues or have feedback, please open a new issue on our [GitHub repository](https://github.com/microsoft/autogen/issues)."
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/release_note/update.md",
        "label": "autogen",
        "content": "##### Update on 0.0.15 (2024-06-13) Milestone: [AutoGen.Net 0.0.15](https://github.com/microsoft/autogen/milestone/3)  ###### Highlights - [Issue 2851](https://github.com/microsoft/autogen/issues/2851) `AutoGen.Gemini` package for Gemini support. Examples can be found [here](https://github.com/microsoft/autogen/tree/main/dotnet/sample/AutoGen.Gemini.Sample)  ##### Update on 0.0.14 (2024-05-28) ###### New features - [Issue 2319](https://github.com/microsoft/autogen/issues/2319) Add `AutoGen.Ollama` package for Ollama support. Special thanks to @iddelacruz for the effort. - [Issue 2608](https://github.com/microsoft/autogen/issues/2608) Add `AutoGen.Anthropic` package for Anthropic support. Special thanks to @DavidLuong98 for the effort. - [Issue 2647](https://github.com/microsoft/autogen/issues/2647) Add `ToolCallAggregateMessage` for function call middleware.  ###### API Breaking Changes - [Issue 2648](https://github.com/microsoft/autogen/issues/2648) Deprecate `Message` type. - [Issue 2649](https://github.com/microsoft/autogen/issues/2649) Deprecate `Workflow` type. ###### Bug Fixes - [Issue 2735](https://github.com/microsoft/autogen/issues/2735) Fix tool call issue in AutoGen.Mistral package. - [Issue 2722](https://github.com/microsoft/autogen/issues/2722) Fix parallel funciton call in function call middleware. - [Issue 2633](https://github.com/microsoft/autogen/issues/2633) Set up `name` field in `OpenAIChatMessageConnector` - [Issue 2660](https://github.com/microsoft/autogen/issues/2660) Fix dotnet interactive restoring issue when system language is Chinese - [Issue 2687](https://github.com/microsoft/autogen/issues/2687) Add `global::` prefix to generated code to avoid conflict with user-defined types.  ##### Update on 0.0.13 (2024-05-09) ###### New features - [Issue 2593](https://github.com/microsoft/autogen/issues/2593) Consume SK plugins in Agent. - [Issue 1893](https://github.com/microsoft/autogen/issues/1893) Support inline-data in ImageMessage - [Issue 2481](https://github.com/microsoft/autogen/issues/2481) Introduce `ChatCompletionAgent` to `AutoGen.SemanticKernel` ###### API Breaking Changes - [Issue 2470](https://github.com/microsoft/autogen/issues/2470) Update the return type of `IStreamingAgent.GenerateStreamingReplyAsync` from `Task<IAsyncEnumerable<IStreamingMessage>>` to `IAsyncEnumerable<IStreamingMessage>` - [Issue 2470](https://github.com/microsoft/autogen/issues/2470) Update the return type of `IStreamingMiddleware.InvokeAsync` from `Task<IAsyncEnumerable<IStreamingMessage>>` to `IAsyncEnumerable<IStreamingMessage>` - Mark `RegisterReply`, `RegisterPreProcess` and `RegisterPostProcess` as obsolete. You can replace them with `RegisterMiddleware`  ###### Bug Fixes - Fix [Issue 2609](https://github.com/microsoft/autogen/issues/2609) Constructor of conversableAgentConfig does not accept LMStudioConfig as ConfigList  ##### Update on 0.0.12 (2024-04-22) - Add AutoGen.Mistral package to support Mistral.AI models ##### Update on 0.0.11 (2024-04-10) - Add link to Discord channel in nuget's readme.md - Document improvements - In `AutoGen.OpenAI`, update `Azure.AI.OpenAI` to 1.0.0-beta.15 and add support for json mode and deterministic output in `OpenAIChatAgent` [Issue #2346](https://github.com/microsoft/autogen/issues/2346) - In `AutoGen.SemanticKernel`, update `SemanticKernel` package to 1.7.1 - [API Breaking Change] Rename `PrintMessageMiddlewareExtension.RegisterPrintFormatMessageHook' to `PrintMessageMiddlewareExtension.RegisterPrintMessage`. ##### Update on 0.0.10 (2024-03-12) - Rename `Workflow` to `Graph` - Rename `AddInitializeMessage` to `SendIntroduction` - Rename `SequentialGroupChat` to `RoundRobinGroupChat` ##### Update on 0.0.9 (2024-03-02) - Refactor over @AutoGen.Message and introducing `TextMessage`, `ImageMessage`, `MultiModalMessage` and so on. PR [#1676](https://github.com/microsoft/autogen/pull/1676) - Add `AutoGen.SemanticKernel` to support seamless integration with Semantic Kernel - Move the agent contract abstraction to `AutoGen.Core` package. The `AutoGen.Core` package provides the abstraction for message type, agent and group chat and doesn't contain dependencies over `Azure.AI.OpenAI` or `Semantic Kernel`. This is useful when you want to leverage AutoGen's abstraction only and want to avoid introducing any other dependencies. - Move `GPTAgent`, `OpenAIChatAgent` and all openai-dependencies to `AutoGen.OpenAI` ##### Update on 0.0.8 (2024-02-28) - Fix [#1804](https://github.com/microsoft/autogen/pull/1804) - Streaming support for IAgent [#1656](https://github.com/microsoft/autogen/pull/1656) - Streaming support for middleware via `MiddlewareStreamingAgent` [#1656](https://github.com/microsoft/autogen/pull/1656) - Graph chat support with conditional transition workflow [#1761](https://github.com/microsoft/autogen/pull/1761) - AutoGen.SourceGenerator: Generate `FunctionContract` from `FunctionAttribute` [#1736](https://github.com/microsoft/autogen/pull/1736) ##### Update on 0.0.7 (2024-02-11) - Add `AutoGen.LMStudio` to support comsume openai-like API from LMStudio local server ##### Update on 0.0.6 (2024-01-23) - Add `MiddlewareAgent` - Use `MiddlewareAgent` to implement existing agent hooks (RegisterPreProcess, RegisterPostProcess, RegisterReply) - Remove `AutoReplyAgent`, `PreProcessAgent`, `PostProcessAgent` because they are replaced by `MiddlewareAgent` ##### Update on 0.0.5 - Simplify `IAgent` interface by removing `ChatLLM` Property - Add `GenerateReplyOptions` to `IAgent.GenerateReplyAsync` which allows user to specify or override the options when generating reply  ##### Update on 0.0.4 - Move out dependency of Semantic Kernel - Add type `IChatLLM` as connector to LLM  ##### Update on 0.0.3 - In AutoGen.SourceGenerator, rename FunctionAttribution to FunctionAttribute - In AutoGen, refactor over ConversationAgent, UserProxyAgent, and AssistantAgent  ##### Update on 0.0.2 - update Azure.OpenAI.AI to 1.0.0-beta.12 - update Semantic kernel to 1.0.1"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/release_note/0.0.16.md",
        "label": "autogen",
        "content": "# AutoGen.Net 0.0.16 Release Notes  We are excited to announce the release of **AutoGen.Net 0.0.16**. This release includes several new features, bug fixes, improvements, and important updates. Below are the detailed release notes:  **[Milestone: AutoGen.Net 0.0.16](https://github.com/microsoft/autogen/milestone/4)** "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/release_note/0.0.16.md",
        "label": "autogen",
        "content": "\ud83d\udce6 New Features 1. **Deprecate `IStreamingMessage`** ([#3045](https://github.com/microsoft/autogen/issues/3045)) - Replaced `IStreamingMessage` and `IStreamingMessage<T>` with `IMessage` and `IMessage<T>`. 2. **Add example for using ollama + LiteLLM for function call** ([#3014](https://github.com/microsoft/autogen/issues/3014)) - Added a new tutorial to the website for integrating ollama with LiteLLM for function calls. 3. **Add ReAct sample** ([#2978](https://github.com/microsoft/autogen/issues/2978)) - Added a new sample demonstrating the ReAct pattern. 4. **Support tools Anthropic Models** ([#2771](https://github.com/microsoft/autogen/issues/2771)) - Introduced support for tools like `AnthropicClient`, `AnthropicClientAgent`, and `AnthropicMessageConnector`. 5. **Propose Orchestrator for managing group chat/agentic workflow** ([#2695](https://github.com/microsoft/autogen/issues/2695)) - Introduced a customizable orchestrator interface for managing group chats and agent workflows. 6. **Run Agent as Web API** ([#2519](https://github.com/microsoft/autogen/issues/2519)) - Introduced the ability to start an OpenAI-chat-compatible web API from an arbitrary agent. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/release_note/0.0.16.md",
        "label": "autogen",
        "content": "\ud83d\udc1b Bug Fixes 1. **SourceGenerator doesn't work when function's arguments are empty** ([#2976](https://github.com/microsoft/autogen/issues/2976)) - Fixed an issue where the SourceGenerator failed when function arguments were empty. 2. **Add content field in ToolCallMessage** ([#2975](https://github.com/microsoft/autogen/issues/2975)) - Added a content property in `ToolCallMessage` to handle text content returned by the OpenAI model during tool calls. 3. **AutoGen.SourceGenerator doesn\u2019t encode `\"` in structural comments** ([#2872](https://github.com/microsoft/autogen/issues/2872)) - Fixed an issue where structural comments containing `\"` were not properly encoded, leading to compilation errors. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/release_note/0.0.16.md",
        "label": "autogen",
        "content": "\ud83d\ude80 Improvements 1. **Sample update - Add getting-start samples for BasicSample project** ([#2859](https://github.com/microsoft/autogen/issues/2859)) - Re-organized the `AutoGen.BasicSample` project to include only essential getting-started examples, simplifying complex examples. 2. **Graph constructor should consider null transitions** ([#2708](https://github.com/microsoft/autogen/issues/2708)) - Updated the Graph constructor to handle cases where transitions\u2019 values are null. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/release_note/0.0.16.md",
        "label": "autogen",
        "content": "\u26a0\ufe0f API-Breakchange 1. **Deprecate `IStreamingMessage`** ([#3045](https://github.com/microsoft/autogen/issues/3045)) - **Migration guide:** Deprecating `IStreamingMessage` will introduce breaking changes, particularly for `IStreamingAgent` and `IStreamingMiddleware`. Replace all `IStreamingMessage` and `IStreamingMessage<T>` with `IMessage` and `IMessage<T>`. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/website/release_note/0.0.16.md",
        "label": "autogen",
        "content": "\ud83d\udcda Document Update 1. **Add example for using ollama + LiteLLM for function call** ([#3014](https://github.com/microsoft/autogen/issues/3014)) - Added a tutorial to the website for using ollama with LiteLLM.  Thank you to all the contributors for making this release possible. We encourage everyone to upgrade to AutoGen.Net 0.0.16 to take advantage of these new features and improvements. If you encounter any issues or have any feedback, please let us know.  Happy coding! \ud83d\ude80"
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/src/AutoGen.SourceGenerator/README.md",
        "label": "autogen",
        "content": "### AutoGen.SourceGenerator  This package carries a source generator that adds support for type-safe function definition generation. Simply mark a method with `Function` attribute, and the source generator will generate a function definition and a function call wrapper for you.  ### Get start  First, add the following to your project file and set `GenerateDocumentationFile` property to true  ```xml <PropertyGroup>     <!-- This enables structural xml document support -->     <GenerateDocumentationFile>true</GenerateDocumentationFile> </PropertyGroup> ``` ```xml <ItemGroup>     <PackageReference Include=\"AutoGen.SourceGenerator\" /> </ItemGroup> ```  > Nightly Build feed: https://devdiv.pkgs.visualstudio.com/DevDiv/_packaging/AutoGen/nuget/v3/index.json  Then, for the methods you want to generate function definition and function call wrapper, mark them with `Function` attribute:  > Note: For the best of performance, try using primitive types for the parameters and return type.  ```csharp // file: MyFunctions.cs  using AutoGen;  // a partial class is required // and the class must be public public partial class MyFunctions {     /// <summary>     /// Add two numbers.     /// </summary>     /// <param name=\"a\">The first number.</param>     /// <param name=\"b\">The second number.</param>     [Function]     public Task<string> AddAsync(int a, int b)     {         return Task.FromResult($\"{a} + {b} = {a + b}\");     } } ```  The source generator will generate the following code based on the method signature and documentation. It helps you save the effort of writing function definition and keep it up to date with the actual method signature.  ```csharp // file: MyFunctions.generated.cs public partial class MyFunctions {     private class AddAsyncSchema     { \t\tpublic int a {get; set;} \t\tpublic int b {get; set;}     }      public Task<string> AddAsyncWrapper(string arguments)     {         var schema = JsonSerializer.Deserialize<AddAsyncSchema>(             arguments,              new JsonSerializerOptions             {                 PropertyNamingPolicy = JsonNamingPolicy.CamelCase,             });         return AddAsync(schema.a, schema.b);     }      public FunctionDefinition AddAsyncFunction     {         get => new FunctionDefinition \t\t{ \t\t\tName = @\"AddAsync\",             Description = \"\"\" Add two numbers. \"\"\",             Parameters = BinaryData.FromObjectAsJson(new             {                 Type = \"object\",                 Properties = new \t\t\t\t{ \t\t\t\t    a = new \t\t\t\t    { \t\t\t\t\t    Type = @\"number\", \t\t\t\t\t    Description = @\"The first number.\", \t\t\t\t    }, \t\t\t\t    b = new \t\t\t\t    { \t\t\t\t\t    Type = @\"number\", \t\t\t\t\t    Description = @\"The second number.\", \t\t\t\t    },                 },                 Required = new [] \t\t\t\t{ \t\t\t\t    \"a\", \t\t\t\t    \"b\", \t\t\t\t},             },             new JsonSerializerOptions \t\t\t{ \t\t\t\tPropertyNamingPolicy = JsonNamingPolicy.CamelCase, \t\t\t})         };     } } ```  For more examples, please check out the following project - [AutoGen.BasicSamples](../sample/AutoGen.BasicSamples/) - [AutoGen.SourceGenerator.Tests](../../test/AutoGen.SourceGenerator.Tests/) "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/src/AutoGen.LMStudio/README.md",
        "label": "autogen",
        "content": "## AutoGen.LMStudio  This package provides support for consuming openai-like API from LMStudio local server. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/src/AutoGen.LMStudio/README.md",
        "label": "autogen",
        "content": "Installation To use `AutoGen.LMStudio`, add the following package to your `.csproj` file:  ```xml <ItemGroup>     <PackageReference Include=\"AutoGen.LMStudio\" Version=\"AUTOGEN_VERSION\" /> </ItemGroup> ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/src/AutoGen.LMStudio/README.md",
        "label": "autogen",
        "content": "Usage ```csharp using AutoGen.LMStudio; var localServerEndpoint = \"localhost\"; var port = 5000; var lmStudioConfig = new LMStudioConfig(localServerEndpoint, port); var agent = new LMStudioAgent(     name: \"agent\",     systemMessage: \"You are an agent that help user to do some tasks.\",     lmStudioConfig: lmStudioConfig)     .RegisterPrintMessage(); // register a hook to print message nicely to console  await agent.SendAsync(\"Can you write a piece of C# code to calculate 100th of fibonacci?\"); ``` "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/dotnet/src/AutoGen.LMStudio/README.md",
        "label": "autogen",
        "content": "Update history ### Update on 0.0.7 (2024-02-11) - Add `LMStudioAgent` to support consuming openai-like API from LMStudio local server. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/.devcontainer/README.md",
        "label": "autogen",
        "content": "# Dockerfiles and Devcontainer Configurations for AutoGen  Welcome to the `.devcontainer` directory! Here you'll find Dockerfiles and devcontainer configurations that are essential for setting up your AutoGen development environment. Each Dockerfile is tailored for different use cases and requirements. Below is a brief overview of each and how you can utilize them effectively.  These configurations can be used with Codespaces and locally. "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/.devcontainer/README.md",
        "label": "autogen",
        "content": "Dockerfile Descriptions  ### base  - **Purpose**: This Dockerfile, i.e., `./Dockerfile`, is designed for basic setups. It includes common Python libraries and essential dependencies required for general usage of AutoGen. - **Usage**: Ideal for those just starting with AutoGen or for general-purpose applications. - **Building the Image**: Run `docker build -f ./Dockerfile -t autogen_base_img .` in this directory. - **Using with Codespaces**: `Code > Codespaces > Click on +` By default + creates a Codespace on the current branch.  ### full  - **Purpose**: This Dockerfile, i.e., `./full/Dockerfile` is for advanced features. It includes additional dependencies and is configured for more complex or feature-rich AutoGen applications. - **Usage**: Suited for advanced users who need the full range of AutoGen's capabilities. - **Building the Image**: Execute `docker build -f full/Dockerfile -t autogen_full_img .`. - **Using with Codespaces**: `Code > Codespaces > Click on ...> New with options > Choose \"full\" as devcontainer configuration`. This image may require a Codespace with at least 64GB of disk space.  ### dev  - **Purpose**: Tailored for AutoGen project developers, this Dockerfile, i.e., `./dev/Dockerfile` includes tools and configurations aiding in development and contribution. - **Usage**: Recommended for developers who are contributing to the AutoGen project. - **Building the Image**: Run `docker build -f dev/Dockerfile -t autogen_dev_img .`. - **Using with Codespaces**: `Code > Codespaces > Click on ...> New with options > Choose \"dev\" as devcontainer configuration`. This image may require a Codespace with at least 64GB of disk space. - **Before using**: We highly encourage all potential contributors to read the [AutoGen Contributing](https://microsoft.github.io/autogen/docs/Contribute) page prior to submitting any pull requests.   ### studio  - **Purpose**: Tailored for AutoGen project developers, this Dockerfile, i.e., `./studio/Dockerfile`, includes tools and configurations aiding in development and contribution. - **Usage**: Recommended for developers who are contributing to the AutoGen project. - **Building the Image**: Run `docker build -f studio/Dockerfile -t autogen_studio_img .`. - **Using with Codespaces**: `Code > Codespaces > Click on ...> New with options > Choose \"studio\" as devcontainer configuration`. - **Before using**: We highly encourage all potential contributors to read the [AutoGen Contributing](https://microsoft.github.io/autogen/docs/Contribute) page prior to submitting any pull requests.  "
    },
    {
        "source": "GitHub",
        "repository": "autogen",
        "file": "autogen/.devcontainer/README.md",
        "label": "autogen",
        "content": "Customizing Dockerfiles  Feel free to modify these Dockerfiles for your specific project needs. Here are some common customizations:  - **Adding New Dependencies**: If your project requires additional Python packages, you can add them using the `RUN pip install` command. - **Changing the Base Image**: You may change the base image (e.g., from a Python image to an Ubuntu image) to suit your project's requirements. - **Changing the Python version**: do you need a different version of python other than 3.11. Just update the first line of each of the Dockerfiles like so:     `FROM python:3.11-slim-bookworm` to `FROM python:3.10-slim-bookworm` - **Setting Environment Variables**: Add environment variables using the `ENV` command for any application-specific configurations. We have prestaged the line needed to inject your OpenAI_key into the docker environment as a environmental variable. Others can be staged in the same way. Just uncomment the line.     `# ENV OPENAI_API_KEY=\"{OpenAI-API-Key}\"` to `ENV OPENAI_API_KEY=\"{OpenAI-API-Key}\"` - **Need a less \"Advanced\" Autogen build**: If the `./full/Dockerfile` is to much but you need more than advanced then update this line in the Dockerfile file. `RUN pip install pyautogen[teachable,lmm,retrievechat,mathchat,blendsearch] autogenra` to install just what you need. `RUN pip install pyautogen[retrievechat,blendsearch] autogenra` - **Can't Dev without your favorite CLI tool**: if you need particular OS tools to be installed in your Docker container you can add those packages here right after the sudo for the `./base/Dockerfile` and `./full/Dockerfile` files. In the example below we are installing net-tools and vim to the environment.      ```code     RUN apt-get update \\         && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \\             software-properties-common sudo net-tools vim\\         && apt-get clean \\         && rm -rf /var/lib/apt/lists/*     ```  ### Managing Your Docker Environment  After customizing your Dockerfile, build the Docker image using the `docker build` command as shown above. To run a container based on your new image, use:  ```bash docker run -it -v $(pwd)/your_app:/app your_image_name ```  Replace `your_app` with your application directory and `your_image_name` with the name of the image you built.  #### Closing for the Day  - **Exit the container**: Type `exit`. - **Stop the container**: Use `docker stop {application_project_name}`.  #### Resuming Work  - **Restart the container**: Use `docker start {application_project_name}`. - **Access the container**: Execute `sudo docker exec -it {application_project_name} bash`. - **Reactivate the environment**: Run `source /usr/src/app/autogen_env/bin/activate`.  ### Useful Docker Commands  - **View running containers**: `docker ps -a`. - **View Docker images**: `docker images`. - **Restart container setup**: Stop (`docker stop my_container`), remove the container (`docker rm my_container`), and remove the image (`docker rmi my_image:latest`).  #### Troubleshooting Common Issues  - Check Docker daemon, port conflicts, and permissions issues.  #### Additional Resources  For more information on Docker usage and best practices, refer to the [official Docker documentation](https://docs.docker.com). "
    }
]