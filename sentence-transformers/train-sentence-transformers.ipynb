{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saturday, June 8, 2024\n",
    "\n",
    "[Training and Finetuning Embedding Models with Sentence Transformers v3](https://huggingface.co/blog/train-sentence-transformers)\n",
    "\n",
    "This notebook was manually created from the above document. \n",
    "\n",
    "*** mamba activate ftllm ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need these next two statements, otherwise we get ...\n",
    "# NotImplementedError: Using RTX 4000 series doesn't support faster communication broadband via P2P or IB. Please set `NCCL_P2P_DISABLE=\\\"1\\\"` and `NCCL_IB_DISABLE=\\\"1\\\" or use `accelerate launch` which will do this automatically.\"\n",
    "# ... when we try to initialize SentenceTransformerTrainingArguments further on down ... \n",
    "os.environ[\"NCCL_P2P_DISABLE\"]=\"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    "    SentenceTransformerModelCardData,\n",
    ")\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TripletEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name microsoft/mpnet-base. Creating a new one with mean pooling.\n",
      "/home/rob/miniforge3/envs/ftllm/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.bias', 'mpnet.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load a model to finetune with 2. (Optional) model card data\n",
    "model = SentenceTransformer(\n",
    "    \"microsoft/mpnet-base\",\n",
    "    model_card_data=SentenceTransformerModelCardData(\n",
    "        language=\"en\",\n",
    "        license=\"apache-2.0\",\n",
    "        model_name=\"MPNet base trained on AllNLI triplets\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# 7m 31.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load a dataset to finetune on\n",
    "dataset = load_dataset(\"sentence-transformers/all-nli\", \"triplet\")\n",
    "train_dataset = dataset[\"train\"].select(range(100_000))\n",
    "eval_dataset = dataset[\"dev\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# 37.3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define a loss function\n",
    "loss = MultipleNegativesRankingLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf16 = torch.cuda.is_bf16_supported()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. (Optional) Specify training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=\"models/mpnet-base-all-nli-triplet\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if GPU can't handle FP16\n",
    "    bf16=False,  # Set to True if GPU supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicates\n",
    "    # Optional tracking/debugging parameters:\n",
    "    # eval_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    run_name=\"mpnet-base-all-nli-triplet\",  # Used in W&B if `wandb` is installed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all-nli-dev_cosine_accuracy': 0.6210510328068044,\n",
       " 'all-nli-dev_dot_accuracy': 0.45337181044957475,\n",
       " 'all-nli-dev_manhattan_accuracy': 0.6831713244228432,\n",
       " 'all-nli-dev_euclidean_accuracy': 0.62226609963548,\n",
       " 'all-nli-dev_max_accuracy': 0.6831713244228432}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. (Optional) Create an evaluator & evaluate the base model\n",
    "dev_evaluator = TripletEvaluator(\n",
    "    anchors=eval_dataset[\"anchor\"],\n",
    "    positives=eval_dataset[\"positive\"],\n",
    "    negatives=eval_dataset[\"negative\"],\n",
    "    name=\"all-nli-dev\",\n",
    ")\n",
    "dev_evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Create a trainer & train\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss=loss,\n",
    "    evaluator=dev_evaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrobkayinto\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rob/Data/Documents/Github/rkaunismaa/LLM-Fine-Tuning-Playground/sentence-transformers/wandb/run-20240608_100337-ivfdyysu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/robkayinto/sentence-transformers/runs/ivfdyysu/workspace' target=\"_blank\">mpnet-base-all-nli-triplet</a></strong> to <a href='https://wandb.ai/robkayinto/sentence-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/robkayinto/sentence-transformers' target=\"_blank\">https://wandb.ai/robkayinto/sentence-transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/robkayinto/sentence-transformers/runs/ivfdyysu/workspace' target=\"_blank\">https://wandb.ai/robkayinto/sentence-transformers/runs/ivfdyysu/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 58:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>All-nli-dev Cosine Accuracy</th>\n",
       "      <th>All-nli-dev Dot Accuracy</th>\n",
       "      <th>All-nli-dev Manhattan Accuracy</th>\n",
       "      <th>All-nli-dev Euclidean Accuracy</th>\n",
       "      <th>All-nli-dev Max Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.643800</td>\n",
       "      <td>1.087817</td>\n",
       "      <td>0.771871</td>\n",
       "      <td>0.281440</td>\n",
       "      <td>0.793894</td>\n",
       "      <td>0.777491</td>\n",
       "      <td>0.793894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.933200</td>\n",
       "      <td>0.838646</td>\n",
       "      <td>0.802552</td>\n",
       "      <td>0.219775</td>\n",
       "      <td>0.808627</td>\n",
       "      <td>0.802855</td>\n",
       "      <td>0.808627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.266300</td>\n",
       "      <td>0.827029</td>\n",
       "      <td>0.810753</td>\n",
       "      <td>0.194866</td>\n",
       "      <td>0.805286</td>\n",
       "      <td>0.805741</td>\n",
       "      <td>0.810753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.807300</td>\n",
       "      <td>0.848850</td>\n",
       "      <td>0.803615</td>\n",
       "      <td>0.195778</td>\n",
       "      <td>0.803919</td>\n",
       "      <td>0.797539</td>\n",
       "      <td>0.803919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>1.023592</td>\n",
       "      <td>0.784933</td>\n",
       "      <td>0.219927</td>\n",
       "      <td>0.788275</td>\n",
       "      <td>0.781288</td>\n",
       "      <td>0.788275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.971800</td>\n",
       "      <td>1.268849</td>\n",
       "      <td>0.778554</td>\n",
       "      <td>0.246051</td>\n",
       "      <td>0.779313</td>\n",
       "      <td>0.778706</td>\n",
       "      <td>0.779313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.812600</td>\n",
       "      <td>1.364640</td>\n",
       "      <td>0.777035</td>\n",
       "      <td>0.225243</td>\n",
       "      <td>0.780529</td>\n",
       "      <td>0.776883</td>\n",
       "      <td>0.780529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.030400</td>\n",
       "      <td>1.444258</td>\n",
       "      <td>0.751215</td>\n",
       "      <td>0.266859</td>\n",
       "      <td>0.752126</td>\n",
       "      <td>0.750152</td>\n",
       "      <td>0.752126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.076300</td>\n",
       "      <td>1.103864</td>\n",
       "      <td>0.780680</td>\n",
       "      <td>0.214611</td>\n",
       "      <td>0.778250</td>\n",
       "      <td>0.779617</td>\n",
       "      <td>0.780680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.016800</td>\n",
       "      <td>1.137608</td>\n",
       "      <td>0.770352</td>\n",
       "      <td>0.226154</td>\n",
       "      <td>0.768834</td>\n",
       "      <td>0.768682</td>\n",
       "      <td>0.770352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.961700</td>\n",
       "      <td>1.365014</td>\n",
       "      <td>0.757898</td>\n",
       "      <td>0.246051</td>\n",
       "      <td>0.755620</td>\n",
       "      <td>0.756075</td>\n",
       "      <td>0.757898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.018300</td>\n",
       "      <td>1.073408</td>\n",
       "      <td>0.769897</td>\n",
       "      <td>0.225395</td>\n",
       "      <td>0.770049</td>\n",
       "      <td>0.767770</td>\n",
       "      <td>0.770049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>1.184292</td>\n",
       "      <td>0.761239</td>\n",
       "      <td>0.242102</td>\n",
       "      <td>0.760024</td>\n",
       "      <td>0.760328</td>\n",
       "      <td>0.761239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.778100</td>\n",
       "      <td>1.130251</td>\n",
       "      <td>0.761391</td>\n",
       "      <td>0.237546</td>\n",
       "      <td>0.762606</td>\n",
       "      <td>0.761999</td>\n",
       "      <td>0.762606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.857400</td>\n",
       "      <td>1.273771</td>\n",
       "      <td>0.764277</td>\n",
       "      <td>0.241191</td>\n",
       "      <td>0.763670</td>\n",
       "      <td>0.764429</td>\n",
       "      <td>0.764429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.728400</td>\n",
       "      <td>1.077232</td>\n",
       "      <td>0.767315</td>\n",
       "      <td>0.231926</td>\n",
       "      <td>0.763973</td>\n",
       "      <td>0.766555</td>\n",
       "      <td>0.767315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.846100</td>\n",
       "      <td>1.139885</td>\n",
       "      <td>0.787363</td>\n",
       "      <td>0.208232</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.787515</td>\n",
       "      <td>0.787515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.855900</td>\n",
       "      <td>1.134300</td>\n",
       "      <td>0.773086</td>\n",
       "      <td>0.228433</td>\n",
       "      <td>0.773390</td>\n",
       "      <td>0.773998</td>\n",
       "      <td>0.773998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.785900</td>\n",
       "      <td>1.149124</td>\n",
       "      <td>0.787819</td>\n",
       "      <td>0.211725</td>\n",
       "      <td>0.787971</td>\n",
       "      <td>0.786148</td>\n",
       "      <td>0.787971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.802400</td>\n",
       "      <td>1.195555</td>\n",
       "      <td>0.795109</td>\n",
       "      <td>0.206106</td>\n",
       "      <td>0.794502</td>\n",
       "      <td>0.794350</td>\n",
       "      <td>0.795109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>1.159352</td>\n",
       "      <td>0.809386</td>\n",
       "      <td>0.191373</td>\n",
       "      <td>0.805741</td>\n",
       "      <td>0.807564</td>\n",
       "      <td>0.809386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.699300</td>\n",
       "      <td>1.309095</td>\n",
       "      <td>0.827764</td>\n",
       "      <td>0.171324</td>\n",
       "      <td>0.828068</td>\n",
       "      <td>0.827612</td>\n",
       "      <td>0.828068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.841800</td>\n",
       "      <td>1.210921</td>\n",
       "      <td>0.804678</td>\n",
       "      <td>0.187272</td>\n",
       "      <td>0.804830</td>\n",
       "      <td>0.803767</td>\n",
       "      <td>0.804830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.782300</td>\n",
       "      <td>1.126743</td>\n",
       "      <td>0.797691</td>\n",
       "      <td>0.208991</td>\n",
       "      <td>0.800425</td>\n",
       "      <td>0.797084</td>\n",
       "      <td>0.800425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.717900</td>\n",
       "      <td>1.098700</td>\n",
       "      <td>0.842649</td>\n",
       "      <td>0.159478</td>\n",
       "      <td>0.840522</td>\n",
       "      <td>0.843408</td>\n",
       "      <td>0.843408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.581200</td>\n",
       "      <td>0.944664</td>\n",
       "      <td>0.824727</td>\n",
       "      <td>0.176337</td>\n",
       "      <td>0.820930</td>\n",
       "      <td>0.822296</td>\n",
       "      <td>0.824727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.888500</td>\n",
       "      <td>1.364992</td>\n",
       "      <td>0.814095</td>\n",
       "      <td>0.183627</td>\n",
       "      <td>0.810905</td>\n",
       "      <td>0.810905</td>\n",
       "      <td>0.814095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.000600</td>\n",
       "      <td>1.046587</td>\n",
       "      <td>0.821841</td>\n",
       "      <td>0.178919</td>\n",
       "      <td>0.818196</td>\n",
       "      <td>0.820626</td>\n",
       "      <td>0.821841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.905100</td>\n",
       "      <td>0.995744</td>\n",
       "      <td>0.834295</td>\n",
       "      <td>0.164338</td>\n",
       "      <td>0.831713</td>\n",
       "      <td>0.833991</td>\n",
       "      <td>0.834295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.849500</td>\n",
       "      <td>1.152577</td>\n",
       "      <td>0.791464</td>\n",
       "      <td>0.218256</td>\n",
       "      <td>0.785844</td>\n",
       "      <td>0.790553</td>\n",
       "      <td>0.791464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.868100</td>\n",
       "      <td>1.028408</td>\n",
       "      <td>0.847661</td>\n",
       "      <td>0.155680</td>\n",
       "      <td>0.841738</td>\n",
       "      <td>0.847813</td>\n",
       "      <td>0.847813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.643500</td>\n",
       "      <td>0.985713</td>\n",
       "      <td>0.857230</td>\n",
       "      <td>0.142922</td>\n",
       "      <td>0.855255</td>\n",
       "      <td>0.857230</td>\n",
       "      <td>0.857230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.825680</td>\n",
       "      <td>0.852673</td>\n",
       "      <td>0.144289</td>\n",
       "      <td>0.849180</td>\n",
       "      <td>0.851154</td>\n",
       "      <td>0.852673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.583900</td>\n",
       "      <td>0.799632</td>\n",
       "      <td>0.848420</td>\n",
       "      <td>0.151124</td>\n",
       "      <td>0.844016</td>\n",
       "      <td>0.847357</td>\n",
       "      <td>0.848420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.580600</td>\n",
       "      <td>0.907060</td>\n",
       "      <td>0.860571</td>\n",
       "      <td>0.140492</td>\n",
       "      <td>0.855711</td>\n",
       "      <td>0.858141</td>\n",
       "      <td>0.860571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.592700</td>\n",
       "      <td>0.848892</td>\n",
       "      <td>0.844927</td>\n",
       "      <td>0.155377</td>\n",
       "      <td>0.841282</td>\n",
       "      <td>0.842649</td>\n",
       "      <td>0.844927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.627600</td>\n",
       "      <td>0.788417</td>\n",
       "      <td>0.856318</td>\n",
       "      <td>0.136847</td>\n",
       "      <td>0.849180</td>\n",
       "      <td>0.852825</td>\n",
       "      <td>0.856318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.570500</td>\n",
       "      <td>0.719874</td>\n",
       "      <td>0.863001</td>\n",
       "      <td>0.132139</td>\n",
       "      <td>0.858900</td>\n",
       "      <td>0.861786</td>\n",
       "      <td>0.863001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.516100</td>\n",
       "      <td>0.790863</td>\n",
       "      <td>0.854344</td>\n",
       "      <td>0.141403</td>\n",
       "      <td>0.846142</td>\n",
       "      <td>0.850091</td>\n",
       "      <td>0.854344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.816900</td>\n",
       "      <td>0.787634</td>\n",
       "      <td>0.865279</td>\n",
       "      <td>0.134569</td>\n",
       "      <td>0.860571</td>\n",
       "      <td>0.863153</td>\n",
       "      <td>0.865279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.034400</td>\n",
       "      <td>0.740156</td>\n",
       "      <td>0.877734</td>\n",
       "      <td>0.122722</td>\n",
       "      <td>0.872266</td>\n",
       "      <td>0.874544</td>\n",
       "      <td>0.877734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.955500</td>\n",
       "      <td>0.679824</td>\n",
       "      <td>0.874241</td>\n",
       "      <td>0.121355</td>\n",
       "      <td>0.871203</td>\n",
       "      <td>0.872418</td>\n",
       "      <td>0.874241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.715400</td>\n",
       "      <td>0.695589</td>\n",
       "      <td>0.885784</td>\n",
       "      <td>0.112242</td>\n",
       "      <td>0.882139</td>\n",
       "      <td>0.882746</td>\n",
       "      <td>0.885784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.636300</td>\n",
       "      <td>0.620599</td>\n",
       "      <td>0.880468</td>\n",
       "      <td>0.118925</td>\n",
       "      <td>0.876823</td>\n",
       "      <td>0.878949</td>\n",
       "      <td>0.880468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.607500</td>\n",
       "      <td>0.602422</td>\n",
       "      <td>0.875608</td>\n",
       "      <td>0.121203</td>\n",
       "      <td>0.870747</td>\n",
       "      <td>0.874696</td>\n",
       "      <td>0.875608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.651700</td>\n",
       "      <td>0.535196</td>\n",
       "      <td>0.892770</td>\n",
       "      <td>0.107230</td>\n",
       "      <td>0.887151</td>\n",
       "      <td>0.891100</td>\n",
       "      <td>0.892770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.680400</td>\n",
       "      <td>0.580784</td>\n",
       "      <td>0.895200</td>\n",
       "      <td>0.104040</td>\n",
       "      <td>0.888518</td>\n",
       "      <td>0.891859</td>\n",
       "      <td>0.895200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.652600</td>\n",
       "      <td>0.574190</td>\n",
       "      <td>0.890188</td>\n",
       "      <td>0.106318</td>\n",
       "      <td>0.888366</td>\n",
       "      <td>0.887303</td>\n",
       "      <td>0.890188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.584700</td>\n",
       "      <td>0.559790</td>\n",
       "      <td>0.887303</td>\n",
       "      <td>0.112090</td>\n",
       "      <td>0.884417</td>\n",
       "      <td>0.885176</td>\n",
       "      <td>0.887303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.687300</td>\n",
       "      <td>0.590254</td>\n",
       "      <td>0.893985</td>\n",
       "      <td>0.106470</td>\n",
       "      <td>0.886847</td>\n",
       "      <td>0.892163</td>\n",
       "      <td>0.893985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.515500</td>\n",
       "      <td>0.543932</td>\n",
       "      <td>0.890796</td>\n",
       "      <td>0.110267</td>\n",
       "      <td>0.885480</td>\n",
       "      <td>0.886087</td>\n",
       "      <td>0.890796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.595500</td>\n",
       "      <td>0.546018</td>\n",
       "      <td>0.892922</td>\n",
       "      <td>0.106166</td>\n",
       "      <td>0.886391</td>\n",
       "      <td>0.889581</td>\n",
       "      <td>0.892922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.541200</td>\n",
       "      <td>0.534425</td>\n",
       "      <td>0.896871</td>\n",
       "      <td>0.100091</td>\n",
       "      <td>0.890644</td>\n",
       "      <td>0.892163</td>\n",
       "      <td>0.896871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.573600</td>\n",
       "      <td>0.581551</td>\n",
       "      <td>0.894289</td>\n",
       "      <td>0.103888</td>\n",
       "      <td>0.888973</td>\n",
       "      <td>0.890340</td>\n",
       "      <td>0.894289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.568000</td>\n",
       "      <td>0.567159</td>\n",
       "      <td>0.894745</td>\n",
       "      <td>0.101610</td>\n",
       "      <td>0.885328</td>\n",
       "      <td>0.889429</td>\n",
       "      <td>0.894745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.584700</td>\n",
       "      <td>0.549911</td>\n",
       "      <td>0.897783</td>\n",
       "      <td>0.095990</td>\n",
       "      <td>0.893378</td>\n",
       "      <td>0.895504</td>\n",
       "      <td>0.897783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.627400</td>\n",
       "      <td>0.521262</td>\n",
       "      <td>0.899909</td>\n",
       "      <td>0.098269</td>\n",
       "      <td>0.893530</td>\n",
       "      <td>0.896719</td>\n",
       "      <td>0.899909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.562200</td>\n",
       "      <td>0.505434</td>\n",
       "      <td>0.898846</td>\n",
       "      <td>0.100851</td>\n",
       "      <td>0.892467</td>\n",
       "      <td>0.895808</td>\n",
       "      <td>0.898846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.752100</td>\n",
       "      <td>0.521506</td>\n",
       "      <td>0.898694</td>\n",
       "      <td>0.098572</td>\n",
       "      <td>0.894289</td>\n",
       "      <td>0.897023</td>\n",
       "      <td>0.898694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.788100</td>\n",
       "      <td>0.546183</td>\n",
       "      <td>0.901428</td>\n",
       "      <td>0.095231</td>\n",
       "      <td>0.895808</td>\n",
       "      <td>0.898542</td>\n",
       "      <td>0.901428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.658000</td>\n",
       "      <td>0.519290</td>\n",
       "      <td>0.901580</td>\n",
       "      <td>0.096750</td>\n",
       "      <td>0.893226</td>\n",
       "      <td>0.896871</td>\n",
       "      <td>0.901580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.519293</td>\n",
       "      <td>0.901883</td>\n",
       "      <td>0.096294</td>\n",
       "      <td>0.893226</td>\n",
       "      <td>0.897479</td>\n",
       "      <td>0.901883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca80c9f1bd6b430481751209d677e294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6250, training_loss=0.7685923744735121, metrics={'train_runtime': 3500.0427, 'train_samples_per_second': 28.571, 'train_steps_per_second': 1.786, 'total_flos': 0.0, 'train_loss': 0.7685923744735121, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "# 58m 20.0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all-nli-test_cosine_accuracy': 0.9140565894991678,\n",
       " 'all-nli-test_dot_accuracy': 0.08533817521561507,\n",
       " 'all-nli-test_manhattan_accuracy': 0.9073990013617794,\n",
       " 'all-nli-test_euclidean_accuracy': 0.9080042366469965,\n",
       " 'all-nli-test_max_accuracy': 0.9140565894991678}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Optional) Evaluate the trained model on the test set, after training completes\n",
    "test_evaluator = TripletEvaluator(\n",
    "    anchors=test_dataset[\"anchor\"],\n",
    "    positives=test_dataset[\"positive\"],\n",
    "    negatives=test_dataset[\"negative\"],\n",
    "    name=\"all-nli-test\",\n",
    ")\n",
    "test_evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Save the trained model\n",
    "model.save_pretrained(\"models/mpnet-base-all-nli-triplet/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. (Optional) Push it to the Hugging Face Hub ... Nope!\n",
    "# model.push_to_hub(\"mpnet-base-all-nli-triplet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ftllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
